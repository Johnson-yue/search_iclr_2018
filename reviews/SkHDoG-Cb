{"notes":[{"tddate":null,"ddate":null,"tmdate":1512196569021,"tcdate":1512196569021,"number":1,"cdate":1512196569021,"id":"Hk-y9aJZz","invitation":"ICLR.cc/2018/Conference/-/Paper930/Official_Comment","forum":"SkHDoG-Cb","replyto":"BJ7oBjolf","signatures":["ICLR.cc/2018/Conference/Paper930/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper930/Authors"],"content":{"title":"Short responses","comment":"Dear Reviewer2, \n\nFirst of all, we really appreciate your constructive review and detailed comments on our work. While we prepare a detailed response and a revised draft, we would like to share our short responses to the concerns raised by the reviewer.\n\n1. While we fully agree that some components of our framework are largely inspired by the CycleGAN and the S+U learning, our framework has its own novelty as follows:\n1) It starts with a novel adaptive data generation process, which we observed necessary to achieve the state-of-the-art performances in our own experiments. We will shortly include the full details of the experiments in the revised draft.\n2) Our approach is different from the traditional S+U learning framework: In the original S+U learning framework, the synthetic data is mapped to the real data, and then a predictor is trained on the translated data. In our work, we do not train our predictors after we learn the bidirectional mapping: Instead, we simply map test images to the synthetic domain, and directly apply predictors, which are trained solely with the synthetic data set. This has a significant advantage over the traditional S+U learning since one does not have to retrain predictors for each target domain. (Having one good predictor trained on the synthetic domain suffices!) We will make our description clearer in the revision.\n\n2. We will make this argument clearer in our revision. \n\n3. We will add a concrete summary of S+U learning in Sec. 2.\n\n4. What we intended to claim was that the cycle-consistency loss itself may not preserve labels, and hence it needs to be used together with feature-consistency loss. That is, we are proposing to use both the cycle-consistency loss and the feature-consistency loss. We will clarify the descriptions to avoid any confusion.\nFurther, in regard to the additional experiments, we will report a full performance table with varying values of \\lambda_{cyc} and \\lambda_{feature}.\n\nAgain, we really appreciate your constructive reviews and hopefully this short answer clears some of the concerns. We will shortly prepare a revised draft! \n\nThanks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings","abstract":"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. While their algorithm is shown to achieve the state-of-the-art performance on the gaze estimation task, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by these limitations, we propose a new S+U learning algorithm, which fully leverage the flexibility  of  data  simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017).","pdf":"/pdf/20c79150ce7adc491a548d89f07d22917313b079.pdf","paperhash":"anonymous|simulatedunsupervised_learning_with_adaptive_data_generation_and_bidirectional_mappings","_bibtex":"@article{\n  anonymous2018simulated+unsupervised,\n  title={Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHDoG-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper930/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222820228,"tcdate":1511925146895,"number":3,"cdate":1511925146895,"id":"BJ7oBjolf","invitation":"ICLR.cc/2018/Conference/-/Paper930/Official_Review","forum":"SkHDoG-Cb","replyto":"SkHDoG-Cb","signatures":["ICLR.cc/2018/Conference/Paper930/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting application of GAN type training. But not good enough.","rating":"3: Clear rejection","review":"General comment:\n\nThis paper proposes a GAN-based method which learns bidirectional mappings between the real-data and the simulated data. The proposed methods builds upon the CycleGAN and the Simulated+Unsupervised (S+U) learning frameworks. The authors show that the proposed method is able to fully leverage the flexibility of simulators by presenting an improved performance on the gaze estimation task.\n\nDetailed comments:\n\n1. The proposed method seems to be a direct combination of the CycleGAN and the S+U learning. Firstly, the CycleGAN propose a to learn a bidirectional GAN model between for image translation. Here the author apply it by \"translating\" the the simulated data to real-data. Moreover, the mapping from simulated data to the real-data is learned, the S+U learning framework propose to train a model on the simulated data.\n\nHence, this paper seems to directly apply S+U learning to CycleGAN. The properties of the proposed method comes immediately from CycleGAN and S+U learning. Without deeper insights of the proposed method, the novelty of this paper is not sufficient.\n\n2. When discussing CycleGAN, the authors claim that CycleGAN is not good at preserving the labels. However, it is not clear what the meaning of preserving labels is. It would be nice if the authors clearly define this notion and rigorously discuss why CycleGAN is insufficient to reach such a goal and why combining with S+U learning would help.\n\n3. This work seems closely related to S+U learning. It would be nice if the authors also summarize S+U learning in Section 2, in the similar way they summarize CycleGAN in Section 2.2.\n\n4. In Section 2.2, the authors claim that the Cycle-consistency loss in CycleGAN is not sufficient for label preservation. To improve, they propose to use the feature consistency loss. However, the final loss function also contains this cycle-consistency loss. Moreover, in the experiments, the authors indeed use the cycle-consistency loss by setting \\lambda_{cyc} = 10. But the feature consistency loss may not be used by setting \\lambda_{feature} = 0 or 0.5. From table Two, it appears that whether using the feature-consistency loss does not have significant effect on the performance.\n\nIt would be nice to conduct more experiments to show the effect of adding the feature-consistent loss. Say, setting \\lambda_{cyc} = 0 and try different values of \\lambda_{feature}. Otherwise it is unclear whether the feature-consistent loss is necessary.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings","abstract":"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. While their algorithm is shown to achieve the state-of-the-art performance on the gaze estimation task, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by these limitations, we propose a new S+U learning algorithm, which fully leverage the flexibility  of  data  simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017).","pdf":"/pdf/20c79150ce7adc491a548d89f07d22917313b079.pdf","paperhash":"anonymous|simulatedunsupervised_learning_with_adaptive_data_generation_and_bidirectional_mappings","_bibtex":"@article{\n  anonymous2018simulated+unsupervised,\n  title={Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHDoG-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper930/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222820275,"tcdate":1511853168384,"number":2,"cdate":1511853168384,"id":"BJ__hY9lz","invitation":"ICLR.cc/2018/Conference/-/Paper930/Official_Review","forum":"SkHDoG-Cb","replyto":"SkHDoG-Cb","signatures":["ICLR.cc/2018/Conference/Paper930/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review: Some interesting ideas, some missing related work and comparisons","rating":"6: Marginally above acceptance threshold","review":"Review, ICLR 2018, Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings\n\nSummary:\n\nThe paper presents several extensions to the method presented in SimGAN (Shirvastava et al. 2017). \nFirst, it adds a procedure to make the distribution of parameters of the simulation closer to the one in real world images. A predictor is trained on simulated images created with a manually initialized distribution. This predictor is used to estimate pseudo labels for the unlabeled real-world data. The distribution of the estimated pseudo labels is used produce a new set of simulated images. This process is iterated. \nSecond, it adds the idea of cycle consistency (e.g., from CycleGAN) in order to counter mode collapse and label shift. \nThird, since cycle consistency does not fully solve the label shift problem, a feature consistency loss is added.\nFinally, in contrast to ll related methods, the final system used for inference is not a predictor trained on a mix of real and “fake” images from the real-world target domain. Instead the predictor is trained purely on synthetic data and it is fed real world examples by using the back/real-to-sim-generator (trained in the conjunction with the forward mapping cycle) to map the real inputs to “fake” synthetic ones.\n\nThe paper is well written. The novelty is incremental in most parts, but the overall system can be seen as novel. \n\nIn particular, I am not aware of any published work that uses of the (backwards) real-to-sim generator plus sim-only trained predictor for inference (although I personally know several people who had the same idea and have been working on it). I like this part because it perfectly makes sense not to let the generator hallucinate real-world effects on rather clean simulated data, but the other way around, remove all kinds of variations to produce a clean image from which the prediction should be easier.\n\nThe paper should include Bousmalis et al., “Unsupervised Pixel-Level Domain Adaptation With Generative Adversarial Networks”, CVPR 2017 in its discussion, since it is very closely related to Shirvastava et al. 2017.\n\nWith respect to the feature consistency loss the paper should also discuss related work defining losses over feature activations for very similar reasons, such as in image stylization (e.g. L. A. Gatys et al. “Image Style Transfer Using Convolutional Neural Networks” CVPR 2016, L. A. Gatys et al. “Controlling Perceptual Factors in Neural Style Transfer” CVPR 2017), or the recently presented “Photographic Image Synthesis with Cascaded Refinement Networks”, ICCV 2017.\nBousmalis et al., “Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping”, arXiv:1709.07857, even uses the same technique in the context of training GANs.\n\nAdaptive Data Generation:\nI do not fully see the point in matching the distribution of parameters of the real world samples with the simulated data. For the few, easily interpretable parameters in the given task it should be relatively easy to specify reasonable ranges. If the simulation in some edge cases produces samples that are beyond the range of what occurs in the real world, that is maybe not very efficient, but I would be surprised if it ultimately hurt the performance of the predictor. \nI do see the advantage when training the GAN though, since a good discriminator would learn to pick out those samples as generated. Again, though, I am not very sure whether that would hurt the performance of the overall system in practice.\n\nLimiting the parameters to those values of the real world data also seems rather restricting. If the real world data does not cover certain ranges, not because those values are infeasible or infrequent, but just because it so happens that this range was not covered in the data acquisition, the simulation could be used to fill in those ranges. \nAdditionally, the whole procedure of training on sim data and then pseudo-labeling the real data with it is based on the assumption that a predictor trained on simulated data only already works quite well on real data. It might be possible in the case of the task at hand, but for more challenging domain adaptation problem it might not be feasible. \nThere is also no guarantee for the convergence of the cycle, which is also evident from the experiments (Table 1. After three iterations the angle error increases again. (The use of the Hellinger distance is unclear to me since it, as explained in the text, does not correspond with what is being optimized). In the experiments the cycle was stopped after two iterations. However, how would you know when to stop if you didn’t have ground truth labels for the real world data?\n\nComparisons:\nThe experiments should include a comparison to using the forward generator trained in this framework to train a predictor on “fake” real data and test it on real data (ie. a line “ours | RS | R | ?” in Table 2, and a more direct comparison to Shrivastava). This would be necessary to prove the benefit of using the back-generator + sim trained predictor.\n\n\nDetailed comments:\n* Figure 1 seems not to be referenced in the text.\n* I don’t understand the choice for reduction of the sim parameters. Why was, e.g., the yaw and pitch parameters of the eyeball set equal to those of the camera? Also, I guess there is a typo in the last equality (pitch and yaw of the camera?).\n* The Bibliography needs to be checked. Names of journals and conferences are inconsistent, long and short forms mixed, year several times, “Proceedings” multiple times, ...\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings","abstract":"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. While their algorithm is shown to achieve the state-of-the-art performance on the gaze estimation task, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by these limitations, we propose a new S+U learning algorithm, which fully leverage the flexibility  of  data  simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017).","pdf":"/pdf/20c79150ce7adc491a548d89f07d22917313b079.pdf","paperhash":"anonymous|simulatedunsupervised_learning_with_adaptive_data_generation_and_bidirectional_mappings","_bibtex":"@article{\n  anonymous2018simulated+unsupervised,\n  title={Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHDoG-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper930/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222820317,"tcdate":1511597647705,"number":1,"cdate":1511597647705,"id":"S1uLIj8lG","invitation":"ICLR.cc/2018/Conference/-/Paper930/Official_Review","forum":"SkHDoG-Cb","replyto":"SkHDoG-Cb","signatures":["ICLR.cc/2018/Conference/Paper930/AnonReviewer1"],"readers":["everyone"],"content":{"title":" This paper considers a generative approach in semi-supervised setting, which is basically a combination of cycleGAN and the Apple's S+U leaning.","rating":"6: Marginally above acceptance threshold","review":"* sec.2.2 is about label-preserving translation and many notations are introduced. However, it is not clear what label here refers to, and it does not shown in the notation so far at all. Only until the end of sec.2.2, the function F(.) is introduced and its revelation - Google Search as label function is discussed only at Fig.4 and sec.2.3.\n* pp.5 first paragraph: when assuming D_X and D_Y being perfect, why L_GAN_forward = L_GAN_backward = 0? To trace back, in fact it is helpful to have at least a simple intro/def. to the functions D(.) and G(.) of Eq.(1). \n* Somehow there is a feeling that the notations in sec.2.1 and sec.2.2 are not well aligned. It is helpful to start providing the math notations as early as sec.2.1, so labels, pseudo labels, the algorithm illustrated in Fig.2 etc. can be consistently integrated with the rest notations. \n* F() is firstly shown in Fig.2 the beginning of pp.3, and is mentioned in the main text as late as of pp.5.\n* Table 2: The CNN baseline gives an error rate of 7.80 while the proposed variants are 7.73 and 7.60 respectively. The difference of 0.07/0.20 are not so significant. Any explanation for that?\nMinor issues:\n* The uppercase X in the sentence before Eq.(2) should be calligraphic X","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings","abstract":"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. While their algorithm is shown to achieve the state-of-the-art performance on the gaze estimation task, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by these limitations, we propose a new S+U learning algorithm, which fully leverage the flexibility  of  data  simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017).","pdf":"/pdf/20c79150ce7adc491a548d89f07d22917313b079.pdf","paperhash":"anonymous|simulatedunsupervised_learning_with_adaptive_data_generation_and_bidirectional_mappings","_bibtex":"@article{\n  anonymous2018simulated+unsupervised,\n  title={Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHDoG-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper930/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092384769,"tcdate":1509137247902,"number":930,"cdate":1510092362216,"id":"SkHDoG-Cb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkHDoG-Cb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings","abstract":"Collecting a large dataset with high quality annotations is expensive and time-consuming. Recently, Shrivastava et al. (2017) propose Simulated+Unsupervised (S+U) learning: It first learns a mapping from synthetic data to real data, translates a large amount of labeled synthetic data to the ones that resemble real data, and then trains a learning model on the translated data. While their algorithm is shown to achieve the state-of-the-art performance on the gaze estimation task, it may have a room for improvement, as it does not fully leverage flexibility of data simulation process and consider only the forward (synthetic to real) mapping. Inspired by these limitations, we propose a new S+U learning algorithm, which fully leverage the flexibility  of  data  simulators and bidirectional mappings between synthetic data and real data. We show that our approach achieves the improved performance on the gaze estimation task, outperforming (Shrivastava et al., 2017).","pdf":"/pdf/20c79150ce7adc491a548d89f07d22917313b079.pdf","paperhash":"anonymous|simulatedunsupervised_learning_with_adaptive_data_generation_and_bidirectional_mappings","_bibtex":"@article{\n  anonymous2018simulated+unsupervised,\n  title={Simulated+Unsupervised Learning With Adaptive Data Generation and Bidirectional Mappings},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHDoG-Cb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper930/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}