{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222709445,"tcdate":1512034080782,"number":3,"cdate":1512034080782,"id":"S1t718Tef","invitation":"ICLR.cc/2018/Conference/-/Paper654/Official_Review","forum":"BkVsWbbAW","replyto":"BkVsWbbAW","signatures":["ICLR.cc/2018/Conference/Paper654/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A moderately surprising but useful study on replay based learning","rating":"7: Good paper, accept","review":"This paper introduces a neural network architecture for continual learning. The model is inspired by current knowledge about long term memory consolidation mechanisms in humans. As a consequence, it uses:\n-\tOne temporary memory storage (inspired by hippocampus) and a long term memory\n-\tA notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks.\nOverall, although the result are not very surprising, the approach is well justified and extensively tested. It provides some insights on the challenges and benefits of replay based memory consolidation.\n\nComments:\n\t\n1-\tThe results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach. \n2-\tIt is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume). \n3-\tSo the main benefit of the approach seems to point towards the direction of what possibly happens in real brains. It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep.\n4-\tDue to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting\n5-\tThe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511930440538,"tcdate":1511930440538,"number":1,"cdate":1511930440538,"id":"r1gLq2olz","invitation":"ICLR.cc/2018/Conference/-/Paper654/Official_Comment","forum":"BkVsWbbAW","replyto":"rkb_ZMilM","signatures":["ICLR.cc/2018/Conference/Paper654/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper654/Authors"],"content":{"title":"Yes, indeed! Our model works for fixed random permutations of all pixels too.","comment":"Our permuted MNIST variant does contain the \"fixed random permutation of all pixels\" tasks (see sec 8.1, appendix B, tasks iv and v for permnist). We basically included a few of all kinds of tasks (whitening-type, permutation-type and reflection-type), as was necessary to prove that our algorithm performs as well as the baselines on a set of tasks used in the past.\nHowever, permnist is not our major focus, since as pointed out in section 4.2.1, permnist is not a good dataset to test for catastrophic forgetting. We observed that it is easily conquered by most approaches if they use a largely overparameterized neural network. See figure 3b of Kirkpatrick et al (2017) and you'll observe that even after learning 10 tasks sequentially, their baseline (SGD+Dropout) drops to only little below 80% accuracy. Such forgetting can hardly be deemed catastrophic, and is partly because of using really large networks. Using a 2-hidden layer network each with above 400 units per layer (Goodfellow et al, 2015; Kirkpatrick et al, 2017) allows the network to essentially finds ways to **memorize** samples and labels from different tasks without inducing much parameter sharing. In such cases, it is unclear if it is the continual learning algorithm at work, or just the overparameterized network aiding in mitigating catastrophic forgetting. But our experiments show that our approach still retains a good accuracy even with small networks (with fully-connected layers having 48, 48 units; see appendix B), whereas most other baselines are not able to retain their accuracy (see figure 5a). \nMoreover, we show that datasets like Digits, although simple at first glance, are actually much more challenging datasets to test for catastrophic forgetting and are hard to conquer even with overparameterized networks. Hence, we focused most of our attention on Digits and TDigits. \nLastly, to clarify again, we did experiment with the full permnist dataset and can include more \"permutation\" tasks if needed, since our algorithm (DGDMN) works perfectly well with all permutation-type tasks and outperforms all baselines on the full permnist too.\n\nThe consolidation phase frequency is characterized by n_{STM} hyperparameter. n_{STM} was 2 for both Digits and Permnist, and 5 for TDigits (see section 8.4 in appendix B)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511887208789,"tcdate":1511887208789,"number":1,"cdate":1511887208789,"id":"rkb_ZMilM","invitation":"ICLR.cc/2018/Conference/-/Paper654/Public_Comment","forum":"BkVsWbbAW","replyto":"BkVsWbbAW","signatures":["~Siddhant_Jayakumar2"],"readers":["everyone"],"writers":["~Siddhant_Jayakumar2"],"content":{"title":"Clarification on tasks","comment":"Interesting approach! \n\nJust a few questions about the tasks and parameters. \n\nThe permuted MNIST variant considered here seems to be different from the setting in the Goodfellow et al (2014) and Kirkpatrick et al (2017) unless I'm mistaken? What was the rationale behind this? Does the model proposed also cope well with the standard \"fixed random permutation of all pixels\" for each task, as opposed to the cropping and whitening style tasks employed in the paper? \n\nFurther, how often was the \"sleep\" or consolidation phase used?\n\n\n\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222709486,"tcdate":1511882414427,"number":2,"cdate":1511882414427,"id":"rk8n0esxf","invitation":"ICLR.cc/2018/Conference/-/Paper654/Official_Review","forum":"BkVsWbbAW","replyto":"BkVsWbbAW","signatures":["ICLR.cc/2018/Conference/Paper654/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Successful sequential learning of MNIST task variants in challenging single pass setting","rating":"6: Marginally above acceptance threshold","review":"This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime. Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction. The method is tested on standard sequential MNIST variants as long as a class incremental variant. Superior performance to recent baselines (e.g. EWC) is reported in several cases. Interesting parallels with human cortical and hippocampal learning and memory are discussed.\n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines. The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though. There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL. Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains. In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to ‘negative interference’ between tasks [1, 2]. I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting. The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL.\n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, Çaglar Gülçehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222709526,"tcdate":1511805743555,"number":1,"cdate":1511805743555,"id":"SydEmAKgM","invitation":"ICLR.cc/2018/Conference/-/Paper654/Official_Review","forum":"BkVsWbbAW","replyto":"BkVsWbbAW","signatures":["ICLR.cc/2018/Conference/Paper654/AnonReviewer3"],"readers":["everyone"],"content":{"title":"It's not clear if it is solving the core issue of catastrophic forgetting ","rating":"4: Ok but not good enough - rejection","review":"This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting. They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory. \n\nThe main novelty of this work are 1-balancing mechanism for the replay memory. 2-Using multiple models for short and long term memory. The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before. As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting. \n\nThe paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model. It is not clear to me as why is the case, if anything it should be the opposite. Humans and animal are not given task ID and it's always clear distinction between task in real world.\n\nDeep Generative Replay section and description of DGDMN are written poorly and is very incomprehensible. It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism. \n\nUsing the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting.\n\nBaselines: why is Shin et al. (2017) not included as one of the baselines? As it is the closet method to this paper it is essential to be compared against.\n\nI disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA.  Overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capacity.\n\n12 pages is very long, 8 pages was the suggested page limit. It’s understandable if the page limit is extend by one page, but 4 pages is over stretching. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739178638,"tcdate":1509130651632,"number":654,"cdate":1509739175981,"id":"BkVsWbbAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkVsWbbAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Generative Dual Memory Network for Continual Learning","abstract":"Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.","pdf":"/pdf/96ffec703fe20e8f3faa1d29b38d91fba2aacedd.pdf","TL;DR":"A dual memory architecture inspired from human brain to learn sequentially incoming tasks, while averting catastrophic forgetting.","paperhash":"anonymous|deep_generative_dual_memory_network_for_continual_learning","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Generative Dual Memory Network for Continual Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVsWbbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper654/Authors"],"keywords":["Continual Learning","Catastrophic Forgetting","Sequential Multitask Learning","Deep Generative Models","Dual Memory Networks","Deep Learning"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}