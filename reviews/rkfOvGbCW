{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222797080,"tcdate":1511818732488,"number":3,"cdate":1511818732488,"id":"ByEeLZ5xz","invitation":"ICLR.cc/2018/Conference/-/Paper857/Official_Review","forum":"rkfOvGbCW","replyto":"rkfOvGbCW","signatures":["ICLR.cc/2018/Conference/Paper857/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very interesting use of episodic memory, could be even stronger","rating":"8: Top 50% of accepted papers, clear accept","review":"This article introduces a new method to improve neural network performances on tasks ranging from continual learning (non-stationary target distribution, appearance of new classes, adaptation to new tasks, etc) to better handling of class imbalance, via a hybrid architecture between nearest neighbours and neural net.\nAfter an introduction summarizing their goal, the authors introduce their Model-based parameter adaptation: this hybrid architecture enriches classical deep architectures with a non-parametric “episodic” memory, which is filled at training time with (possibly learned) encodings of training examples and then polled at inference time to refine the neural network parameters with a few steps of gradient in a direction determined by the closest neighbours in memory to the input being processed.  The authors justify this inference-time SGD update with three different interpretations: one linked in Maximum A Posteriori optimization, another to Elastic Weight Regularisation (the current state of the art in continual learning), and one generalising attention mechanisms (although to be honest that later was more elusive to this reviewer). The mandatory literature review on the abundant recent uses of memory in neural networks is then followed by experiments on continual learning tasks involving permuted MNIST tasks, ImageNET incremental inclusion of classes, ImageNet unbalanced, and two language modeling tasks. \n\nThis is an overall very interesting idea, which has the merit of being rather simple in its execution and can be combined with many other methods: it is fully compatible with any optimiser (e.g. ADAM) and can be tacked on top of EWC (which the authors do). The justification is clear, the examples reasonably thorough. It is a very solid paper, which this reviewer believes to be of real interest to the ICLR community.\n\n\nThe following important clarifications from the authors could make it even better:\n*  Algorithm 1 in its current form seems to imply an infinite memory, which the experiments make clear is not the case. Therefore: how does the algorithm decide what entries to discard when the memory fills up?\n* In most non-trivial settings, the parameter $gamma$ of the encoding is learned, and therefore older entries in the memory lose any ability to be compared to more recent encodings. How do the authors handle this obsolescence of the memory, other than the trivial scheme of relying on KNN to only match recent entries?\n* Because gamma needs to be “recent”, this means “theta” is also recent: could the authors give a good intuition on how the two sets of parameters can evolve at different enough timescales to really make the episodic memory relevant? Is it anything else than relying on the fact that the lower levels of a neural net converge before the upper levels?\n* Table 1:  could the authors explain why the pre-trained Parametric (and then Mixture) models have the best  AUC in the low-data regime, whereas MbPA was designed very much to be superior in such regimes?\n* Paragraph below equation (5), page 3: why not including the regularisation term, whereas the authors just went to great pain to explain it? Rationale? Not including it is also akin to using an improper non-information prior on theta^x independent of theta, which is quite a strong choice to be made “by default”.\n* The extra complexity of choosing the learning rate alpha_M and the number of  MpAB steps is worrying this reviewer somewhat. In practice, in Section 4.1the authors explain using grid search to tune the parameters. Is this reviewer correct in understanding that this search is done across all tasks, as opposed to only the first task? And if so, doesn’t this grid search introduce an information leak by bringing information from the whole pre-determined set of task, therefore undermining the very “continuous learning” aim? How do the algorithm performs if the grid search is done only on the first task?\n* Figure 3:  the text could clarify that the accuracy is measured across all tasks seen so far. It would be interesting to add a figure (in the Appendix) showing the evolution of the accuracy *per task*, not just the aggregated accuracy. \n* In the related works linking neural networks to encoded episodic memory, the authors might want to include the stream of research on HMAX of Anselmi et al 2014 (https://arxiv.org/pdf/1311.4158.pdf) , Leibo et al 2015 (https://arxiv.org/abs/1512.08457), and Blundell et al 2016 (https://arxiv.org/pdf/1606.04460.pdf ).\n\nMinor typos:\n* Figure 4: the title of the key says “New/Old” but then the lines read, in order, “Old” then “New” -- it would be nicer to have them in the same order.\n* Section 5: missing period between \"ephemeral gradient modifications\" and \"Further\".\n* Section 4.2, parenthesis should be \"perform well across all 1000 classes\", not \"all 100 classes\".\n \nWith the above clarifications, this article could become a very remarked contribution.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222797119,"tcdate":1511654496847,"number":2,"cdate":1511654496847,"id":"rktPEKveG","invitation":"ICLR.cc/2018/Conference/-/Paper857/Official_Review","forum":"rkfOvGbCW","replyto":"rkfOvGbCW","signatures":["ICLR.cc/2018/Conference/Paper857/AnonReviewer2"],"readers":["everyone"],"content":{"title":"In general, I found this paper interesting but it needs improvement in writing and more clarity in the contribution. There are quite similarities (e.g. memory architecture and model components) with previous works which authors need to be more clear about their contributions. ","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a non-parametric episodic memory that can be used for the rapid acquisition of new knowledge while preserving the old ones. More specially, it locally adapts the parameters of a network using the episodic memory structure. \n\nStrength:\n+   The paper works on a relevant and interesting problem.\n+   The experiment sections are very thorough and I like the fact that the authors selected different tasks to compare their models with. \n+ The paper is well-written except for sections 2 and 3. \nWeakness and Questions:\n-    Even though the paper addresses the interesting and challenging problem of slow adaption when distribution shifts, their episodic memory is quite similar (if not same as) to the Pritzel et al., 2017. \n- In addition, as the author mentioned in the text, their model is also similar to the Kirkpatrick et al., 2017,  Finn et al., 2017, Krause et al., 2017. That would be great if the author can list \"explicitly\" the contribution of the paper with comparing with those. Right now, the text mentioned some of the similarity but it spreads across different sections and parts. \n- The proposed model does adaption during the test time, but other papers such as Li & Hoiem, 2016 handles the shift across domain in the train time. Can authors say sth about the motivation behind adaptation during test time vs. training time? \n- There are some inconsistencies in the text about the parameters and formulations:\n      -- what is second subscript in {v_i}_i? (page 2, 3rd paragraph)\n      -- in Equation 4, what is the difference between x_c and x?\n      -- What happened to $x$ in Eq 5?\n      -- The \"−\" in Eq. 7 doesn't make sense. \n- Section 2.2, after equation 7, the text is not that clear.\n- Paper is well beyond the 8-page limit and should be fitted to be 8 pages.\n- In order to make the experiments reproducible, the paper needs to contain full details (in the appendix) about the setup and hyperparameters of the experiments.   \n\nOthers:\nDo the authors plan to release the codes?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511472747168,"tcdate":1511472630556,"number":1,"cdate":1511472630556,"id":"rJyW0n4eM","invitation":"ICLR.cc/2018/Conference/-/Paper857/Official_Comment","forum":"rkfOvGbCW","replyto":"B1tJwiMef","signatures":["ICLR.cc/2018/Conference/Paper857/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper857/Authors"],"content":{"title":"Reply: Few Questions","comment":"Thanks for your comment. Please see below our response.\n\n(1) A valid point is made about memory constraints, which can be clarified in the text. When comparing MbPA against the Neural Cache we do use a bounded memory in both cases. We swept over memory sizes from 500 - 20,000. E.g. for WikiText2 the optimal memory size for the neural cache was 6,000, whereas for MbPA it was 5,000. We will update the paper with further details of hyper-parameter sweeps and optimal parameters. For the language modeling experiments it is apparent the unbounded cache, which was posted on arXiv after the ICLR submission date, performs strictly worse than the bounded cache and so we prefer to compare our method versus the best performing variant. As for the point about performance for a given LSTM: on PTB, MbPA produced the best results in our experiments. On WikiText2, we find that MbPA does not focus purely on most recent terms and thus the combination of all three models (LSTM baseline, MbPA and neural cache) produced the largest drop of 15 perplexity. \n\n(2) MbPA and MAML are indeed similar in that they aim to fine-tune or adapt a network with relevant data. The general idea of adapting a network to a relevant context is also not unique to these methods but an old idea: speaker adaptation, dynamic evaluation, Dyna2, etc.\nThe contribution of MAML is to train a base model through the task-specific optimization, to obtain a set of ‘easily tunable’ parameters that can be adapted to the task at hand, while MbPA adapts its parameters in an online fashion using its episodic memory.\n\nIn the continual learning setup (eg. permuted MNIST) - one does not have the ability to re-train on a previous task and thus there is then no way of applying MAML in this setting without access to this task oracle (which would give MAML privileged information).\n\nThe same is true for language modeling where there are in fact no clear “tasks”, but a changing data distribution at test time which MbPA’s context based lookup can cope with. Instead for LM we compare MbPA to dynamic evaluation, which is a more relevant alternative method of local fitting. Conversely, applying MbPA to the MAML tasks would not work well on the first visit, as its memory would have no experience from this un-seen task, but would be able to cope with the task at test time. \n\n(3) This is a great list of references that we will be sure to describe it in an expanded literature review. Most however seem to have been published around or after the ICLR deadline and thus have not been addressed. We discuss the unbounded cache in comment (1) above.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222797162,"tcdate":1511356311350,"number":1,"cdate":1511356311350,"id":"HkJsPxmxG","invitation":"ICLR.cc/2018/Conference/-/Paper857/Official_Review","forum":"rkfOvGbCW","replyto":"rkfOvGbCW","signatures":["ICLR.cc/2018/Conference/Paper857/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors propose a memory-based mechanism to adapt model parameters with local context lookup. ","rating":"6: Marginally above acceptance threshold","review":"Overall, the idea of this paper is simple but interesting. Via weighted mean NLL over retrieved neighbors, one can update parameters of output network for a given query input. The MAP interpretation provides a flexible Bayesian explanation about this MbPA.\n\nThe paper is written well, and the proposed method is evaluated on a number of relevant applications (e.g., continuing learning, incremental learning, unbalanced data, and domain shifts.)\n\nHere are some comments:\n1 MbPA is built upon memory. How large should it be? Is it efficient to retrieve neighbors for a given query?\n2 For each test, how many steps of MbPA do we need in general? Furthermore, it is a bit unfair for me to retrain deep model, based on test inputs. It seems that, you are implicitly using test data to fit model.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511335740454,"tcdate":1511335648796,"number":1,"cdate":1511335648796,"id":"B1tJwiMef","invitation":"ICLR.cc/2018/Conference/-/Paper857/Public_Comment","forum":"rkfOvGbCW","replyto":"rkfOvGbCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Few Questions","comment":"1) You compare against the cache model of Grave et al, however their results depend on the size of the cache.\nThe comparisons doesn't seem fair, as the neural cache has bounded memory, but I didn't catch the amount of memory deployed in your model. Furthermore as your results suggest that for a given LSTM that neural cache seemed to be better. Can you provide some details about the size of cache used versus the amount of memory your model used.\n\n2) Since the method seems very related to MAML (Finn, Abbeel, and Levine 2017), a comparison against it would be good to see, where both methods are applicable.\n\n3) There has been other recent work on online modeling and adaptation using history/memory.  A discussion about relevance/similarity/differences between your model wrt these models would be great.\n\nUnbounded cache model for online language modeling with open vocabulary\nhttps://arxiv.org/pdf/1711.02604.pdf by Grave, Cisse and Joulin\n\nImproving One-Shot Learning through Fusing Side Information\nhttps://arxiv.org/pdf/1710.08347.pdf by Tsai and Salakhutdinov \n(This one uses attention on history, and doesn't seem fundamentally different from memory)\n\nMeta-Learning via Feature-Label Memory Network\nhttps://arxiv.org/pdf/1710.07110.pdf by Mureja, Park and Yoo\n\nLabel Organized Memory Augmented Neural Network \nhttps://arxiv.org/pdf/1707.01461.pdf by Shankar and Sarawagi\n\nOnline Adaptation of Convolutional Neural Networks for Video Object Segmentation\nhttps://arxiv.org/pdf/1706.09364.pdf by Voigtlaender and Leibe\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739064411,"tcdate":1509136234391,"number":857,"cdate":1509739061748,"id":"rkfOvGbCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkfOvGbCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Memory-based Parameter Adaptation","abstract":" Deep neural networks have excelled on a wide range of problems, from vision to language and game playing. Neural networks very gradually incorporate information into weights as they process data, requiring very low learning rates. If the training distribution shifts, the network is slow to adapt, and when it does adapt, it typically performs badly on the training distribution before the shift. Our method, Memory-based Parameter Adaptation, stores examples in memory and then uses a context-based lookup to directly modify the weights of a neural network. Much higher learning rates can be used for this local adaptation, reneging the need for many iterations over similar data before good predictions can be made. As our method is memory-based, it alleviates several shortcomings of neural networks, such as catastrophic forgetting, fast, stable acquisition of new knowledge, learning with an imbalanced class labels, and fast learning during evaluation. We demonstrate this on a range of supervised tasks: large-scale image classification and language modelling.","pdf":"/pdf/96b335527a59b725ec46c9408ee7cf85ffe0d8a9.pdf","paperhash":"anonymous|memorybased_parameter_adaptation","_bibtex":"@article{\n  anonymous2018memory-based,\n  title={Memory-based Parameter Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkfOvGbCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper857/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}