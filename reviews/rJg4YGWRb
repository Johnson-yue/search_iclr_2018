{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222802843,"tcdate":1512153480603,"number":3,"cdate":1512153480603,"id":"S1Z9bmyZf","invitation":"ICLR.cc/2018/Conference/-/Paper886/Official_Review","forum":"rJg4YGWRb","replyto":"rJg4YGWRb","signatures":["ICLR.cc/2018/Conference/Paper886/AnonReviewer4"],"readers":["everyone"],"content":{"title":"few questions","rating":"6: Marginally above acceptance threshold","review":"SUMMARY.\n\nThe paper presents an extension of graph convolutional networks.\nGraph convolutional networks are able to model nodes in a graph taking into consideration the structure of the graph.\nThe authors propose two extensions of GCNs, they first remove intermediate non-linearities from the GCN computation, and then they add an attention mechanism in the aggregation layer, in order to weight the contribution of neighboring nodes in the creation of the new node representation.\nInterestingly, the proposed linear model obtains results that are on-par with the state-of-the-art model, and the linear model with attention outperforms the state-of-the-art models on several standard benchmarks.\n\n\n----------\n\nOVERALL JUDGMENT\nThe paper is, for the most part, clear, although some improvement on the presentation would be good (see below).\nAn important issue the authors should address is the notation consistency, the indexes i and j are used for defining nodes and labels, please use another index for labels.\nIt is very interesting that stripping standard GCN out of nonlinearities gives pretty much the same results, I would appreciate if the authors could give some insights of why this is the case.\nIt seems to me that an important experiment is missing here, have the authors tried to apply the attention model with the standard GCN?\nI like the idea of using a very minimal attention mechanism. The similarity function used for the attention (cosine) is symmetric, this means that if two nodes are connected in both directions, they will be equally important for each other. But intuitively this is not true in general. It would be interesting if the authors could elaborate a bit more on the choice of the similarity function.\n\n\n----------\n\nDETAILED COMMENTS\nPage 2. I do not understand the point of so many details on Graph Laplacian Regularization.\nPage 2. The use of the term 'skip-grams' is somewhat odd, it is not clear what the authors mean with that.\nPage 3. 'the natural random walk' ???\nBottom of page 4. When the authors introduce the attention based network also introduce the input/embedding layer, I believe there is a better place to do so instead of that together with the most important contribution of the paper.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222802884,"tcdate":1511955519100,"number":2,"cdate":1511955519100,"id":"HJvS2zhgz","invitation":"ICLR.cc/2018/Conference/-/Paper886/Official_Review","forum":"rJg4YGWRb","replyto":"rJg4YGWRb","signatures":["ICLR.cc/2018/Conference/Paper886/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting paper with nice findings. The originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.","rating":"7: Good paper, accept","review":"The paper proposes a semi supervised learning algorithm for graph node classification. The Algorithm is inspired from Graph Neural Networks and more precisely graph convolutional NNs recently proposed by ref (Kipf et al  2016)) in the paper.  These NNs alternate 2 types of layers:  non linear projection and diffusion, the latter incorporates the graph relational information by constraining neighbor nodes to have close representations according to some “graph metrics”. The authors propose a model with simplified projection layers and more sophisticated diffusion ones, incorporating a simple attention mechanism. Experiments are performed on citation textual datasets. Comparisons with published results on the same datasets are presented.\n\nThe paper is clear and develops interesting ideas relevant to semi-supervised graph node classification. One finding is that simple models perform as well as more complex ones in this setting where labeled data is scarce. Another one is the importance of integrating relational information for classifying nodes when it is available. The attention mechanism itself is extremely simple, and learns one parameter per diffusion layers. One parameter weights correlations between node embeddings in a diffusion layer. I understand that you tried more complex attention mechanisms, but the one finally selected is barely an attention mechanism and rather a simple “importance” weight. This is not a criticism, but this makes the title somewhat misleading. The experiments show that the proposed model is state of the art for graph node classification. The performance is on par with some other recent models according to table 2. The other tests are also interesting, but the comparison could have been extended to other models e.g. GCN.\nYou advocate the role of the diffusion layers, and in the experiments you stack 3 to 4 such layers. It would be interesting to have indications on the compromise performance/ number of diffusion layers and on the evolution of these performances when adding such layers.\nThe bibliography on semi-supervised learning in graphs for classification is light and should be enhanced.\nOverall this is an interesting paper with nice findings. The originality is however relatively limited in a field where many recent papers have been proposed, and the experiments need to be completed.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1512222802932,"tcdate":1511584123538,"number":1,"cdate":1511584123538,"id":"rJmKbdIgM","invitation":"ICLR.cc/2018/Conference/-/Paper886/Official_Review","forum":"rJg4YGWRb","replyto":"rJg4YGWRb","signatures":["ICLR.cc/2018/Conference/Paper886/AnonReviewer2"],"readers":["everyone"],"content":{"title":"idea would be reasonable and constains interesting insight","rating":"6: Marginally above acceptance threshold","review":"The paper proposes graph-based neural network in which weights from neighboring nodes are adaptively determined. The paper shows importance of propagation layer while showing the non-linear layer does not have significant effect. Further the proposed method also provides class relation based on the edge-wise relevance.\n\nThe paper is easy to follow and the idea would be reasonable. \n\nImportance of the propagation layer than the non-linear layer is interesting, and I think it is worth showing.\n\nVariance of results of AGNN is comparable or even smaller than GLN. This is a bit surprising because AGNN would be more complicated computation than GLN. Is there any good explanation of this low variance of AGNN?\n\nInterpretation of Figure 2 is not clear. All colored nodes except for the thick circle are labeled node? I couldn't judge those predictions are appropriate or not.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1511280274149,"tcdate":1511280274149,"number":2,"cdate":1511280274149,"id":"rk5cCabxf","invitation":"ICLR.cc/2018/Conference/-/Paper886/Official_Comment","forum":"rJg4YGWRb","replyto":"H1buZ4xlz","signatures":["ICLR.cc/2018/Conference/Paper886/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper886/Authors"],"content":{"title":"Thank you for the insightful comment.","comment":"We agree that graph classification is another exciting application where graph neural networks are making breakthroughs. There are several key differences in the dataset (from citation networks) and we have not tried the idea of linear architecture for the molecular dataset yet. For example, the edges have attributes. There are straight forward ways to incorporate such information into GNNs, but we have not pursued this direction yet. I do agree the experiments you suggested will both (a) clarify what the gain is in non-linear activation; and (b) give insights on how different datasets (and applications) might require different architectures. \n\nFor the linear model, we did not tune the hyper parameters and the same hyper parameters are used as your (Kipf and Welling) original GCN. We made a small change in the stopping criteria  to take the best model in validation error out of all epochs. We did not see any significant change when we use the same stopping criteria as GCN. We will make this explicit during the revision process. Overall, there was no hyperparameter tuning for the linear model, and all the numbers should provide fair comparisons.\n\nThank you for the references, we will surely include and discuss all the great prior work you pointed out. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1511175072203,"tcdate":1511174505372,"number":2,"cdate":1511174505372,"id":"H1buZ4xlz","invitation":"ICLR.cc/2018/Conference/-/Paper886/Public_Comment","forum":"rJg4YGWRb","replyto":"rJg4YGWRb","signatures":["~Thomas_N._Kipf1"],"readers":["everyone"],"writers":["~Thomas_N._Kipf1"],"content":{"title":"Linear model and further references on attention-based models","comment":"Very interesting work!\n\nYour insight about using a linear activation function on the hidden layers of a graph neural net looks interesting and indeed simplifies this class of models significantly. Have you been able to verify this architecture on some more challenging tasks, e.g. for molecule classification or the tasks presented in https://arxiv.org/abs/1511.05493, where graph neural networks typically show very strong performance as well? \n\nI am also wondering about your choice of hyper parameters when you compare your linear model to the one in https://arxiv.org/abs/1609.02907 : do you similarly use the same set of hyper parameters for all three datasets (Cora, Citeseer and Pubmed), or do you tune them individually? To make your result stronger, it would be good to tune the baseline (GCN) with the same procedure and include GCN baseline results on all of your experiments - it should be very simple by just running: https://github.com/tkipf/gcn/\n\nAs noted by Yedid Hoshen, it would be good if you could refer to some earlier work on graph neural networks with attention mechanisms:\n\nhttps://arxiv.org/abs/1703.07326 - Introduces \"Neighborhood attention\"\nhttps://arxiv.org/abs/1706.06383 - Improved version of \"Neighborhood attention\"\nhttps://arxiv.org/abs/1706.06122 - Attention mechanism in a graph neural net model for multi-agent reinforcement learning (as noted by Yedid Hoshen)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1510583672908,"tcdate":1510583672908,"number":1,"cdate":1510583672908,"id":"rkWt6QPyG","invitation":"ICLR.cc/2018/Conference/-/Paper886/Official_Comment","forum":"rJg4YGWRb","replyto":"rk8rhql1G","signatures":["ICLR.cc/2018/Conference/Paper886/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper886/Authors"],"content":{"title":"Thank you for pointing out the missing reference.","comment":"Thank you for your interest in our paper and bringing the VAIN model to our attention.\n\nWe see that VAIN uses attention between multiple-agents in a system. We were not aware of this line of literature when we submitted the paper. We will cite this line of work in our final version. Below is a comparison between VAIN and Attention-based-Graph Neural Network.\n\nMain similarities and differences between VAIN and Attention-based Graph Neural Network (AGNN) are as follows:\n1. Experimental results: AGNN is tested on semi-supervised classification of nodes on a graph where as VAIN is tested on prediction of future state in multi-agent systems.\n\n2. Side information: AGNN is graph processing neural network, but VAIN model does not take graph as an input as initially proposed (although it could).  In VAIN, it is assumed that every agent can possibly interact with every other agent in the system. Where as in AGNN we have a known graph in which real world first order interaction between two nodes are represented as edges and attention is computed only for these first order interactions. VAIN clubs all the higher order (long range) interactions into a single attention mechanism, where as AGNN computes higher order interactions through multiple hops of first order attention mechanism.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1510153279634,"tcdate":1510153279634,"number":1,"cdate":1510153279634,"id":"rk8rhql1G","invitation":"ICLR.cc/2018/Conference/-/Paper886/Public_Comment","forum":"rJg4YGWRb","replyto":"rJg4YGWRb","signatures":["~Yedid_Hoshen1"],"readers":["everyone"],"writers":["~Yedid_Hoshen1"],"content":{"title":"Related Work","comment":"Nice work! \n\nIt would be interesting to compare this work and VAIN (NIPS'17) https://arxiv.org/abs/1706.06122"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]}},{"tddate":null,"ddate":null,"tmdate":1509739047938,"tcdate":1509136680311,"number":886,"cdate":1509739045275,"id":"rJg4YGWRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJg4YGWRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Attention-based Graph Neural Network for Semi-supervised Learning","abstract":"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other.","pdf":"/pdf/2171447de61701922ab934d1bcfac5f0a1274455.pdf","TL;DR":"We propose a novel attention-based interpretable Graph Neural Network architecture which outperforms the current state-of-the-art Graph Neural Networks in standard benchmark datasets","paperhash":"anonymous|attentionbased_graph_neural_network_for_semisupervised_learning","_bibtex":"@article{\n  anonymous2018attention-based,\n  title={Attention-based Graph Neural Network for Semi-supervised Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJg4YGWRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper886/Authors"],"keywords":["Graph Neural Network","Attention","Semi-supervised"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}