{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222703097,"tcdate":1511820027676,"number":3,"cdate":1511820027676,"id":"rkzZoW5xf","invitation":"ICLR.cc/2018/Conference/-/Paper623/Official_Review","forum":"H1BLjgZCb","replyto":"H1BLjgZCb","signatures":["ICLR.cc/2018/Conference/Paper623/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting paper which is marginally above acceptance threshold","rating":"6: Marginally above acceptance threshold","review":"The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space). The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\\gamma}, which can be regarded as the inverse function of the original GAN G_{\\theta}, is trained to learn a map from the original input data space to the latent z-space. The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input).\n\nGenerally, I think that the paper is written well (except some issues listed at the end). The intuition of the proposed approach is clearly explained and it seems very reasonable to me.  \nMy main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper. The efficiency of such a search method decreases very fast when the dimensions of the z-space increases. Furthermore, such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation (3). This makes me feel that there is large room to further advance the paper. Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method (such as FGSM) in generating the adversaries. The example in Table 1 is very good; but more examples (especially involving the quantitative comparison) are needed to demonstrate the claimed advantages. For example, could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural? \n\nOther issues are listed as follows:\n(1). Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3?\n(2). In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases. Could you still say that your generated “adversaries” leading to the wrong decision from LeNet? Are these really “adversaries”?\n(3). How do you choose the parameter \\lambda in Equation (2)?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1512222703136,"tcdate":1511741716016,"number":2,"cdate":1511741716016,"id":"By2zFR_gz","invitation":"ICLR.cc/2018/Conference/-/Paper623/Official_Review","forum":"H1BLjgZCb","replyto":"H1BLjgZCb","signatures":["ICLR.cc/2018/Conference/Paper623/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors present an interesting research problem of generating adversarial examples to show differences in predictions in black-box classifiers. However, I feel the novelty of the perturbation idea in semantic space is questionable and author needs to highlight the significance in a more explicit way. ","rating":"4: Ok but not good enough - rejection","review":"Quality: Although the research problem is an interesting direction the quality of the work is not of a high standard. My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way. How different it will be compared to a perturbation in an input space? \n\nClarity: The use of the term \"adversarial\" is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label (e.g. from \"church\" to \"tower\" or vice-versa)\n\nOriginality: The generation of adversarial examples in black-box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too. What is the main benefit of the proposed mechanism compared to the existing ones?\n\nSignificance: The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail.\n\npros:\n(a) An interesting problem to evaluate the robustness of black-box classifier systems\n(b) generating adversarial examples for image classification as well as text analysis.\n(c) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples.\n\ncons:\n(a) The proposed search algorithm in the semantic latent space could be computationally intensive. any remedy for this problem?\n(b) Searching in the latent space z could be strongly dependent on the matching inverter $I_\\gamma(.)$. any comment on this?\n(c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1512222703173,"tcdate":1511698957920,"number":1,"cdate":1511698957920,"id":"HJLfGN_xM","invitation":"ICLR.cc/2018/Conference/-/Paper623/Official_Review","forum":"H1BLjgZCb","replyto":"H1BLjgZCb","signatures":["ICLR.cc/2018/Conference/Paper623/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A novel idea for generating more useful adversary examples. ","rating":"7: Good paper, accept","review":"\nSummary:\n A method for creation of semantical adversary examples in suggested. The ‘semantic’ property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter). The generator is trained with a WGAN optimization. Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space. The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences. It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.\nPage 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated. This should be explicitly said and the parameter value should be stated.\nPage 4: \n-\tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers. This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described.\n-\tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier. In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier). In the churce-vs. tower case, a  relatively weak MLP classifier was used. It would be more instructive to see the results for a better, convolutional classifier.\nPage 5:\n-\tthe description of the various networks used for text generation is insufficient for understanding:\no\tThe AREA is described in two sentences. It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task\no\t ‘inverter’ here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces.\no\t It is not clear what the ‘four-layers strided CNN’ is: its structure, its role in the system. How is it optimized?\no\tIn general: a block diagram showing the relation between all the system’s components may be useful, plus the details about the structure and optimization of the various modules. It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough. Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.\no\tSNLI data should be described: content, size, the task it is used for\n\n\nPro:\n-\tA novel idea of producing natural adversary examples with a GAN\n-\tThe generated examples are in some cases useful for interpretation and network understanding \n-\tThe method enables creation of adversarial examples for block box classifiers\nCons\n-\tThe idea implementation is basic. Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested\n-\tThe generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier). Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1510344422543,"tcdate":1510344422543,"number":2,"cdate":1510344422543,"id":"HJR1wYX1z","invitation":"ICLR.cc/2018/Conference/-/Paper623/Public_Comment","forum":"H1BLjgZCb","replyto":"Bk8fG911z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Adversarial and GAN Literature","comment":"I asked the question because you have used the term \"adversarial\" and my point was, if your method changes both the image and the label, it may not be suitable to call the modified image as adversarial. \n\nMoreover, I think the concept of slowly transiting from images of one class to another has been widely studied and examined in GAN literature."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1510092431369,"tcdate":1510085134375,"number":1,"cdate":1510085134375,"id":"Bk8fG911z","invitation":"ICLR.cc/2018/Conference/-/Paper623/Official_Comment","forum":"H1BLjgZCb","replyto":"HyF4SB6A-","signatures":["ICLR.cc/2018/Conference/Paper623/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper623/Authors"],"content":{"title":"Reply to \"Adversarial example belonging to a different class?\"","comment":"We are glad that the commenter finds the idea interesting. Natural adversarial examples are defined differently here from the conventional adversaries, where one is searching for minimal adversarial change to the input directly. Our objective is to find the minimal amount of semantic change to the input that results in different prediction in order to interpret the decision behavior of the classifier. Indeed, while the change in semantic space may sometimes be sufficiently substantial to make the generated sample actually end up in a different class, the sample is still generated from the minimal semantic change (not just some random sample of a different class), and the way in which it differs from the original input can provide useful insights into the classifier."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1509958823624,"tcdate":1509934384572,"number":1,"cdate":1509934384572,"id":"HyF4SB6A-","invitation":"ICLR.cc/2018/Conference/-/Paper623/Public_Comment","forum":"H1BLjgZCb","replyto":"H1BLjgZCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Adversarial example belonging to a different class?","comment":"Interesting research direction. However, considering Table 2, I was wondering if we take an image of \"tower\" and semantically change it to an image of \"church,\" then how do we expect the classifier to classify it as \"tower\"? In essence, the adversarial example must belong to the same class as the original image, otherwise one can completely replace the original image with a new one."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]}},{"tddate":null,"ddate":null,"tmdate":1509739195779,"tcdate":1509129036958,"number":623,"cdate":1509739193117,"id":"H1BLjgZCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1BLjgZCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generating Natural Adversarial Examples","abstract":"Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers in a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.","pdf":"/pdf/85c1c905b123f99d0a409ce4de3b9edcaae0febf.pdf","TL;DR":"We propose a framework to generate “natural” adversaries against black-box classifiers for both visual and textual domains, by doing the search for adversaries in the latent semantic space.","paperhash":"anonymous|generating_natural_adversarial_examples","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Natural Adversarial Examples},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1BLjgZCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper623/Authors"],"keywords":["adversarial examples","generative adversarial networks","interpretability","image classification","textual entailment","machine translation"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}