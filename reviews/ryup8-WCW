{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222719451,"tcdate":1511949918452,"number":3,"cdate":1511949918452,"id":"B1IwI-2xz","invitation":"ICLR.cc/2018/Conference/-/Paper683/Official_Review","forum":"ryup8-WCW","replyto":"ryup8-WCW","signatures":["ICLR.cc/2018/Conference/Paper683/AnonReviewer2"],"readers":["everyone"],"content":{"title":"ICLR 2018 official review (Reviewer 2)","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. Taking the full dimensionality to be the total number of parameters of the network model, the authors assess intrinsic dimensionality by randomly projecting the network to a domain with fewer parameters (corresponding to a low-dimensional subspace within the original parameter), and then training the original network while restricting the projections of its parameters to lie within this subspace. Performance on this subspace is then evaluated relative to that over the full parameter space (the baseline). As an empirical standard, the authors focus on the subspace dimension that achieves a performance of 90% of the baseline. The authors then test out their measure of intrinsic dimensionality for fully-connected networks and convolutional networks, for several well-known datasets, and draw some interesting conclusions.\n\nPros:\n\n* This paper continues the recent research trend towards a better characterization of neural networks and their performance. The authors show a good awareness of the recent literature, and to the best of my knowledge, their empirical characterization of the number of latent parameters is original. \n\n* The characterization of the number of latent variables is an important one, and their measure does perform in a way that one would intuitively expect. For example, as reported by the authors, when training a fully-connected network on the MNIST image dataset, shuffling pixels does not result in a change in their intrinsic dimensionality. For a convolutional network the observed 3-fold rise in intrinsic dimension is explained by the authors as due to the need to accomplish the classification task while respecting the structural constraints of the convnet.\n\n* The proposed measures seem very practical - training on random projections uses far fewer parameters than in the original space (the baseline), and presumably the cost of determining the intrinsic dimensionality would presumably be only a fraction of the cost of this baseline training.\n\n* Except for the occasional typo or grammatical error, the paper is well-written and organized. The issues are clearly identified, for the most part (but see below...).\n\nCons:\n\n* In the main paper, the authors perform experiments and draw conclusions without taking into account the variability of performance across different random projections. Variance should be taken into account explicitly, in presenting experimental results and in the definition and analysis of the empirical intrinsic dimension itself. How often does a random projection lead to a high-quality solution, and how often does it not?\n\n* The authors are careful to point out that training in restricted subspaces cannot lead to an optimal solution for the full parameter domain unless the subspace intersects the optimal solution region (which in general cannot be guaranteed). In their experiments (FC networks of varying depths and layer widths for the MNIST dataset), between projected and original solutions achieving 90% of baseline performance, they find an order of magnitude gap in the number of parameters needed. This calls into question the validity of random projection as an empirical means of categorizing the intrinsic dimensionality of a neural network.\n\n* The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension. However, I don't think that they make a convincing case for this approach. Again, variation is the difficulty: two different projective subspaces of the same dimensionality can lead to solutions that are extremely different in character or quality. How then can we be sure that our compressed network can be reconstituted into a solution of reasonable quality, even when its dimensionality greatly exceeds the intrinsic dimension?\n\n* The authors argue for a relationship between intrinsic dimensionality and the minimum description length (MDL) of their solution, in that the intrinsic dimensionality should serve as an upper bound on the MDL. However they don't formally acknowledge that there is no standard relationship between the number of parameters and the actual number of bits needed to represent the model - it varies from setting to setting, with some parameters potentially requiring many more bits than others. And given this uncertain connection, and given the lack of consideration given to variation in the proposed measure of intrinsic dimensionality, it is hard to accept that \"there is some rigor behind\" their conclusion that LeNet is better than FC networks for classification on MNIST because its empirical intrinsic dimensionality score is lower.\n\n* The experimental validation of their measure of intrinsic dimension could be made more extensive. In the main paper, they use three image datasets - MNIST, CIFAR-10 and ImageNet. In the supplemental information, they report intrinsic dimensions for reinforcement learning and other training tasks on four other sets.\n\nOverall, I think that this characterization does have the potention to give insights into the performance of neural networks, provided that variation across projections is properly taken into account. For now, more work is needed.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Measuring the Intrinsic Dimension of Objective Landscapes","abstract":"Many recently trained neural networks employ tens or hundreds of  millions of parameters to achieve good performance. Researchers may  intuitively use the number of parameters required as a rough gauge   of the difficulty of a problem. But how accurate are such notions?   How many parameters are really needed?  In this paper we attempt to   answer this question by training networks not in their native   parameter space, but instead in smaller, randomly oriented  subspaces. By slowly increasing the dimension of this subspace, we   note when solutions first appear and define this to be the  intrinsic dimension of the problem. A few suggestive  conclusions result. Many problems have smaller intrinsic dimension   than one might suspect, and the intrinsic dimension for a given  dataset varies little across a family of models with vastly   different sizes. Intrinsic dimension allows some quantitative  problem comparison across supervised, reinforcement, and other types   of learning where we conclude, for example, that solving the   cart-pole RL problem is in a sense 100 times easier than classifying digits from MNIST.  In addition to providing new cartography of the objective  landscapes wandered by parameterized models, the results encompass a  simple method for constructively obtaining an upper bound on the  minimum description length of a solution, leading in some cases to  very compressible networks.\n","pdf":"/pdf/ed9d15ce5428fcd17e1e17c0d13eca2a76287bda.pdf","TL;DR":"We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.","paperhash":"anonymous|measuring_the_intrinsic_dimension_of_objective_landscapes","_bibtex":"@article{\n  anonymous2018measuring,\n  title={Measuring the Intrinsic Dimension of Objective Landscapes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryup8-WCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper683/Authors"],"keywords":["machine learning","neural networks","intrinsic dimension","random subspace","model understanding"]}},{"tddate":null,"ddate":null,"tmdate":1512222719491,"tcdate":1511685567247,"number":2,"cdate":1511685567247,"id":"BJva6gOgM","invitation":"ICLR.cc/2018/Conference/-/Paper683/Official_Review","forum":"ryup8-WCW","replyto":"ryup8-WCW","signatures":["ICLR.cc/2018/Conference/Paper683/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An proposal that reduces the degree of freedom in deep learning ","rating":"6: Marginally above acceptance threshold","review":"While deep learning usually involves estimating a large number of variable, this paper suggests to reduce its number by assuming that these variable lie in a low-dimensional subspace. In practice, this subspace is chosen randomly. Simulations show the promise of the proposed method.  In particular, figure 2 shows that the number of parameters could be greatly reduced while keeping 90% of the performance; and figure 4 shows that this method outperforms the standard method. The method is clearly written and the idea looks original. \n\nA con that I have is about the comparison in figure 4. While the proposed subspace method might have the same number of parameters as the direct method, I wonder if it is a fair comparison since the subspace method could still be more computational expensive, due to larger number of latent variables.\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Measuring the Intrinsic Dimension of Objective Landscapes","abstract":"Many recently trained neural networks employ tens or hundreds of  millions of parameters to achieve good performance. Researchers may  intuitively use the number of parameters required as a rough gauge   of the difficulty of a problem. But how accurate are such notions?   How many parameters are really needed?  In this paper we attempt to   answer this question by training networks not in their native   parameter space, but instead in smaller, randomly oriented  subspaces. By slowly increasing the dimension of this subspace, we   note when solutions first appear and define this to be the  intrinsic dimension of the problem. A few suggestive  conclusions result. Many problems have smaller intrinsic dimension   than one might suspect, and the intrinsic dimension for a given  dataset varies little across a family of models with vastly   different sizes. Intrinsic dimension allows some quantitative  problem comparison across supervised, reinforcement, and other types   of learning where we conclude, for example, that solving the   cart-pole RL problem is in a sense 100 times easier than classifying digits from MNIST.  In addition to providing new cartography of the objective  landscapes wandered by parameterized models, the results encompass a  simple method for constructively obtaining an upper bound on the  minimum description length of a solution, leading in some cases to  very compressible networks.\n","pdf":"/pdf/ed9d15ce5428fcd17e1e17c0d13eca2a76287bda.pdf","TL;DR":"We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.","paperhash":"anonymous|measuring_the_intrinsic_dimension_of_objective_landscapes","_bibtex":"@article{\n  anonymous2018measuring,\n  title={Measuring the Intrinsic Dimension of Objective Landscapes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryup8-WCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper683/Authors"],"keywords":["machine learning","neural networks","intrinsic dimension","random subspace","model understanding"]}},{"tddate":null,"ddate":null,"tmdate":1512222719531,"tcdate":1511666326601,"number":1,"cdate":1511666326601,"id":"BkJsM2vgf","invitation":"ICLR.cc/2018/Conference/-/Paper683/Official_Review","forum":"ryup8-WCW","replyto":"ryup8-WCW","signatures":["ICLR.cc/2018/Conference/Paper683/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper","rating":"7: Good paper, accept","review":"Authors introduce ransom subspace training (random subspace neural nets) where for a fixed architecture, only a subset of the parameters is trained, and the update for all the parameters is derived via random projection which is fixed for the duration of the training. Using this type of a network, authors introduce a notion of intrinsic dimension of optimization problems - it is minimal dimension of a subset, for which random subset neural net already reaches best (or comparable) performance.\nAuthors mention that this can be used for compressing networks - one would need to store the seed for the random matrix and the # of params equal to the intrinsic dimension of the net. \nThey then demonstrate that the intrinsic dimension for the same problem stays the same when different architectures are chosen. Finally they mention neural nets with comparable number of params to intrinsic dimension but that don’t use random subspace trick don’t achieve comparable performance. This does not always hold for CNNs\nModel with smaller intrinsic dimension is suggested to be better . They also suggest that intrinsic dimension might be a good approximation to Minimum Description Length metric\n\nMy main concern is computational efficiency. They state that if used for compressing, their method  is different from post-train compression, and the authors state that they train once end-to-end. It is indeed the case that once they found model that performs well, it is easy to compress, however they do train a number of models (up to a number of intrinsic dimension) until they get to this admissible model, which i envision would be computationally very expensive.\n\nQuestions:\n- Are covets always better on MNIST: didn’t understand when authors said that intrinsic dimension of FC on shuffled data stayed the same (why) and then say that it becomes 190K - which one is correct?\n- MNIST - state the input dimension size, not clear how you got to that number of parameters overall\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Measuring the Intrinsic Dimension of Objective Landscapes","abstract":"Many recently trained neural networks employ tens or hundreds of  millions of parameters to achieve good performance. Researchers may  intuitively use the number of parameters required as a rough gauge   of the difficulty of a problem. But how accurate are such notions?   How many parameters are really needed?  In this paper we attempt to   answer this question by training networks not in their native   parameter space, but instead in smaller, randomly oriented  subspaces. By slowly increasing the dimension of this subspace, we   note when solutions first appear and define this to be the  intrinsic dimension of the problem. A few suggestive  conclusions result. Many problems have smaller intrinsic dimension   than one might suspect, and the intrinsic dimension for a given  dataset varies little across a family of models with vastly   different sizes. Intrinsic dimension allows some quantitative  problem comparison across supervised, reinforcement, and other types   of learning where we conclude, for example, that solving the   cart-pole RL problem is in a sense 100 times easier than classifying digits from MNIST.  In addition to providing new cartography of the objective  landscapes wandered by parameterized models, the results encompass a  simple method for constructively obtaining an upper bound on the  minimum description length of a solution, leading in some cases to  very compressible networks.\n","pdf":"/pdf/ed9d15ce5428fcd17e1e17c0d13eca2a76287bda.pdf","TL;DR":"We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.","paperhash":"anonymous|measuring_the_intrinsic_dimension_of_objective_landscapes","_bibtex":"@article{\n  anonymous2018measuring,\n  title={Measuring the Intrinsic Dimension of Objective Landscapes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryup8-WCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper683/Authors"],"keywords":["machine learning","neural networks","intrinsic dimension","random subspace","model understanding"]}},{"tddate":null,"ddate":null,"tmdate":1509739161588,"tcdate":1509131967888,"number":683,"cdate":1509739158921,"id":"ryup8-WCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryup8-WCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Measuring the Intrinsic Dimension of Objective Landscapes","abstract":"Many recently trained neural networks employ tens or hundreds of  millions of parameters to achieve good performance. Researchers may  intuitively use the number of parameters required as a rough gauge   of the difficulty of a problem. But how accurate are such notions?   How many parameters are really needed?  In this paper we attempt to   answer this question by training networks not in their native   parameter space, but instead in smaller, randomly oriented  subspaces. By slowly increasing the dimension of this subspace, we   note when solutions first appear and define this to be the  intrinsic dimension of the problem. A few suggestive  conclusions result. Many problems have smaller intrinsic dimension   than one might suspect, and the intrinsic dimension for a given  dataset varies little across a family of models with vastly   different sizes. Intrinsic dimension allows some quantitative  problem comparison across supervised, reinforcement, and other types   of learning where we conclude, for example, that solving the   cart-pole RL problem is in a sense 100 times easier than classifying digits from MNIST.  In addition to providing new cartography of the objective  landscapes wandered by parameterized models, the results encompass a  simple method for constructively obtaining an upper bound on the  minimum description length of a solution, leading in some cases to  very compressible networks.\n","pdf":"/pdf/ed9d15ce5428fcd17e1e17c0d13eca2a76287bda.pdf","TL;DR":"We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.","paperhash":"anonymous|measuring_the_intrinsic_dimension_of_objective_landscapes","_bibtex":"@article{\n  anonymous2018measuring,\n  title={Measuring the Intrinsic Dimension of Objective Landscapes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryup8-WCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper683/Authors"],"keywords":["machine learning","neural networks","intrinsic dimension","random subspace","model understanding"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}