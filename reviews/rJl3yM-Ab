{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222744501,"tcdate":1511983312489,"number":3,"cdate":1511983312489,"id":"S1OAdY3eG","invitation":"ICLR.cc/2018/Conference/-/Paper758/Official_Review","forum":"rJl3yM-Ab","replyto":"rJl3yM-Ab","signatures":["ICLR.cc/2018/Conference/Paper758/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Incremental idea but with solid results","rating":"6: Marginally above acceptance threshold","review":"Traditional open-domain QA systems typically have two steps: passage retrieval and aggregating answers extracted from the retrieved passages.  This paper essentially follows the same paradigm, but leverages the state-of-the-art reading comprehension models for answer extraction, and develops the neural network models for the aggregating component.  Although the idea seems incremental, the experimental results do seem solid.  The paper is generally easy to follow, but in several places the presentation can be further improved.\n\nDetailed comments/questions:\n  1. In Sec. 2.2, the justification for adding H^{aq} and \\bar{H}^{aq} is to downweigh the impact of stop word matching.  I feel this is a somewhat indirect and less effective design, if avoiding stop words is really the reason.  A standard preprocessing step may be better.\n  2. In Sec. 2.3, it seems that the final score is just the sum of three individual normalized scores. It's not truly a \"weighted\" combination, where the weights are typically assumed to be tuned.\n  3. Figure 3: Connecting the dots in the two subfigures on the right does not make sense.  Bar charts should be used instead.\n  4. The end of Sec. 4.2: I feel it's a bad example, as the passage does not really support the answer. The fact that \"Sesame Street\" got picked is probably just because it's more famous.\n  5. It'd be interesting to see how traditional IR answer aggregation methods perform, such as simple classifiers or heuristics by word matching (or weighted by TFIDF) and counting.  This will demonstrates the true advantages of leveraging modern NN models.\n\nPros:\n  1. Updating a traditional open-domain QA approach with neural models\n  2. Experiments demonstrate solid positive results\n\nCons:\n  1. The idea seems incremental\n  2. Presentation could be improved\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering","abstract":"Very recently, it comes to be a popular approach for answering open-domain questions by first searching question-related passages, then applying reading comprehension models to extract answers. Existing works usually extract answers from single passages independently, thus not fully make use of the multiple searched passages, especially for the some questions requiring several evidences, which can appear in different passages, to be answered. The above observations raise the problem of evidence aggregation from multiple passages. In this paper, we deal with this problem as answer re-ranking. Specifically, based on the answer candidates generated from the existing state-of-the-art QA model, we propose two different re-ranking methods, strength-based and coverage-based re-rankers, which make use of the aggregated evidences from different passages to help entail the ground-truth answer for the question. Our model achieved state-of-the-arts on three public open-domain QA datasets, Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8\\% improvement on the former two datasets. ","pdf":"/pdf/9899f7ffab89be3ab471e2000f8848fc033b5537.pdf","TL;DR":"We propose a method that can make use of the multiple passages information for open-domain QA.","paperhash":"anonymous|evidence_aggregation_for_answer_reranking_in_opendomain_question_answering","_bibtex":"@article{\n  anonymous2018evidence,\n  title={Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl3yM-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper758/Authors"],"keywords":["Question Answering","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222744540,"tcdate":1511800088887,"number":2,"cdate":1511800088887,"id":"rJZQa3YgG","invitation":"ICLR.cc/2018/Conference/-/Paper758/Official_Review","forum":"rJl3yM-Ab","replyto":"rJl3yM-Ab","signatures":["ICLR.cc/2018/Conference/Paper758/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Very good contribution on multi-sentences answer reranking with significant experimental results.","rating":"8: Top 50% of accepted papers, clear accept","review":"The authors propose an approach where they aggregate, for each candidate answer, text from supporting passages. They make use of two ranking components. A strength-based re-ranker captures how often a candidate answer would be selected while a coverage-based re-ranker aims to estimate the coverage of the question by the supporting passages. Potential answers are extracted using a machine comprehension model. A bi-LSTM model is used to estimate the coverage of the question. A weighted combination of the outputs of both components generates the final ranking (using softmax). \nThis article is really well written and clearly describes the proposed scheme. Their experiments clearly indicate that the combination of the two re-ranking components outperforms raw machine comprehension approaches. The paper also provides an interesting analysis of various design issues. Finally they situate the contribution with respect to some related work pertaining to open domain QA. This paper seems to me like an interesting and significant contribution.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering","abstract":"Very recently, it comes to be a popular approach for answering open-domain questions by first searching question-related passages, then applying reading comprehension models to extract answers. Existing works usually extract answers from single passages independently, thus not fully make use of the multiple searched passages, especially for the some questions requiring several evidences, which can appear in different passages, to be answered. The above observations raise the problem of evidence aggregation from multiple passages. In this paper, we deal with this problem as answer re-ranking. Specifically, based on the answer candidates generated from the existing state-of-the-art QA model, we propose two different re-ranking methods, strength-based and coverage-based re-rankers, which make use of the aggregated evidences from different passages to help entail the ground-truth answer for the question. Our model achieved state-of-the-arts on three public open-domain QA datasets, Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8\\% improvement on the former two datasets. ","pdf":"/pdf/9899f7ffab89be3ab471e2000f8848fc033b5537.pdf","TL;DR":"We propose a method that can make use of the multiple passages information for open-domain QA.","paperhash":"anonymous|evidence_aggregation_for_answer_reranking_in_opendomain_question_answering","_bibtex":"@article{\n  anonymous2018evidence,\n  title={Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl3yM-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper758/Authors"],"keywords":["Question Answering","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222744577,"tcdate":1511724501521,"number":1,"cdate":1511724501521,"id":"H1pRH5def","invitation":"ICLR.cc/2018/Conference/-/Paper758/Official_Review","forum":"rJl3yM-Ab","replyto":"rJl3yM-Ab","signatures":["ICLR.cc/2018/Conference/Paper758/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is a neural-based approach for improving QA systems by aggregating answers from multiple passages.","rating":"6: Marginally above acceptance threshold","review":"The paper is clear, although there are many English mistakes (that should be corrected).\nThe proposed method aggregates answers from multiple passages in the context of QA. The new method is motivated well and departs from prior work. Experiments on three datasets show the proposed method to be notably better than several baselines (although two of the baselines, GA and BiDAF, appear tremendously weak). The analysis of the results is interesting and largely convincing, although a more dedicated error analysis or discussion of the limitation of the proposed approach would be welcome.\n\nMinor point: in the description of Quasar-T, the IR model is described as lucene index. An index is not an IR model. Lucene is an IR system that implements various IR models. The terminology should be corrected here. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering","abstract":"Very recently, it comes to be a popular approach for answering open-domain questions by first searching question-related passages, then applying reading comprehension models to extract answers. Existing works usually extract answers from single passages independently, thus not fully make use of the multiple searched passages, especially for the some questions requiring several evidences, which can appear in different passages, to be answered. The above observations raise the problem of evidence aggregation from multiple passages. In this paper, we deal with this problem as answer re-ranking. Specifically, based on the answer candidates generated from the existing state-of-the-art QA model, we propose two different re-ranking methods, strength-based and coverage-based re-rankers, which make use of the aggregated evidences from different passages to help entail the ground-truth answer for the question. Our model achieved state-of-the-arts on three public open-domain QA datasets, Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8\\% improvement on the former two datasets. ","pdf":"/pdf/9899f7ffab89be3ab471e2000f8848fc033b5537.pdf","TL;DR":"We propose a method that can make use of the multiple passages information for open-domain QA.","paperhash":"anonymous|evidence_aggregation_for_answer_reranking_in_opendomain_question_answering","_bibtex":"@article{\n  anonymous2018evidence,\n  title={Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl3yM-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper758/Authors"],"keywords":["Question Answering","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739119109,"tcdate":1509134247801,"number":758,"cdate":1509739116442,"id":"rJl3yM-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJl3yM-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering","abstract":"Very recently, it comes to be a popular approach for answering open-domain questions by first searching question-related passages, then applying reading comprehension models to extract answers. Existing works usually extract answers from single passages independently, thus not fully make use of the multiple searched passages, especially for the some questions requiring several evidences, which can appear in different passages, to be answered. The above observations raise the problem of evidence aggregation from multiple passages. In this paper, we deal with this problem as answer re-ranking. Specifically, based on the answer candidates generated from the existing state-of-the-art QA model, we propose two different re-ranking methods, strength-based and coverage-based re-rankers, which make use of the aggregated evidences from different passages to help entail the ground-truth answer for the question. Our model achieved state-of-the-arts on three public open-domain QA datasets, Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8\\% improvement on the former two datasets. ","pdf":"/pdf/9899f7ffab89be3ab471e2000f8848fc033b5537.pdf","TL;DR":"We propose a method that can make use of the multiple passages information for open-domain QA.","paperhash":"anonymous|evidence_aggregation_for_answer_reranking_in_opendomain_question_answering","_bibtex":"@article{\n  anonymous2018evidence,\n  title={Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJl3yM-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper758/Authors"],"keywords":["Question Answering","Deep Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}