{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222544552,"tcdate":1512120713388,"number":3,"cdate":1512120713388,"id":"BkbcZjAgM","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Review","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["ICLR.cc/2018/Conference/Paper1049/AnonReviewer2"],"readers":["everyone"],"content":{"title":"RNNs decode convolutional codes very well","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper shows how RNNs can be used to decode convolutional error correcting codes. While previous recent work has shown neural decoders for block codes results had limited success and for small block lengths. \nThis paper shows that RNNs are very suitable for convolutional codes and achieves state of the art performance for the first time. \nThe second contribution is on adaptivity outside the AWGN noise model. The authors show that their decoder performs well for different noise statistics outside what it was trained on. This is very interesting and encouraging. It was not very clear to me if the baseline decoders (Turbo/BCJR) are fairly compared here since better decoders may be used for the different statistics, or some adaptivity could be used in standard decoders in various natural ways. \n\nThe last part goes further in designing new error correcting schemes using RNN encoders and decoders for noisy feedback communication. \nFor this case capacity is known to be impossible to improve, but the bit error error can be improved for finite lenghts. \nIt seems quite remarkable that they beat Schalkwijk and Kailath and shows great promise for other communication problems.\n\nThe paper is very well written with good historical context and great empirical results. I think it opens a new area for information theory and communications with new tools. \n\nMy only concern is that perhaps the neural decoders can be attacked with adversarial noise (which would not be possible for good-old Viterbi ). It would be interesting to discuss this briefly. \nA second (related) concern is the lack of theoretical understanding of these new decoders. It would be nice if we could prove something about them, but of course this will probably be challenging. \n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1512222544591,"tcdate":1511849022677,"number":2,"cdate":1511849022677,"id":"S1PB3Ocef","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Review","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["ICLR.cc/2018/Conference/Paper1049/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting paper that brings in the tools of recursive neural networks to error-correcting codes for communication ","rating":"6: Marginally above acceptance threshold","review":"Error-correcting codes constitute a well-researched area of study within communication engineering. In communication, messages that are to be transmitted are encoded into binary vector called codewords that contained some redundancy. The codewords are then transmitted over a channel that has some random noise. At the receiving end the noisy codewords are then decoded to recover the messages. Many well known families of codes exist, notably convolutional codes and Turbo codes, two code families that are central to this paper, that achieve the near optimal possible performance with efficient algorithms. For Turbo and convolutional codes the efficient MAP decodings are known as Viterbi decoder and the BCJR decoder. For drawing baselines, it is assumed that the random noise in channel is additive Gaussian (AWGN).\n\nThis paper makes two contributions. First, recurrent neural networks (RNN) are proposed to replace the Viterbi and BCJR algorithms for decoding of convolutional and Turbo decoders. These decoders are robust to changes in noise model and blocklength - and shows near optimal performance.\n\nIt is unclear to me what is the advantage of using RNNs instead of Viterbi or BCJR, both of which are optimal, iterative and runs in linear time. Moreover the authors point out that RNNs are shown to emulate BCJR and Viterbi decodings in prior works - in light of that, why their good performance surprising?\n\nThe second contribution of the paper constitutes the design and decoding of codes based on RNNs for a Gaussian channel with noisy feedback. For this channel the optimal codes are unknown. The authors propose an architecture to design codes for this channel. This is a nice step. However, in the performance plot (figure 8), the RNN based code-decoder does not seem to be outperforming the existing codes except for two points. For both in high and low SNR the performance is suboptimal to  Turbo codes and a code by Schalkwijk & Kailath. The section is also super-concise to follow. I think it was necessary to introduce an LSTM encoder - it was hard to understand the overall encoder. This is an issue with the paper - the authors previously mentioned (8,16) polar code without mentioning what the numbers mean. \n\nHowever, I overall liked the idea of using neural nets to design codes for some non-standard channels. While at the decoding end it does not bring in anything new (modern coding theory already relies on iterative decoders, that are super fast), at the designing-end the Gaussian feedback channel part can be a new direction. This paper lacks theoretical aspect, as to no indication is given why RNN based design/decoders can be good. I am mostly satisfied with the experiments, barring Fig 8, which does not show the results that the authors claim.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1512222544635,"tcdate":1511798828292,"number":1,"cdate":1511798828292,"id":"ByEN_hFgM","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Review","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["ICLR.cc/2018/Conference/Paper1049/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Deep learning for channel coding ","rating":"4: Ok but not good enough - rejection","review":"In this paper the authors propose to use RNNs and LSTMs for channel coding. But I have the impression the authors completely miss the state of the art in channel coding and the results are completely useless for any current communication system. I believe that machine learning, in general, and deep learning, in particular, might be of useful for physical layer communications. I just do not see why it would be useful for channel coding over the AWGN channel. Let me explain.\n\nIf the decoder knows that the encoder is using a convolutional code, why does it need to learn the decoder instead of using the Viterbi or BCJR algorithms that are known to be optimal for sequences and symbols, respectively. I cannot imagine an scenario in which the decoder does not know the convolutional code that it is being used and the encoder sends 120,000 bits of training sequence (useless bits from information standpoint) for the decoder to learn it. More important question, do the authors envision that this learning is done every time there is a new connection or it is learnt once and for all. If it is learnt every time that would be ideal if we were discovering new channel codes everyday, clearly not the case. If we learnt it one and for all and then we incorporated in the standard that would only make sense if the GRU structure was computationally better than the BCJR or Viterbi. I would be surprise if it is. If instead of using 2 or 3 memories, we used 6-8 does 120,000 bits be good enough or we need to exponentially increase the training sequence? So the first result in the paper shows that a tailored structure for convolutional encoding can learn to decode it. Basically, the authors are solving a problem that does not need solving. \n\nFor the Turbocodes the same principle as before applies. In this case the comments of the authors really show that they do not know anything about coding. In Page 6, we can read: “Unlike the convolutional codes, the state of the art (message-passing) decoders for turbo codes are not the corresponding MAP decoders, so there is no contradiction in that our neural decoder would beat the message-passing ones”. This is so true, so I expected the DNN structure to be significantly better than turbodecoding. But actually, they do not. These results are in Figure 15 page 6 and the solution for the turbo decoders and the DNN architecture are equivalent. I am sure that the differences in the plots can be explained by the variability in the received sequence and not because the DNN is superior to the turbodecoder. Also in this case the training sequence is measured in the megabits for extremely simple components. If the convolutional encoders were larger 6-8 bits, we would be talking about significantly longer training sequences and more complicated NNs.\n\nIn the third set the NNs seems to be superior to the standard methods when burst-y noise is used, but the authors seems to indicate that that NN is trained with more information about these bursts that the other methods do not have. My impression is that the authors would be better of focusing on this example and explain it in a way that it is reproducible. This experiment is clearly not well explained and it is hard to know if there is any merit for the proposed NN structure. \n\nFinally, the last result would be the more interesting one, because it would show that we can learn a better channel coding and decoding mechanism that the ones humans have been able to come up with. In this sense, if NNs can solve this problem that would be impressive and would turn around how channel coding is done nowadays. If this result were good enough, the authors should only focus in it and forget about the other 3 cases. The issue with this result is that it actually does not make sense. The main problem with the procedure is that the feedback proposal is unrealistic, this is easy to see in Figure 16 in which the neural encoder is proposed. It basically assumes that the received real-valued y_k can be sent (almost) noiselessly to the encoder with minimal delay and almost instantaneously. So the encoder knows the received noise and is able to cancel it out. Even if this procedure could be implemented, which it cannot be. The code only uses 50 bits and it needed 10^7 iterations (500Mbs) to converge. The authors do not show how far they are from the Shannon limit, but I can imagine that with 50 bit code, it should be pretty far.  \n\nWe know that with long enough LDPC codes we can (almost) reach the Shannon limit, so new structure are not needed. If we are focusing on shorter codes (e.g. latency?) then it will be good to understand why do we need to learn the channel codes. A comparison to the state of the art would be needed. Because clearly the used codes are not close to state of the art. For me the authors either do not know about coding or are assuming that we do not, which explains part of the tone of this review. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510947365031,"tcdate":1510947365031,"number":4,"cdate":1510947365031,"id":"SJTmc33JG","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Comment","forum":"ryazCMbR-","replyto":"B1krcrCRZ","signatures":["ICLR.cc/2018/Conference/Paper1049/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1049/Authors"],"content":{"title":"Re: OTHER NEURAL NETWORK ARCHITECTURES FOR N-RSC AND N-BCJR","comment":"Tables 1 and 2 are for the code of Fig. 1. \n\nRe codes with state dimension higher than 2 (or 3):\n\nFirst note that the Viterbi/BCJR decoder's computational complexity increases exponentially in the (memory) state dimension. While we have not tried a careful analysis on the complexity of neural decoders for codes with state dimension higher than 3, we observe the following: from dimension 2 to 3, we increased the size of neural decoder - from 2 layered bi-GRUs (200 hidden nodes) to 2 layered bi-LSTMs (400 hidden nodes). It seems that the network has to scale as the state dimension increases, but we don't have a good guess of in what order it will scale. \n\nOn a related note: ​Modern coding theory recommends improving the 'quality' of the convolutional code not by increasing the memory state dimension, but via the 'turbo effect'.  The advantage of turbo codes over convolutional codes is that ​it uses convolutional codes with a short memory as the constituent codes, but the interleaver allows for very long range memory, that is naturally decoded via iterative methods. The end result is that turbo codes can be decoded with a far lesser decoding complexity than convolutional codes with a very long memory, for the same BER performance. Indeed, turbo codes have largely replaced convolutional codes  in modern practice. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510268090408,"tcdate":1510268090408,"number":3,"cdate":1510268090408,"id":"Syfah8z1z","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Comment","forum":"ryazCMbR-","replyto":"BJG7lOZJz","signatures":["ICLR.cc/2018/Conference/Paper1049/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1049/Authors"],"content":{"title":"Re: \"Leaning models\" for \"deterministic functions\"?","comment":"Thanks for your interest. \n\n1) Viterbi algorithm is not a simple deterministic function but an algorithm to find the shortest path on a directed graph (defined by the encoding structure as well as the number of information bits) with non-negative weights on edges (defined by the Gaussian noise samples).  In other words, it is the Dijkstra's shortest path algorithm (dynamic programming) on a specific graph that changes instance-by-instance (since the noise samples and number of information bits vary). While it is conceivable that Viterbi algorithm can be represented with a high capacity neural network, it is unclear if it can be learnt from data in reasonable training time. This is what we demonstrate. \n\n2) Re memory/computation complexity: please see our response to \"complexity comparison\" below. \n\n3) Why should one use data to learn the Viterbi algorithm (or BCJR or Turbo decoding) when we know them already? This is because, we only know the optimal algorithms in simple settings and how to generalize them to more complicated or unknown channel models is sometimes unclear. We demonstrate that neural networks that learn from data can yield more robust and adaptable algorithms in those settings; see Section 3.\n\n4) We agree that it will be very interesting if the ML model leads to discovery of new class of coding schemes. Indeed, this is exactly what we show in Section 4, that for the Gaussian channel with feedback, the NN discovered codes beat the state-of-the-art codes. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510267238136,"tcdate":1510267238136,"number":2,"cdate":1510267238136,"id":"HkRwKLzkG","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Comment","forum":"ryazCMbR-","replyto":"r1_T4BACW","signatures":["ICLR.cc/2018/Conference/Paper1049/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1049/Authors"],"content":{"title":"Re: Complexity comparison","comment":"Thanks for your interest. \n\nThe number of multiplications is quadratic in \n- the dimension of hidden states​ of GRU​ (200) for the proposed neural decoder, and \n- the number of encoder states (4) for Viterbi and BCJR. \n\nThe number of add-compare-select units is \n- 0 for the proposed neural decoder, and \n- linear in the number of encoder states (4) for Viterbi.  \n\nThe dimension of hidden states in the GRU can be potentially reduced, using ideas such as network distillation. Apart from optimizing the size/complexity of the current neural decoder, significant parallelization is possible in the multiplicative units in the neural decoder, as well as pipelining. These designs in conjunction with a careful analysis of the fixed point arithmetic requirements of the different weights are under active research, and outside the scope of this paper.   \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510207615524,"tcdate":1510207513764,"number":4,"cdate":1510207513764,"id":"BJG7lOZJz","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Public_Comment","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"\"Leaning models\" for \"deterministic functions\"?","comment":"Interesting work; however, given that Viterbi algorithm, for example, is a simple (but elegant) well-defined algebraic function, isn't it expectable that a neural network with sufficient capacity would be able to approximate that? \n\nAlso, given the existing algorithm with efficient implementation, is it reasonable to replace it with an RNN? Will the time and memory complexity of such an RNN not be a major issue? \n\nAnd, in general, is it a good practice to replace (or replicate) deterministic functions with \"learning\" data-driven models?\n\nOf course, it would be very interesting if the ML model leads to the discovery of (or gives some insight into) a new class of coding schemes. But, this direction is not specifically pursued or examined in paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510001207256,"tcdate":1510001207256,"number":3,"cdate":1510001207256,"id":"B1krcrCRZ","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Public_Comment","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"OTHER NEURAL NETWORK ARCHITECTURES FOR N-RSC AND N-BCJR","comment":"For the results in Tables 1 and 2: are you using the code of Fig.1 or one of the codes of Fig.9 ?\n\nAlso, did you try similar analysis for codes with state dimension higher than 2 (or 3) ? \nIf not, what is your educated guess for the case of higher state dimensions?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1509999807971,"tcdate":1509999807971,"number":2,"cdate":1509999807971,"id":"r1_T4BACW","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Public_Comment","forum":"ryazCMbR-","replyto":"B1vxh70A-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: Complexity comparison","comment":"Thanks for your answer. \n\nCan you approximately calculate how many operations per information BIT you have in the neural decoer as compared to the viterbi decoder (like add-compare-select , multiplication, etc.) ?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510267289703,"tcdate":1509993455199,"number":1,"cdate":1509993455199,"id":"B1vxh70A-","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Official_Comment","forum":"ryazCMbR-","replyto":"ryUtBwdRW","signatures":["ICLR.cc/2018/Conference/Paper1049/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1049/Authors"],"content":{"title":"Re: Complexity comparison ","comment":"\nThe complexity of all decoders (Viterbi, BCJR, Neural) is linear in the number of information bits (block length). \n\nThe actual run times are hard to compare since ​some operations can be​ ​parallelized: e.g., matrix vector multiplications in the neural decoder can be easily parallelized in a GPU.\n\nThanks for pointing out the typo. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1509614973942,"tcdate":1509614973942,"number":1,"cdate":1509614973942,"id":"ryUtBwdRW","invitation":"ICLR.cc/2018/Conference/-/Paper1049/Public_Comment","forum":"ryazCMbR-","replyto":"ryazCMbR-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Complexity comparison ","comment":"Can you please elaborate on the complexity of the proposed neural decoder as compared to this of the Viterbi/BCJR decoder ?\n\nAlso, note a typo in Fig. 9: (a) and (b) figures should be exchanged."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]}},{"tddate":null,"ddate":null,"tmdate":1510092381842,"tcdate":1509137951596,"number":1049,"cdate":1510092360460,"id":"ryazCMbR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryazCMbR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Communication Algorithms via Deep Learning","abstract":"Coding theory is a central discipline underpinning wireline and wireless modems that are the workhorses of the information age. Progress in coding theory is largely driven by individual human ingenuity with sporadic breakthroughs over the past century. In this paper we study whether it is possible to automate the discovery of coding and decoding algorithms via deep learning. We study a family of sequential codes parametrized by recurrent neural network (RNN) architectures. We show that creatively designed and trained RNN architectures can decode well known sequential codes such as the convolutional and turbo codes with close to optimal performance on the additive white Gaussian noise (AWGN) channel, which itself is achieved by breakthrough algorithms of our times (Viterbi and BCJR decoders, representing dynamic programing and forward-backward algorithms). We also show strong generalizations: we train at a specific signal to noise ratio and block length but test at a wide range of these quantities, and also demonstrate robustness and adaptivity to deviations from the AWGN setting. Finally, we use the RNN architectures to design new nonlinear codes that represent a major progress in the long standing open problem of communicating reliably over the AWGN channel with noisy output feedback.","pdf":"/pdf/3aea5e46f4814781d3d3edbe98155d0a57723dff.pdf","TL;DR":"We show that creatively designed and trained RNN architectures can decode well known sequential codes and achieve close to optimal performances.","paperhash":"anonymous|communication_algorithms_via_deep_learning","_bibtex":"@article{\n  anonymous2018communication,\n  title={Communication Algorithms via Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryazCMbR-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1049/Authors"],"keywords":["coding theory","recurrent neural network","communication"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}