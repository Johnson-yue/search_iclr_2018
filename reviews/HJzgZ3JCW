{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222583518,"tcdate":1512016718176,"number":3,"cdate":1512016718176,"id":"HJ8UsZ6gM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well-written paper introducing a novel method of reducing multiplications in CNNs with very minor loss in accuracy","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary: \nThe paper presents a modification of the Winograd convolution algorithm that enables a reduction of multiplications in a forward pass of 10.8x almost without loss of accuracy. \nThis modification combines the reduction of multiplications achieved by the Winograd convolution algorithm with weight pruning in the following way:\n- weights are pruned after the Winograd transformation, to prevent the transformation from filling in zeros, thus preserving weight sparsity\n- the ReLU activation function associated with the previous layer is applied to the Winograd transform of the input activations, not directly to the spatial-domain activations, also yielding sparse activations\n\nThis way sparse multiplication can be performed. Because this yields a network, which is not mathematically equivalent to a vanilla or Winograd CNN, the method goes through three stages: dense training, pruning and retraining. The authors highlight that a dimension increase in weights and ReLU activations provide a more powerful representation and that stable dynamic activation densities over layer depths benefit the representational power of ReLU layers.\n\nReview:\nThe paper shows good results using the proposed method and the description is easy to follow. I particularly like Figure 1. \nI only have a couple of questions/comments:\n1) I’m not familiar with the term m-specific (“Matrices B, G and A are m-specific.”) and didn’t find anything that seemed related in a very quick google search. Maybe it would make sense to add at least an informal description.\n2) Although small filters are the norm, you could add a note, describing up to what filter sizes this method is applicable. Or is it almost exactly the same as for general Winograd CNNs?\n3) I think it would make sense to mention weight and activation quantization in the intro as well (even if you leave a combination with quantization for future work), e.g. Rastegari et al. (2016), Courbariaux et al. (2015) and Lin et al. (2015)\n4) Figure 5 caption has a typo: “acrruacy”\n\nReferences:\nCourbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. \"Binaryconnect: Training deep neural networks with binary weights during propagations.\" In Advances in Neural Information Processing Systems, pp. 3123-3131. 2015.\nLin, Zhouhan, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio. \"Neural networks with few multiplications.\" arXiv preprint arXiv:1510.03009 (2015).\nRastegari, Mohammad, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. \"Xnor-net: Imagenet classification using binary convolutional neural networks.\" In European Conference on Computer Vision, pp. 525-542. Springer International Publishing, 2016.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1512222583566,"tcdate":1511844682292,"number":2,"cdate":1511844682292,"id":"rJMLjDqeM","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A promising Method, though with some limitations","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a method to build a CNN in the Winograd domain, where weight pruning and ReLU can be applied in this domain to improve sparsity and reduce the number of multiplication. The resultant CNN can achieve ~10x theoretical speedup with little performance loss.\n\nThe paper is well-written. It provides a new way to combine the Winograd transformation and the threshold-based weight pruning strategy. Rather than strictly keeping the architecture of ordinary CNNs, the proposed method applied ReLU to the transform domain, which is interesting.  \n\nThe results on Cifar-10 and ImageNet are promising. In particular, the pruned model in the Winograd domain performs comparably to the state-of-the-art dense neural networks and shows significant theoretical speedup. \nThe results on ImageNet using ResNet-18 architecture are also promising. However, no results are provided for deeper networks, so it is unclear how this method can benefit the computation of very deep neural networks \n\nA general limitation of the proposed method is the network architecture inconsistency with the ordinary CNNs. Due to the location change of ReLUs, it is unclear how to transform a pretrained ordinary CNNs to the new architectures accurately. It seems training from scratch using the transformed architectures is the simplest solution. \n\nThe paper does not report the actual speedup in the wall clock time. The actual implementation is what matters in the end. \n\nIt will be more informative to present Figure 2,3,4 with respect to the workload in addition to the weight density. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1512222583608,"tcdate":1511585002087,"number":1,"cdate":1511585002087,"id":"SyMeSO8ef","invitation":"ICLR.cc/2018/Conference/-/Paper168/Official_Review","forum":"HJzgZ3JCW","replyto":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference/Paper168/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper with thorough experiments","rating":"7: Good paper, accept","review":"This paper proposes to combine Winograd transformation with sparsity to reduce the computation for deep convolutional neural network. Specifically, ReLU nonlinearity was moved after Winograd transformation to increase the dynamic sparsity in the Winograd domain, while an additional pruning on low magnitude weights and re-training procedure based on pruning is used to increase static sparsity of weights, which decreases computational demand. The resulting Winograd-ReLU\nCNN shows strong performance in three scenarios (CIFAR10 with VGG, CIFAR100 with ConvPool-CNN-C, and ImageNEt with ResNet-18). The proposed method seems to improve over the two baseline approaches (Winograd and sparsity, respectively).\n\nOverall, the paper is well-written and the experiments seems to be quite thorough and clear. Note that I am not an expert in this field and I might miss important references along this direction. I am leaving it to other reviewers to determine its novelty. \n\nPutting ReLU in the Winograd domain (or any transformed domain, e.g., Fourier) seems to be an interesting idea, and deserves some further exploration. Also, I am curious about the performance after weight pruning but before retraining).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]}},{"tddate":null,"ddate":null,"tmdate":1509739448354,"tcdate":1509044457680,"number":168,"cdate":1509739445697,"id":"HJzgZ3JCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJzgZ3JCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Efficient Sparse-Winograd Convolutional Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are computationally intensive, which limits their application on mobile devices. Their energy is dominated by the number of multiplies needed to perform the convolutions. Winograd’s minimal filtering algorithm (Lavin, 2015) and network pruning (Han et al., 2015) can reduce the operation count, but these two methods cannot be straightforwardly combined — applying the Winograd transform fills in the sparsity in both the weights and the activations. We propose two modifications to Winograd-based CNNs to enable these methods to exploit sparsity. First, we move the ReLU operation into the Winograd domain to increase the sparsity of the transformed activations. Second, we prune the weights in the Winograd domain to exploit static weight sparsity. For models on CIFAR-10, CIFAR-100 and ImageNet datasets, our method reduces the number of multiplications by 10.4x, 6.8x and 10.8x respectively with loss of accuracy less than 0.1%, outperforming previous baselines by 2.0x-3.0x. We also show that moving ReLU to the Winograd domain allows more aggressive pruning.","pdf":"/pdf/127fefbc54991293c094886621d17e55f20e019b.pdf","TL;DR":"Prune and ReLU in Winograd domain for efficient convolutional neural network","paperhash":"anonymous|efficient_sparsewinograd_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018efficient,\n  title={Efficient Sparse-Winograd Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJzgZ3JCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper168/Authors"],"keywords":["deep learning","convolutional neural network","pruning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}