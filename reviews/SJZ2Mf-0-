{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222768614,"tcdate":1512007999838,"number":3,"cdate":1512007999838,"id":"HJ_BtypgM","invitation":"ICLR.cc/2018/Conference/-/Paper796/Official_Review","forum":"SJZ2Mf-0-","replyto":"SJZ2Mf-0-","signatures":["ICLR.cc/2018/Conference/Paper796/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Very promising approach, but it seems the authors were not able to submit a finished manuscript on time.","rating":"4: Ok but not good enough - rejection","review":"This paper offers a very promising approach to the processing of the type of sequences we find in dialogues, somewhat in between RNNs which have problem modeling memory, and memory networks whose explicit modeling of the memory is too rigid.\n\nTo achieve that, the starting point seems to be a strength GRU that has the ability to dynamically add memory banks to the original dialogue and question sentence representations, thanks to the use of imperative DNN programming. The use of the reparametrization trick to enable global differentiability is reminiscent of an ICLR'17 paper \"Learning graphical state transitions\". Compared to the latter, the current paper seems to offer a more tractable architecture and optimization problem that does not require strong supervision and should be much faster to train.\n\nUnfortunately, this is the best understanding I got from this paper, as it seems to be in such a preliminary stage that the exact operations of the SGRU are not parsable. Maybe the authors have been taken off guard by the new review process where one can no longer improve the manuscript during this 2017 review (something that had enabled a few paper to pass the 2016 review).\n\nAfter a nice introduction, everything seems to fall apart in section 4, as if the authors did not have time to finish their write-up. \n- N is both the number of sentences and number of word per sentence, which does not make sense.\n- i iterates over both the sentences and the words. \n\nThe critical SGRU algorithm is impossible to parse\n- The hidden vector sigma, which is usually noted h in the GRU notation, is not even defined\n- The critical reset gate operation in Eq.(6) is not even explained, and modified in a way I do not understand compared to standard GRU.\n- What is t? From algorithm 1 in Appendix A, it seems to correspond to looping over both sentences and words.\n- The most novel and critical operation of this SGRU, to process the entities of the memory bank, is not even explained. All we get at the end of section 4.2 is \" After these steps are finished, all entities are passed through the strength modified GRU (4.1) to recompute question relevance.\"\n\nThe algorithm in Appendix A does  not help much.  With PyTorch being so readable, I wish some source code had been made available.\n\nExperiments reporting also contains unacceptable omissions and errors:\n- The definition of 'failed task', essential for understanding, is not stated (more than 5% error)\n- Reported numbers of failed tasks are erroneous: it should be 1 for DMN+ and 3 for MemN2N.\n\n\n\nPage 3: dynanet -> dynet","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]}},{"tddate":null,"ddate":null,"tmdate":1512222768651,"tcdate":1511834114239,"number":2,"cdate":1511834114239,"id":"By5ZMrqxG","invitation":"ICLR.cc/2018/Conference/-/Paper796/Official_Review","forum":"SJZ2Mf-0-","replyto":"SJZ2Mf-0-","signatures":["ICLR.cc/2018/Conference/Paper796/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The authors propose a model for QA that given a question and a story adaptively determines the number of  entity groups (banks). The paper is rather hard to follow as many task specific terms are not explained. For instance, it would benefit the paper if the authors introduced the definitions of a bank and a story. This will help the reader have a more comprehensive understanding of their framework.\n\nThe paper capitalized on the argument of faster inference and no wall-time for inference is shown. The authors only report the number of used banks. What are the runtime gains compared to Entnet? \nThis was the core motivation behind this work and the authors fail to discuss this completely.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]}},{"tddate":null,"ddate":null,"tmdate":1512222768691,"tcdate":1511756393955,"number":1,"cdate":1511756393955,"id":"rJGuMGYef","invitation":"ICLR.cc/2018/Conference/-/Paper796/Official_Review","forum":"SJZ2Mf-0-","replyto":"SJZ2Mf-0-","signatures":["ICLR.cc/2018/Conference/Paper796/AnonReviewer1"],"readers":["everyone"],"content":{"title":"\"faster inference\" is not convincing.","rating":"7: Good paper, accept","review":"Summary: \n\nThis paper proposes a dynamic memory augmented neural network for question answering. The proposed model iteratively creates a shorter list of relevant entities such that the decoder can look at only a smaller set of entities to answer the given question. Authors show results in bAbi dataset.\n\nMy comments:\n\n1. While the proposed model is very interesting, I disagree with the claim that AMN has lower inference times. The memory creation happens only after reading the question and hence the entire process can be considered as part of inference. So it is not clear if there is a huge reduction in the inference time when compared to other models that the authors compare. However, the proposed model looks like a nice piece of interpretable reasoning module. In that sense, it is not any better than EntNet based on the error rate since EntNet is doing better than AMN in 15 out of 20 tasks. So it is not very clear what is the advantage of AMN over EntNet or other MANN architectures.\n\n2. Can you explain equation 9 in detail? What is the input to the softmax function? What is the output size of the softmax? I assume q produces a scalar output. But what is the input size to the q function?\n\n3. In the experiment, when you say “best of 10 runs”, is it based on a separate validation set? Please report the mean and variance of the 10 runs. It is sad that people just report best of multiple runs in the bAbi tasks and not report the variance in the performance. I would like to see the mean and variance in the performance.\n\n4. What happens when number of entities is large? Can you comment about how this model will be useful in situations other than reading comprehension style QA? \n\n5. Are the authors willing to release the code for reproducing the results?\n\nMinor comments:\n\n1. Page 2, second line: “Networks(AMN)” should be “Networks (AMN).\n2. In page 3, first line: “Hazy et al. (2006)” should be “(Hazy et al. 2006)”.\n3. In page 3, second para, first line, both references should use \\citep instead of \\citet.\n4. In page 4, fourth para, Vanhoucke et al should also be inside \\citep.\n5. In page 4, notations paragraph: “a question is a sequence of N_q words” - “of” is missing.\n6. In page 5, first paragraph is not clear.\n7. In page 6, point 4, 7th line: “nodes in the its path” should be “nodes in its path”.\n8. In page 9, section 5.3, multiple questions, 2nd line: “We extend the our model” should be “We extend our model”.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]}},{"tddate":null,"ddate":null,"tmdate":1511321840569,"tcdate":1511321270799,"number":1,"cdate":1511321270799,"id":"Skkp0DGgM","invitation":"ICLR.cc/2018/Conference/-/Paper796/Official_Comment","forum":"SJZ2Mf-0-","replyto":"SkYhYmGlz","signatures":["ICLR.cc/2018/Conference/Paper796/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper796/Authors"],"content":{"title":"bank consists of multiple entities","comment":"Thanks for reading our paper. We agree that the abstract does not describe memory bank/slot clearly. We plan to clarify the this in the next revision. A bank in the paper does mean a series of similar entities.\n\nAn entity is a 3-tuple of word ID, hidden state, and a question relevance strength. A memory slot can hold this entity. A bank consists of multiple entities. The network learns to store various entities as the story is ingested into a bank. As the story is read in, the network learns to create newer banks and copy the entities. During inference, only a single bank or a few banks are used to answer the question, saving inference times.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]}},{"tddate":null,"ddate":null,"tmdate":1511303600907,"tcdate":1511303600907,"number":1,"cdate":1511303600907,"id":"SkYhYmGlz","invitation":"ICLR.cc/2018/Conference/-/Paper796/Public_Comment","forum":"SJZ2Mf-0-","replyto":"SJZ2Mf-0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Confusion memory bank / memory slot","comment":"The authors may wish to consult the definition of \"bank\" which is used extensively throughout the paper (as \"memory bank\") without a definition. A bank is supposed to be \"series of similar things\". It seems they use \"memory bank\" for \"memory slot\" making the text somewhat confusing."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]}},{"tddate":null,"ddate":null,"tmdate":1509739096343,"tcdate":1509135017283,"number":796,"cdate":1509739093671,"id":"SJZ2Mf-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJZ2Mf-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adaptive Memory Networks","abstract":"Real-world Question Answering (QA) tasks consist of thousands of words that often represent many facts and entities. Existing models based on LSTMs require a large number of parameters to support external memory and do not generalize well for long sequence inputs. Memory networks attempt to address these limitations by storing information to an external memory module but must examine all inputs in the memory. Hence, for longer sequence inputs the intermediate memory components proportionally scale in size resulting in poor inference times and high computation costs.\n\nIn this paper, we present Adaptive Memory Networks (AMN) that process input question pairs to dynamically construct a network architecture optimized for lower inference times. During inference, AMN parses input text into entities within different memory slots. However, distinct from previous approaches, AMN is a dynamic network architecture that creates variable numbers of memory banks weighted by question relevance. Thus, the decoder can select a variable number of memory banks to construct an answer using fewer banks, creating a runtime trade-off between accuracy and speed. \n\nAMN is enabled by first, a novel bank controller that makes discrete decisions with high accuracy and second, the capabilities of a dynamic framework (such as PyTorch) that allow for dynamic network sizing and efficient variable mini-batching. In our results, we demonstrate that our model learns to construct a varying number of memory banks based on task complexity and achieves faster inference times for standard bAbI tasks, and modified bAbI tasks. We achieve state of the art accuracy over these tasks with an average 48% lower entities are examined during inference.","pdf":"/pdf/51c0d9801b0c2d56da5282177005c5c6bdea20e1.pdf","TL;DR":"Memory networks with faster inference","paperhash":"anonymous|adaptive_memory_networks","_bibtex":"@article{\n  anonymous2018adaptive,\n  title={Adaptive Memory Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJZ2Mf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper796/Authors"],"keywords":["Memory Networks","Dynamic Networks","Faster Inference","Reasoning","QA"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}