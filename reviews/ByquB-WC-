{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222718088,"tcdate":1511923648169,"number":3,"cdate":1511923648169,"id":"SyuT1isxG","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input. The model achieves good results on bAbI compared to memory networks and the relation network model. From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question). This average is then fed to a softmax layer for answer prediction. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)? The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments. I'd encourage the authors to do a more detailed experimental study with more tasks, but I can't recommend this paper's acceptance in its current form.\n\nother questions / comments:\n- \"we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?\n- writing could be cleaned up for spelling / grammar (e.g., \"last 70 stories\" instead of \"last 70 sentences\"), currently the paper is very hard to read and it took me a while to understand the model","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/a330e860f3cee63b3f043f7e4343dcc9034e9d27.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1512222718129,"tcdate":1511825576865,"number":2,"cdate":1511825576865,"id":"rk-hlXcez","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Finding ReMO review","rating":"4: Ok but not good enough - rejection","review":"This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN). RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017). RMN reduces the complexity to linear time for the bAbi dataset. RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning. RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning. However, how widespread is this problem across other models or are you simply addressing a point problem for RN? For example, Entnet is able to reason as the input is fed in and the decoding costs are low. Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply. \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP). It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks. For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion. More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. Showing results with multiple hops (1,2,..) would be useful here.\n\nMore details are needed about Figure 3. Is this on bAbi as well? How did you generate these stories with so many sentences? Another clarification is the bAbi performance over Entnet which claims to solve all tasks. Your results show 4 failed tasks, is this your reproduction of Entnet?\n\nFinally, what are the savings from reducing this time complexity? Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.\n\nOverall, this paper feels like a small improvement over RN. Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement. One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/a330e860f3cee63b3f043f7e4343dcc9034e9d27.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1512222718173,"tcdate":1511762568693,"number":1,"cdate":1511762568693,"id":"r1Z9q7Ygf","invitation":"ICLR.cc/2018/Conference/-/Paper674/Official_Review","forum":"ByquB-WC-","replyto":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference/Paper674/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not sure what is novel","rating":"4: Ok but not good enough - rejection","review":"The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer. The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task.\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.\n- The paper is clearly written.\n\nCons:\n- I am not sure what is novel in the proposed model. While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network. Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N. Please tell me if I am missing something, but I am not sure of the contribution of the paper. Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful.\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/a330e860f3cee63b3f043f7e4343dcc9034e9d27.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]}},{"tddate":null,"ddate":null,"tmdate":1509739167612,"tcdate":1509131634536,"number":674,"cdate":1509739164941,"id":"ByquB-WC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByquB-WC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning","abstract":"Memory Network based models have shown a remarkable progress on the task of relational reasoning.\nRecently, a simpler yet powerful neural network module called Relation Network (RN) has been introduced. \nDespite its architectural simplicity, the time complexity of relation network grows quadratically with data, hence limiting its application to tasks with a large-scaled memory.\nWe introduce Related Memory Network, an end-to-end neural network architecture exploiting both memory network and relation network structures. \nWe follow memory network's four components while each component operates similar to the relation network without taking a pair of objects. \nAs a result, our model is as simple as RN but the computational complexity is reduced to linear time.\nIt achieves the state-of-the-art results in jointly trained bAbI-10k story-based question answering and  bAbI dialog dataset. ","pdf":"/pdf/a330e860f3cee63b3f043f7e4343dcc9034e9d27.pdf","TL;DR":"A simple reasoning architecture based on the memory network (MemNN) and relation network (RN), reducing the time complexity compared to the RN and achieving state-of-the-are result on bAbI story based QA and bAbI dialog.","paperhash":"anonymous|finding_remo_related_memory_object_a_simple_neural_architecture_for_text_based_reasoning","_bibtex":"@article{\n  anonymous2018finding,\n  title={Finding ReMO (Related Memory Object): A Simple neural architecture for Text based Reasoning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByquB-WC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper674/Authors"],"keywords":["Natural Language Processing","Deep Learning","Reasoning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}