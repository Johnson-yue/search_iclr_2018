{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222557005,"tcdate":1511923591824,"number":3,"cdate":1511923591824,"id":"SkgcJoogf","invitation":"ICLR.cc/2018/Conference/-/Paper1118/Official_Review","forum":"S1XolQbRW","replyto":"S1XolQbRW","signatures":["ICLR.cc/2018/Conference/Paper1118/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A very interesting paper with good motivation and good results.","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression. It proposed both the quantized distillation and also the differentiable quantization. The quantized distillation method just simply adapt the distillation work for the task of model compression, and give good results to the baseline method. While the differentiable quantization optimise the quantization function in a unified back-propagation framework. It is interesting to see the performance improvements by using the one-step optimisation method. \n\nI like this paper very much as it is in good motivation to utilize the distillation framework for the task of model compression. The starting point is quite interesting and reasonable. The information from the teacher network is useful for constructing a better compressed model. I believe this idea is quite similar to the idea of Learning using Privileged Information, in which the information on teacher model is only used during training, but is not utilised during testing. \n\nSome minor comments:\nIn table 3, it seems that the results for 2 bits are not stable, and are there any explanations?\nWhat will be the results if the student model performs the same with the teacher model (e.g., use the teacher model as the student model to do the compression) or even better (reverse the settings)?\nWhat will be the prediction speed for each of models? We can also get the time of speedup for the compressed model.\n\nIt will be better if the authors could discuss the connections between distillation and the recent work for the Learning using Privileged Information setting:\nVladimir Vapnik, Rauf Izmailov:\nLearning using privileged information: similarity control and knowledge transfer. Journal of Machine Learning Research 16: 2023-2049 (2015)\nXinxing Xu, Joey Tianyi Zhou, IvorW. Tsang, Zheng Qin, Rick Siow Mong Goh, Yong Liu: Simple and Efficient Learning using Privileged Information. BeyondLabeler: Human is More Than a Labeler, Workshop of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). New York City, USA. July, 2016.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model compression via distillation and quantization","abstract":"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n","pdf":"/pdf/6a770d7c95ac938be4c78c7d38abb92a01749769.pdf","paperhash":"anonymous|model_compression_via_distillation_and_quantization","_bibtex":"@article{\n  anonymous2018model,\n  title={Model compression via distillation and quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XolQbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1118/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1511918585946,"tcdate":1511918585946,"number":1,"cdate":1511918585946,"id":"rkMb3Yjlz","invitation":"ICLR.cc/2018/Conference/-/Paper1118/Public_Comment","forum":"S1XolQbRW","replyto":"S1XolQbRW","signatures":["~bruce_matthew_kuzak1"],"readers":["everyone"],"writers":["~bruce_matthew_kuzak1"],"content":{"title":"Source Code","comment":"I was wondering if you guys have an open source code for your experiment along with a the data used for training and validating that we could use to reproduce your results.\n\nMe and my team, would like to revise your research paper for a final class project.\n\nthank you"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model compression via distillation and quantization","abstract":"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n","pdf":"/pdf/6a770d7c95ac938be4c78c7d38abb92a01749769.pdf","paperhash":"anonymous|model_compression_via_distillation_and_quantization","_bibtex":"@article{\n  anonymous2018model,\n  title={Model compression via distillation and quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XolQbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1118/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222557051,"tcdate":1511808501792,"number":2,"cdate":1511808501792,"id":"HkCg0RFlz","invitation":"ICLR.cc/2018/Conference/-/Paper1118/Official_Review","forum":"S1XolQbRW","replyto":"S1XolQbRW","signatures":["ICLR.cc/2018/Conference/Paper1118/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review of \"Model compression via distillation and quantization\"","rating":"6: Marginally above acceptance threshold","review":"The paper proposes to combine two approaches to compress deep neural networks - distillation and quantization. The authors proposed two methods, one largely relying on the distillation loss idea then followed by a quantization step, and another one that also learns the location of the quantization points. Somewhat surprisingly, nobody has combined the two approaches before, which makes this paper interesting. Experiments show that both methods work well in compressing large deep neural network models for applications where resources are limited, like on mobile devices. \n\nOverall I am mostly OK with this paper but not impressed by it.  Detailed comments below.\n\n1. Quantizing with respect to the distillation loss seems to do better than with the normal loss - this needs more discussion. \n2. The idea of using the gradient with respect to the quantization points to learn them is interesting but not entirely new (see, e.g., \"Matrix Recovery from Quantized and Corrupted Measurements\", ICASSP 2014 and \"OrdRec: An Ordinal Model for Predicting Personalized Item Rating Distributions\", RecSys 2011, although in a different context). I also wonder if it would work better if you can also allow the weights to move a little bit (it seems to me from Algorithm 2 that you only update the quantization points). How about learning them altogether? Also this differentiable quantization method does not really depend on distillation, which is kind of confusing given the title.\n3. I am a little bit confused by how the bits are redistributed in the second method, as in the end it seems to use more than the proposed number of bits shown in the table (as recognized in section 4.2). This makes the comparison a little bit unfair (especially for the CIFAR 100 case, where the \"2 bits\" differentiable quantization is actually using 3.23 bits). This needs more clarification.\n4. The writing can be improved. For example, the concepts of \"teacher\" and \"student\" is not clear at all in the abstract - consider putting the first sentence of Section 3 in there instead. Also, the first sentence of the paper reads as \"... have showed tremendous performance\", which is not proper English. At the top of page 3 I found \"we will restrict our attention to uniform and non-uniform quantization\". What are you not restricting to, then?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model compression via distillation and quantization","abstract":"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n","pdf":"/pdf/6a770d7c95ac938be4c78c7d38abb92a01749769.pdf","paperhash":"anonymous|model_compression_via_distillation_and_quantization","_bibtex":"@article{\n  anonymous2018model,\n  title={Model compression via distillation and quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XolQbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1118/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222557094,"tcdate":1511697885022,"number":1,"cdate":1511697885022,"id":"SkBJ0mdlG","invitation":"ICLR.cc/2018/Conference/-/Paper1118/Official_Review","forum":"S1XolQbRW","replyto":"S1XolQbRW","signatures":["ICLR.cc/2018/Conference/Paper1118/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors try to compress models by combining distillation and quantization. The problem is important. However the solution is not good enough yet. ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to learn small and low-cost models by combining distillation and quantization. Two strategies are proposed and the ideas are reasonable and clearly introduced. Experiments on various datasets are conducted to show the effectiveness of the proposed method.\n\nPros:\n(1) The paper is well written, the review of distillation and quantization is clear.\n(2) Extensive experiments on vision and neural machine translation are conducted.\n(3) Detailed discussions about implementations are provided.\n\nCons:\n(1) The teacher model of CIFAR 10 and CIFAR 100 are far from state-of-the-art.\n(2) No results on ImageNet are provide\n(3) The differentiable quantization strategy seems to be no better than the straightforward quantized distillation.\n(4) Comparisons with state-of-the-art compression methods are not enough.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Model compression via distillation and quantization","abstract":"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n","pdf":"/pdf/6a770d7c95ac938be4c78c7d38abb92a01749769.pdf","paperhash":"anonymous|model_compression_via_distillation_and_quantization","_bibtex":"@article{\n  anonymous2018model,\n  title={Model compression via distillation and quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XolQbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1118/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092380473,"tcdate":1509138587404,"number":1118,"cdate":1510092359763,"id":"S1XolQbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1XolQbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Model compression via distillation and quantization","abstract":"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method,  differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model.  We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.\n","pdf":"/pdf/6a770d7c95ac938be4c78c7d38abb92a01749769.pdf","paperhash":"anonymous|model_compression_via_distillation_and_quantization","_bibtex":"@article{\n  anonymous2018model,\n  title={Model compression via distillation and quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XolQbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1118/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}