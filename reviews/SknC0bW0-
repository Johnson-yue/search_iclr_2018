{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222742902,"tcdate":1511819449541,"number":3,"cdate":1511819449541,"id":"ryZpu-qlM","invitation":"ICLR.cc/2018/Conference/-/Paper746/Official_Review","forum":"SknC0bW0-","replyto":"SknC0bW0-","signatures":["ICLR.cc/2018/Conference/Paper746/AnonReviewer1"],"readers":["everyone"],"content":{"title":"a paper with some new results","rating":"6: Marginally above acceptance threshold","review":"Minor comments:\n- page 7. “Then, observe that the same reasoning we used to develop the cfKG acquistion function\nin (3.2) can be used when when we observe gradients to motivate the acquisition function…” - some misprints, e.g. double “when”\n- The paper lacks theoretical analysis of convergence of the proposed modification of the knowledge gradient criterion.\n\nMajor comments:\n\nCurrent approaches to optimisation of expensive functions are mainly based on Gaussian process model. Such approaches are important for Auto ML algorithms.\n\nThere are a lot of cases, when for an expensive function we can obtain measurements of its values with continuous fidelity by leveraging costs for evaluation vs. fidelity of the obtained values. E.g. as fidelity we can consider a size of the training set used to train a deep neural network.\n\nThe paper contains a some new algorithm to perform Bayesian optimisation of a function with continuous fidelity. Using modification of the knowledge gradient acquisition function the authors obtained black box optimisation method taking into account continuous fidelity. \n\nDue to some reason the authors forgot to take the cost function into account when formulating the algorithm 1 in 3.3.2 and corresponding formula (3.7).\n\nSo, the logic of the definition of q-cfKG is understandable, but the issue with the missing denominator, containing cost function, remains.\n\nThe approach, proposed in section 3.3.2, looks as follows:\n- the authors used formulas from [Wu t al (2017) - https://arxiv.org/abs/1703.04389] \n- and include additional argument in the mean function of the Gaussian process.\nHowever, in Wu t al (2017) they consider usual knowledge gradient, but in this paper they divide by the value of max(cost(z)), which is not differentiable.\n\nOther sections of the paper are sufficiently well written, except \n- the section 3.3.2, \n- section with results of experiments: I was not able to understand how the authors defined cost function in sections 4.2 and 4.3 for their neural network and large scale kernel learning.\n\nIn principle, the paper contains some new results, but it should be improved before publishing.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-fidelity Bayesian Optimization with Knowledge Gradient","abstract":"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning.","pdf":"/pdf/a23542bd478f999ef12b9d90716b6ac5353570d2.pdf","TL;DR":"We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.","paperhash":"anonymous|continuousfidelity_bayesian_optimization_with_knowledge_gradient","_bibtex":"@article{\n  anonymous2018continuous-fidelity,\n  title={Continuous-fidelity Bayesian Optimization with Knowledge Gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SknC0bW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper746/Authors"],"keywords":["Continuous fidelity","Bayesian optimization","fast","knowledge gradient","hyperparameter optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222742942,"tcdate":1511727387662,"number":2,"cdate":1511727387662,"id":"Sy4mWsOeG","invitation":"ICLR.cc/2018/Conference/-/Paper746/Official_Review","forum":"SknC0bW0-","replyto":"SknC0bW0-","signatures":["ICLR.cc/2018/Conference/Paper746/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Incremental Technical Contribution, Weak Empirical Comparisons","rating":"4: Ok but not good enough - rejection","review":"\nMany black-box optimization problems are \"multi-fidelity\", in which it\nis possible to acquire data with different levels of cost and\nassociated uncertainty.  The training of machine learning models is a\ncommon example, in which more data and/or more training may lead to\nmore precise measurements of the quality of a hyperparameter\nconfiguration.  This has previously been referred to as a special case\nof \"multi-task\" Bayesian optimization, in which the tasks can be\nconstructed to reflect different fidelities.  The present paper\nexamines this construction with three twists: using the knowledge\ngradient acquisition function, using batched function evaluations, and\nincorporating derivative observations.  Broadly speaking, the idea is\nto allow fidelity to be represented as a point in a hypercube and then\ninclude this hypercube as a covariate in the Gaussian process.  The\nknowledge gradient acquisition function then becomes \"knowledge\ngradient per unit cost\" the KG equivalent to the \"expected improvement\nper unit cost\" discussed in Snoek et al (2012), although that paper\ndid not consider treating fidelity separately.\n\nI don't understand the claim that this is \"the first multi-fidelity\nalgorithm that can leverage gradients\".  Can't any Gaussian process\nmodel use gradient observations trivially, as discussed in the\nRasmussen and Williams book?  Why can't any EI or entropy search\nmethod also use gradient observations?  This doesn't usually come up\nin hyperparameter optimization, but it seems like a grandiose claim.\nSimilarly, although I don't know of a paper that explicitly does \"A +\nB\" for multi-fidelity BO and parallel BO, it is an incremental\ncontribution to combine them, not least because no other parallel BO\nmethods get evaluated as baselines.\n\nFigure 1 does not make sense to me.  How can the batched algorithm\noutperform the sequential algorithm on total cost?  The sequential\ncfKG algorithm should always be able to make better decisions with its\nremaining budget than 8-cfKG.  Is the answer that \"cost\" here means\n\"wall-clock time when parallelism is available\"?  If that's the case,\nthen it is necessary to include plots of parallelized EI, entropy\nsearch, and KG.  The same is true for Figure 2; other parallel BO\nalgorithms need to appear.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-fidelity Bayesian Optimization with Knowledge Gradient","abstract":"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning.","pdf":"/pdf/a23542bd478f999ef12b9d90716b6ac5353570d2.pdf","TL;DR":"We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.","paperhash":"anonymous|continuousfidelity_bayesian_optimization_with_knowledge_gradient","_bibtex":"@article{\n  anonymous2018continuous-fidelity,\n  title={Continuous-fidelity Bayesian Optimization with Knowledge Gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SknC0bW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper746/Authors"],"keywords":["Continuous fidelity","Bayesian optimization","fast","knowledge gradient","hyperparameter optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222742985,"tcdate":1510763103448,"number":1,"cdate":1510763103448,"id":"H1Dw9y51z","invitation":"ICLR.cc/2018/Conference/-/Paper746/Official_Review","forum":"SknC0bW0-","replyto":"SknC0bW0-","signatures":["ICLR.cc/2018/Conference/Paper746/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Neat work of low novelty.","rating":"5: Marginally below acceptance threshold","review":"This paper studies hyperparameter-optimization by Bayesian optimization, using the Knowledge Gradient framework and allowing the Bayesian optimizer to tune fideltiy against cost.\n\nThere’s nothing majorly wrong with this paper, but there’s also not much that is exciting about it. As the authors point out very clearly in Table 1, this setting has been addressed by several previous groups of authors. This paper does tick a previously unoccupied box in the problem-type-vs-algorithm matrix, but all the necessary steps are relatively straightforward.\n\nThe empirical results look good in comparison to the competing methods, but I suspsect an author of those competitors could find a way to make their own method look better in those plots, too.\n\nIn short: This is a neat paper, but it’s novelty is low. I don't think it would be a problem if this paper were accepted, but there are probably other, more groundbreaking papers in the batch.\n\nMinor question: Why are there no results for 8-cfKG and Hyperband in Figure 2 for SVHN?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-fidelity Bayesian Optimization with Knowledge Gradient","abstract":"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning.","pdf":"/pdf/a23542bd478f999ef12b9d90716b6ac5353570d2.pdf","TL;DR":"We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.","paperhash":"anonymous|continuousfidelity_bayesian_optimization_with_knowledge_gradient","_bibtex":"@article{\n  anonymous2018continuous-fidelity,\n  title={Continuous-fidelity Bayesian Optimization with Knowledge Gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SknC0bW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper746/Authors"],"keywords":["Continuous fidelity","Bayesian optimization","fast","knowledge gradient","hyperparameter optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739126366,"tcdate":1509134036327,"number":746,"cdate":1509739123706,"id":"SknC0bW0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SknC0bW0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous-fidelity Bayesian Optimization with Knowledge Gradient","abstract":"While Bayesian optimization (BO) has achieved great success in optimizing expensive-to-evaluate black-box functions, especially tuning hyperparameters of neural networks, methods such as random search (Li et al., 2016) and multi-fidelity BO (e.g. Klein et al. (2017)) that exploit cheap approximations, e.g. training on a smaller training data or with fewer iterations, can outperform standard BO approaches that use only full-fidelity observations. In this paper, we propose a novel Bayesian optimization algorithm, the continuous-fidelity knowledge gradient (cfKG) method, that can be used when fidelity is controlled by one or more continuous settings such as training data size and the number of training iterations. cfKG characterizes the value of the information gained by sampling a point at a given fidelity, choosing to sample at the point and fidelity with the largest value per unit cost. Furthermore, cfKG can be generalized, following Wu et al. (2017), to settings where derivatives are available in the optimization process, e.g. large-scale kernel learning, and where more than one point can be evaluated simultaneously. Numerical experiments show that cfKG outperforms state-of-art algorithms when optimizing synthetic functions, tuning convolutional neural networks (CNNs) on CIFAR-10 and SVHN, and in large-scale kernel learning.","pdf":"/pdf/a23542bd478f999ef12b9d90716b6ac5353570d2.pdf","TL;DR":"We propose a Bayes-optimal Bayesian optimization algorithm for hyperparameter tuning by exploiting cheap approximations.","paperhash":"anonymous|continuousfidelity_bayesian_optimization_with_knowledge_gradient","_bibtex":"@article{\n  anonymous2018continuous-fidelity,\n  title={Continuous-fidelity Bayesian Optimization with Knowledge Gradient},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SknC0bW0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper746/Authors"],"keywords":["Continuous fidelity","Bayesian optimization","fast","knowledge gradient","hyperparameter optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}