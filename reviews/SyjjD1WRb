{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222667402,"tcdate":1511876593596,"number":3,"cdate":1511876593596,"id":"S15xOyjgf","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting combination of evolutionary algorithms and EM; however, key technical details are missing and evaluation is contrived","rating":"4: Ok but not good enough - rejection","review":"This paper proposes an evolutionary algorithm for solving the variational E step in expectation-maximization algorithm for probabilistic models with binary latent variables. This is done by (i) considering the bit-vectors of the latent states as genomes of individuals, and by (ii) defining the fitness of the individuals as the log joint distribution of the parameters and the latent space.\n \nPros:\nThe paper is well written and the methodology presented is largely clear.\n\nCons:\nWhile the reviewer is essentially fine with the idea of the method, the reviewer is much less convinced of the empirical study. There is no comparison with other methods such as Monte carlo sampling.\nIt is not clear how computationally Evolutionary EM performs comparing to Variational EM algorithm and there is neither experimental results nor analysis for the computational complexity of the proposed model.\nThe datasets used in the experiments are quite old. The reviewer is concerned that these datasets may not be representative of real problems.\nThe applicability of the method is quite limited. The proposed model is only applicable for the probabilistic models with binary latent variables, hence it cannot be applied to more realistic complex model with real-valued latent variables.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/bf6cd7bc71ab61968c51ae9b567fad88a262f13c.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222667446,"tcdate":1511823228902,"number":2,"cdate":1511823228902,"id":"SyHYDG5lf","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer2"],"readers":["everyone"],"content":{"title":"lacks comparisons to non-EA approaches, hard to gain insight on how to design the EA substep for new applications","rating":"4: Ok but not good enough - rejection","review":"## Review summary\n\nOverall, the paper makes an interesting effort to tightly integrate\nexpectation-maximization (EM) training algorithms with evolutionary algorithms\n(EA). However, I found the technical description lacking key details and the\nexperimental comparisons inadequate. There were no comparisons to non-\nevolutionary EM algorithms, even though they exist for the models in question.\nFurthermore, the suggested approach lacks a principled way to select\nand tune key hyperparameters. I think the broad idea of using EA as a substep\nwithin a monotonically improving free energy algorithm could be interesting,\nbut needs far more experimental justification.\n\n\n## Pros / Stengths\n\n+ effort to study more than one model family\n\n+ maintaining monotonic improvement in free energy\n\n\n## Cons / Limitations\n\n- poor technical description and justification of the fitness function\n\n- lack of comparisons to other, non-EA algorithms\n\n- lack of study of hyperparameter sensitivity\n\n\n## Paper summary\n\nThe paper suggests a variant of the EM algorithm for binary hidden variable\nmodels, where the M-step proceeds as usual but the E-step is different in two\nways. First, following work by J. Lucke et al on Truncated Posteriors, the\ntrue posterior over the much larger space of all possible bit vectors is\napproximated by a more tractable small population of well-chosen bit vectors,\neach with some posterior weight. Second, this set of bit vectors is updated\nusing an evolutionary/genetic algorithm. This EA is the core contribution,\nsince the work on Trucated Posteriors has appeared before in the literature.\nThe overall EM algorithm still maintains monotonic improvement of a free\nenergy objective.\n\nTwo well-known generative models are considered: Noisy-Or models for discrete\ndatasets and Binary Sparse Coding for continuous datasets. Each has a\npreviously known, closed-form M-step (given in supplement). The focus is on\nthe E-step: how to select the H-dimensional bit vector for each data point.\n\nExperiments on artificial bars data and natural image patch datasets compare\nseveral variants of the proposed method, while varying a few EA method\nsubsteps such as selecting parents by fitness or randomly, including crossover\nor not, or using generic or specialized mutation rates.\n\n\n## Significance\n\nCombining evolutionary algorithms (EA) within EM has been done previously, as\nin Martinez and Vitria (Pattern Recog. Letters, 2000) or Pernkopf and\nBouchaffra (IEEE TPAMI, 2005) for mixture models. However, these efforts seem\nto use EA in an \"outer loop\" to refine different runs of EM, while the present\napproach uses EA in a substep of a single run of EM. I guess this is\ntechnically different, but it is already well known that any E-step method\nwhich monotonically improves the free energy is a valid algorithm. Thus, the\npaper's significance hinges on demonstrating that the particular E step chosen\nis better than alternatives. I don't think the paper succeeded very well at\nthis: there were no comparisons to non-EA algorithms, or to approaches that\nuse EA in the \"outer loop\" as above.\n\n\n## Clarity of Technical Approach\n\nWhat is \\tilde{log P} in Eq. 7? This seems a fundamental expression. Its\nplain-text definition is: \"the logarithm of the joint probability where\nsummands that do not depend on the state s have been elided\". To me, this\ndefinition is not precise enough for me to reproduce confidently... is it just\nlog p(s_n, y_n | theta)? I suggest revisions include a clear mathematical\ndefinition. This omission inhibits understanding of this paper's core\ncontributions.\n\nWhy does the fitness expression F defined in Eq. 7 satisfy the necessary\ncondition for fitness functions in Eq. 6? This choice of fitness function does\nnot seem intuitive to me. I think revisions are needed to *prove* this fitness\nfunction obeys the comparison property in Eq. 6.\n\nHow can we compute the minimization substep in Eq. 7 (min_s \\tilde{logP})? Is\nthis just done by exhaustive search over bit vectors? I think this needs\nclarification.\n\n\n## Quality of Experiments\n\nThe experiments are missing a crucial baseline: non-EA algorithms. Currently\nonly several varieties of EA are compared, so it is impossible to tell if the\nsuggested EA strategies even improve over non-EA baselines. As a specific\nexample, previous work already cited in this paper -- Henniges et al (2000) --\nhas developed a non-EA EM algorithm for Binary Sparse Coding, which already\nuses the truncated posterior formulation. Why not compare to this?\n\nThe proposed algorithm has many hyperparameters, including number of\ngenerations, number of parents, size of the latent space H, size of the\ntruncation, etc. The current paper offers little advice about selecting these\nvalues intelligently, but presumably performance is quite sensitive to these\nvalues. I'd like to see some more discussion of this and (ideally) more\nexperiments to help practitioners know which parameters matter most,\nespecially in the EA substep.\n\nRuntime analysis is missing as well: Is runtime dominated by the EA step? How\ndoes it compare to non-EA approaches? How big of datasets can the proposed\nmethod scale to?\n\nThe reader walks away from the current toy bars experiment somewhat confused.\nThe Noisy-Or experiment did not favor crossover and and favored specialized\nmutations, while the BSC experiment reached the opposite conclusions. How does\none design an EA for a new dataset, given this knowledge? Do we need to\nexhaustively try all different EA substeps, or are there smarter lessons to\nlearn?\n\n\n\n## Detailed comments\n\nBottom of page 1: I wouldn't say that \"variational EM\" is an approximation to\nEM. Sometimes moving from EM to variational EM can mean we estimate posteriors\n(not point estimates) for both local (example-specific) and global parameters.\nInstead, the *approximation* comes simply from restricting the solution space\nto gain tractability.\n\nSec. 2: Make clear earlier that hidden var \"s\" is assumed to be discrete, not\ncontinuous.\n\nAfter Mutation section: Remind readers that \"N_g\" is number of generations\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/bf6cd7bc71ab61968c51ae9b567fad88a262f13c.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222667497,"tcdate":1511727768991,"number":1,"cdate":1511727768991,"id":"ryWjGsdef","invitation":"ICLR.cc/2018/Conference/-/Paper484/Official_Review","forum":"SyjjD1WRb","replyto":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference/Paper484/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very specialised evolutionary EM algorithm","rating":"4: Ok but not good enough - rejection","review":"The paper presents a combination of evolutionary computation (EC) and variational EM for models with binary latent variables represented via a particle-based approximation.\n\nThe scope of the paper is quite narrow as the proposed method is only applicable to very specialised models. Furthermore, the authors do not seem to present any realistic modelling problems where the proposed approach would clearly advance the state of the art. There are no empirical comparisons with state of the art, only between different variants of the proposed method.\n\nBecause of these limitations, I do not think the paper can be considered for acceptance.\n\nDetailed comments:\n\n1. When revising the paper for next submission, please make the title more specific. Papers with very broad titles that only solve a very small part of the problem are very annoying.\n\n2. Your use of crossover operators seems quite unimaginative. Genomes have a linear order but in the case of 2D images you use it is not obvious how that should be mapped to 1D. Combining crossovers in different representations or 2D crossovers might fit your problem much better.\n\n3. Please present a real learning problem where your approach advances state of the art.\n\n4. For the results in Fig. 7, please run the algorithm until convergence or justify why that is not necessary.\n\n5. Please clarify the notation: what is the difference between y^n and y^(n)?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Evolutionary Expectation Maximization","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/bf6cd7bc71ab61968c51ae9b567fad88a262f13c.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739277233,"tcdate":1509124003501,"number":484,"cdate":1509739274570,"id":"SyjjD1WRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyjjD1WRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Evolutionary Expectation Maximization","abstract":"We establish a theoretical link between evolutionary algorithms and variational parameter optimization of probabilistic generative models with binary hidden variables.\nWhile the novel approach is independent of the actual generative model, here we use two such models to investigate its applicability and scalability: a noisy-OR Bayes Net (as a standard example of binary data) and Binary Sparse Coding (as a model for continuous data).\n\nLearning of probabilistic generative models is first formulated as approximate maximum likelihood optimization using variational expectation maximization (EM).\nWe choose truncated posteriors as variational distributions in which discrete latent states serve as variational parameters. In the variational E-step,\nthe latent states are then  \noptimized according to a tractable free-energy objective. Given a data point, we can show that evolutionary algorithms can be used for the variational optimization loop by (A)~considering the bit-vectors of the latent states as genomes of individuals, and by (B)~defining the fitness of the\nindividuals as the (log) joint probabilities given by the used generative model.\n\nAs a proof of concept, we apply the novel evolutionary EM approach to the optimization of the parameters of noisy-OR Bayes nets and binary sparse coding on artificial and real data (natural image patches). Using point mutations and single-point cross-over for the evolutionary algorithm, we find that scalable variational EM algorithms are obtained which efficiently improve the data likelihood. In general we believe that, with the link established here, standard as well as recent results in the field of evolutionary optimization can be leveraged to address the difficult problem of parameter optimization in generative models.","pdf":"/pdf/bf6cd7bc71ab61968c51ae9b567fad88a262f13c.pdf","TL;DR":"We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization","paperhash":"anonymous|evolutionary_expectation_maximization","_bibtex":"@article{\n  anonymous2018evolutionary,\n  title={Evolutionary Expectation Maximization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyjjD1WRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper484/Authors"],"keywords":["unsupervised","learning","evolutionary","sparse","coding","noisyOR","BSC","EM","expectation-maximization","variational EM","optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}