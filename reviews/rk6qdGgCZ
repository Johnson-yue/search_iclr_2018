{"notes":[{"tddate":null,"ddate":null,"tmdate":1511831786477,"tcdate":1511831786477,"number":1,"cdate":1511831786477,"id":"H1zlFEcxM","invitation":"ICLR.cc/2018/Conference/-/Paper226/Official_Comment","forum":"rk6qdGgCZ","replyto":"BJNHyGYlG","signatures":["ICLR.cc/2018/Conference/Paper226/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper226/Authors"],"content":{"title":"+ corrected sentence, + extended Figure 1","comment":"Thanks for the note! \n\nEven if the shape of the hyperparameter space would drastically change outside of the current range, the claim would be correct for SGD because the already presented results alone make it impossible to first fix the learning rate LR to any value from the range and then expect that the best weight decay found for that LR value would be nearly-optimal for all other possible values of LR. However, we agree that the example given in the sentence is unfortunate because it asks the reader to extrapolate instead of dealing with the data that is shown. It is confusing and we will correct that with a better example whose results are shown in Figure 1:  when LR=0.5, optimal weight decay factor is 1/8 *0.001 but it is not optimal for all other settings of LR. \n\nRegarding the values outside of the current range, it seems very unlikely that better results for LR>0.2 exist given the isolines shown in Figure 1 (note the elliptic shape and that the top results for LR=0.2 are worse than for LR=0.1) and that none of the papers with ResNets on CIFAR-10 (with standard settings of batch size, etc.) we are aware of use LR>0.2. In fact, since momentum-SGD is a standard baseline, its hyperparameters for ResNets on CIFAR-10 have been heavily tuned by researchers so that LR often lies in [0.05, 0.1] that matches the best region of momentum-SGD shown in Figure 1.\n\nThank you for helping to avoid possible confusions: we will correct the sentence and extend Figure 1 of momentum-SGD by an additional column with LR=0.4 and even larger LR if necessary."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222591648,"tcdate":1511786531312,"number":3,"cdate":1511786531312,"id":"SJs7uYYeM","invitation":"ICLR.cc/2018/Conference/-/Paper226/Official_Review","forum":"rk6qdGgCZ","replyto":"rk6qdGgCZ","signatures":["ICLR.cc/2018/Conference/Paper226/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Important work supported with experiments","rating":"7: Good paper, accept","review":"At the heart of the paper, there is a single idea: to decouple the weight decay from the number of steps taken by the optimization process (the paragraph at the end of page 2 is the key to the paper). This is an important and largely overlooked area of implementation and most off-the-shelf optimization algorithms, unfortunately, miss this point, too. I think that the proposed implementation should be taken seriously, especially in conjunction with the discussion that has been carried out with the work of Wilson et al., 2017 (https://arxiv.org/abs/1705.08292).\n\nThe introduction does a decent job explaining why it is necessary to pay attention to the norm of the weights as the training progresses within its scope. However, I would like to add a couple more points to the discussion: \n- \"Optimal weight decay is a function (among other things) of the total number of epochs / batch passes.\" in principle, it is a function of weight updates. Clearly, it depends on the way the decay process is scheduled. However, there is a bad habit in DL where time is scaled by the number of epochs rather than the number of weight updates which sometimes lead to misleading plots (for instance, when comparing two algorithms with different batch sizes).\n- Another ICLR 2018 submission has an interesting take on the norm of the weights and the algorithm (https://openreview.net/forum?id=HkmaTz-0W&noteId=HkmaTz-0W). Figure 3 shows the histograms of SGD/ADAM with and without WD (the *un-fixed* version), and it clearly shows how the landscape appear misleadingly different when one doesn't pay attention to the weight distribution in visualizations. \n- In figure 2, it appears that the training process has three phases, an initial decay, a steady progress, and a final decay that is more pronounced in AdamW. This final decay also correlates with the better test error of the proposed method. This third part also seems to correspond to the difference between Adam and AdamW through the way they branch out after following similar curves. One wonders what causes this branching and whether the key the desired effects are observed at the bottom of the landscape.\n- The paper concludes with \"Advani & Saxe (2017) analytically showed that in the limited data regime of deep networks the presence of eigenvalues that are zero forms a frozen subspace in which no learning occurs and thus smaller (e.g., zero) initial weight norms should be used to achieve best generalization results.\" Related to this there is another ICLR 2018 submission (https://openreview.net/forum?id=rJrTwxbCb), figure 1 shows that the eigenvalues of the Hessian of the loss have zero forms at the bottom of the landscape, not at the beginning. Back to the previous point, maybe that discussion should focus on the second and third phases of the training, not the beginning. \n- Finally, it would also be interesting to discuss the relation of the behavior of the weights at the last parts of the training and its connection to pruning. \n\nI'm aware that one can easily go beyond the scope of the paper by adding more material. Therefore, it is not completely reasonable to expect all such possible discussions to take place at once. The paper as it stands is reasonably self-contained and to the point. Just a minor last point that is irrelevant to the content of the work: The slash punctuation mark that is used to indicate 'or' should be used without spaces as in 'epochs/batch'.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1511755580123,"tcdate":1511755580123,"number":1,"cdate":1511755580123,"id":"BJNHyGYlG","invitation":"ICLR.cc/2018/Conference/-/Paper226/Public_Comment","forum":"rk6qdGgCZ","replyto":"rk6qdGgCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Need to extend domain of plot in Figure 1?","comment":"In Figure 1 top, the blue region of SGDW is fully visible in the plot. But for SGD, the blue region gets chopped off the edge of the plot. This seems to make a fair comparison difficult. In particular, the following statement seems questionable, since it is not clear what happens for SGD outside of the visible region in the plot.\n\n\"even if the learning rate is not well tuned yet (e.g., consider the value of 1/1024 in Figure 1, top right), leaving it fixed and only optimizing the weight decay factor would yield a good value (of 1/4*0.001). This is not the case for the original SGD shown in Figure 1 (top left).\""},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222591687,"tcdate":1511714126982,"number":2,"cdate":1511714126982,"id":"rJvLpvdez","invitation":"ICLR.cc/2018/Conference/-/Paper226/Official_Review","forum":"rk6qdGgCZ","replyto":"rk6qdGgCZ","signatures":["ICLR.cc/2018/Conference/Paper226/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The paper presents an alternative way to implement weight decay in Adam. Empirical results are shown to support this idea.\n\nThe idea presented in the paper is interesting, but I have some concerns about it.\n\nFirst, the authors argue that the weight decay should be implemented in a way different from the minimization of a L2 regularization. This seems a very weird statement to me. In fact, it easy to see that what the authors propose is to minimize two different objective functions in SGDW and AdamW! I am not even sure how I should interpret what they propose. The fact is that SGD and Adam are optimization algorithms, so we cannot just change the update rule in the same way in both algorithms and expect them to behave in the same way just because the added terms have the same shape!\n\nSecond, the equation (5) that re-normalize the weight decay parameter as been obtained on one dataset, as the author admit, and tested only on another one. I am not sure this is enough to be considered as a scientific proof.\n\nAlso, the empirical experiments seem to use the cosine annealing of the learning rate. This means that the only thing the authors proved is that their proposed change yields better results when used with a particular setting of the cosine annealing. What happens in the other cases?\n\nTo summarize, I think the idea is interesting but the paper might not be ready to be presented in a scientific conference.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1512222591726,"tcdate":1511712026921,"number":1,"cdate":1511712026921,"id":"HJX7HvOez","invitation":"ICLR.cc/2018/Conference/-/Paper226/Official_Review","forum":"rk6qdGgCZ","replyto":"rk6qdGgCZ","signatures":["ICLR.cc/2018/Conference/Paper226/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Novel investigation and insight about weight decay in SGD variants","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper investigates weight decay issues lied in the SGD variants, especially Adam. Current implementations of adaptive gradient algorithms implicitly contain a crucial flaw, by which \bweight decay in these methods does not correspond to L2 regularization. To fix this issue, this paper proposes the decoupling method between weight decay and the gradient-based update.\n\nOverall, this paper is well-written and contain sufficient references to note the overview of recent adaptive gradient-based methods for DNN. In addition, this paper investigates the crucial issue in the recent adaptive gradient methods and find the problem in weight decay. This is an interesting finding. And the proposed method to fix this issue is simple and reasonable. Their experimental results to validate the effectiveness of their proposed method are well-organized. In particular, the investigation on hyperparameter spaces shows the strong advantage of the proposed methods.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]}},{"tddate":null,"ddate":null,"tmdate":1509739418793,"tcdate":1509070996815,"number":226,"cdate":1509739416131,"id":"rk6qdGgCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rk6qdGgCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Fixing Weight Decay Regularization in Adam","abstract":"We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as would be expected for standard weight decay) but by an additive constant factor. We propose a simple way to resolve this issue by decoupling the weight decay and the optimization steps taken w.r.t. the loss function. We provide empirical evidence that the proposed decoupling (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also demonstrate that longer optimization runs require smaller weight decay values for optimal results and introduce a normalized variant of weight decay to reduce this dependence. Finally, we propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. ","pdf":"/pdf/732f6c8ba5b92844ef737498a14403e6480c9108.pdf","TL;DR":"Fixing weight decay regularization in adaptive gradient methods such as Adam","paperhash":"anonymous|fixing_weight_decay_regularization_in_adam","_bibtex":"@article{\n  anonymous2018fixing,\n  title={Fixing Weight Decay Regularization in Adam},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rk6qdGgCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper226/Authors"],"keywords":["Adam","Adaptive Gradient Methods","weight decay","L2 regularization"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}