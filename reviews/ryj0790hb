{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222810119,"tcdate":1511970753360,"number":3,"cdate":1511970753360,"id":"HyK6w83xM","invitation":"ICLR.cc/2018/Conference/-/Paper9/Official_Review","forum":"ryj0790hb","replyto":"ryj0790hb","signatures":["ICLR.cc/2018/Conference/Paper9/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Extensive experiments but inconclusive for the main message (task-incremental learning)","rating":"5: Marginally below acceptance threshold","review":"----------------- Summary -----------------\nThe paper tackles the problem of task-incremental learning using deep networks. It devises an architecture and a training procedure aiming for some desirable properties; a) it does not require retraining using previous tasks’ data, b) the number of network parameters grows only sublinearly c) it preserves the output of the previous tasks intact.\n\n----------------- Overall -----------------\nThe paper tackles an important problem, aims for important characteristics, and does extensive and various experiments. While the broadness of the experiments are encouraging, the main task which is to propose an effective task-incremental learning procedure is not conclusively tested, mainly due to the lack of thorough ablation studies (for instance when convolutional layers are fixed) and the architecture seems to change from one baseline (method) to another.\n\n----------------- Details -----------------\n- in the abstract it says: \"Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network.\"\nThe linear-combination constraint in the proposed approach is a strong one and can learn a sub-optimal solution for the newly introduced tasks.\n\n- Page 3: R^C → R^{C_o}\n\n- The notation is (probably unnecessarily) too complicated, perhaps it’s better to formulate it without being faithful to the actual implementation but for higher clarity and ease of understanding. For instance, one could start from denoting feature maps and applying the controller/transform matrix W on that, circumventing the clutter of convolutional kernels.\n\n- What is the DAN architecture? \n\n- In table 1 a better comparison is when using same architecture (instead of VGG) to train it from scratch or fine-tune from ImageNet (the first two rows)\n\n- What is the architecture used for random-weights baseline?\n\n- An experiment is needed where no controller is attached but just the additional fully-connected layers to see the isolated improvements gained by the linear transform of convolutional layers.\n\n- Multiple Base Networks: The assumption in incremental learning is that one does not have access to all tasks/datasets at once, otherwise one would train them jointly which would save parameters, training time and performance. So, finding the best base network using the validation set is not relevant.\n\n- The same concern as above applies to the transferability and dataset decider experiments\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning through Deep Adaptation","abstract":"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","pdf":"/pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf","TL;DR":"An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks.","paperhash":"anonymous|incremental_learning_through_deep_adaptation","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning through Deep Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj0790hb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper9/Authors"],"keywords":["Transfer Learning","Learning without forgetting","Multitask Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222810159,"tcdate":1511833696471,"number":2,"cdate":1511833696471,"id":"HyOveS5gf","invitation":"ICLR.cc/2018/Conference/-/Paper9/Official_Review","forum":"ryj0790hb","replyto":"ryj0790hb","signatures":["ICLR.cc/2018/Conference/Paper9/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea but missing some simple baselines.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes new idea of using controller modules for increment learning. Instead of finetuning the whole network, only the added parameters of the controller modules are learned while the output of the old task stays the same. Experiments are conducted on multiple image classification datasets. \n\nI found the idea of using controller modules for increment learning interesting and have some practical use cases. However, this paper has the following weakness:\n1) Missing simple baselines. I m curious to see some other multitask learning approach, e.g. branch out on the last few layers for different tasks and finetune the last few layers. The number of parameters won't be affected so much and it will achieve better performance than 'feature' in table 3.\n2) Gain of margin is really small. The performance improvements in Table1 and Table3 are very small. I understand the point is to argue with fewer parameters the model can achieve comparable accuracies. However, there could be other ways to design the network architecture to reduce the size (sharing the lower level representations).\n3) Presentation of the paper is not quite good. Figures are blurry and too small. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning through Deep Adaptation","abstract":"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","pdf":"/pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf","TL;DR":"An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks.","paperhash":"anonymous|incremental_learning_through_deep_adaptation","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning through Deep Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj0790hb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper9/Authors"],"keywords":["Transfer Learning","Learning without forgetting","Multitask Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222810201,"tcdate":1511815094629,"number":1,"cdate":1511815094629,"id":"BJJTve9gM","invitation":"ICLR.cc/2018/Conference/-/Paper9/Official_Review","forum":"ryj0790hb","replyto":"ryj0790hb","signatures":["ICLR.cc/2018/Conference/Paper9/AnonReviewer2"],"readers":["everyone"],"content":{"title":"-","rating":"6: Marginally above acceptance threshold","review":"This paper proposes to adapt convnet representations to new tasks while avoiding catastrophic forgetting by learning a per-task “controller” specifying weightings of the convolution-al filters throughout the network while keeping the filters themselves fixed.\n\n\nPros\n\nThe proposed approach is novel and broadly applicable.  By definition it maintains the exact performance on the original task, and enables the network to transfer to new tasks using a controller with a small number of parameters (asymptotically smaller than that of the base network).\n\nThe method is tested on a number of datasets (each used as source and target) and shows good transfer learning performance on each one.  A number of different fine-tuning regimes are explored.\n\nThe paper is mostly clear and well-written (though with a few typos that should be fixed).\n\n\nCons/Questions/Suggestions\n\nThe distinction between the convolutional and fully-connected layers (called “classifiers”) in the approach description (sec 3) is somewhat arbitrary -- after all, convolutional layers are a generalization of fully-connected layers. (This is hinted at by the mention of fully convolutional networks.)  The method could just as easily be applied to learn a task-specific rotation of the fully-connected layer weights.  A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network (for K={0, 1, …, N}) and learning independent weights for the latter N-K layers, but I understand this would be a rather large experimental burden.\n\nWhen discussing the controller initialization (sec 4.3), it’s stated that the diagonal init works the best, and that this means one only needs to learn the diagonals to get the best results.  Is this implying that the gradients wrt off-diagonal entries of the controller weight matrix are 0 under the diagonal initialization, hence the off-diagonal entries remain zero after learning?  It’s not immediately clear to me whether this is the case -- it could help to clarify this in the text.\n\nIf the off-diag gradients are indeed 0 under the diag init, it could also make sense to experiment with an “identity+noise” initialization of the controller matrix, which might give the best of both worlds in terms of flexibility and inductive bias to maintain the original representation. (Equivalently, one could treat the controller-weighted filters as a “residual” term on the original filters F with the controller weights W initialized to noise, with the final filters being F+(W\\crossF) rather than just W\\crossF.)\n\nThe dataset classifier (sec 4.3.4) could be learnt end-to-end by using a softmax output of the dataset classifier as the alpha weighting. It would be interesting to see how this compares with the hard thresholding method used here.  (As an intermediate step, the performance could also be measured with the dataset classifier trained in the same way but used as a soft weighting, rather than the hard version rounding alpha to 0 or 1.)\n\n\nOverall, the paper is clear and the proposed method is sensible, novel, and evaluated reasonably thoroughly.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Incremental Learning through Deep Adaptation","abstract":"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","pdf":"/pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf","TL;DR":"An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks.","paperhash":"anonymous|incremental_learning_through_deep_adaptation","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning through Deep Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj0790hb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper9/Authors"],"keywords":["Transfer Learning","Learning without forgetting","Multitask Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739532511,"tcdate":1507922898856,"number":9,"cdate":1509739529853,"id":"ryj0790hb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryj0790hb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Incremental Learning through Deep Adaptation","abstract":"Given an existing trained neural network, it is often desirable to learn new capabilities without hindering performance of those already learned. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method called Deep Adaptation Networks (DAN) that constrains newly learned filters to be linear combinations of existing ones. DANs preserve performance on the original task, require a fraction (typically 13%) of the number of parameters compared to standard fine-tuning procedures and converge in less cycles of training to a comparable or better level of performance. When coupled with standard network quantization techniques, we further reduce the parameter cost to around 3% of the original with negligible or no loss in accuracy. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method on a range of image classification tasks and explore different aspects of its behavior.","pdf":"/pdf/85a6012f830b872e7a9e70c88a18f36383755bcd.pdf","TL;DR":"An alternative to transfer learning that learns faster, requires much less parameters (3-13 %), usually achieves better results and precisely preserves performance on old tasks.","paperhash":"anonymous|incremental_learning_through_deep_adaptation","_bibtex":"@article{\n  anonymous2018incremental,\n  title={Incremental Learning through Deep Adaptation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryj0790hb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper9/Authors"],"keywords":["Transfer Learning","Learning without forgetting","Multitask Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}