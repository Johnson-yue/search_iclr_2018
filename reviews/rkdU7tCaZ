{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222827977,"tcdate":1511817449990,"number":3,"cdate":1511817449990,"id":"BkGlZbceG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"dynamic evaluation of neural sequence models\"","rating":"3: Clear rejection","review":"This paper proposes a dynamic evaluation of recurrent neural network language models by updating model parameters with certain segment lengths.\n\nPros.\n- Simple adaptation scheme seems to work, and the paper also shows (marginal) improvement from a conventional method (neural cache RNNLM) \nCons.\n- The paper is not well written due to undefined variables/indexes, confused explanations, not clear explanations of the proposed method in abstract and introduction (see the comments below)\n- Although the perplexity is an important measure, it’s better to show the effectiveness of the proposed method with more practical tasks including machine translation and speech recognition. \n\nComments:\n- Abstract: it is difficult to guess the characteristics of the proposed method only with a term “dynamic evaluation”. It’s better to explain it in more detail in the abstract.\n- Abstract: It’s better to provide relative performance (comparison) of the numbers (perplexity and bits/char) from conventional methods.\n- Section 2: Some variables are not explicitly introduced when they are appeared including i, n, g, and l\n- Section 3: same comment with the above for M. Also n is already used in Section 2 as a number of sequences.\n- Section 5. Why does the paper only provide examples for SGD and RMSprop? Can we apply it to other optimization methods including Adam and Adadelta?\n- Section 6, equation (9): is this new matrix introduced for every layer? Need some explanations.\n- Section 7.1: It’s better to provide the citation of Chainer.\n- Section 7.1 “AWD-LSTM”: The paper should provide the full name of AWD-LSTM when it is first appeared.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/1cb8c118b293cf605fdcd454c46ee638876f4131.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512222828021,"tcdate":1511627561510,"number":2,"cdate":1511627561510,"id":"Sk-EjzwxG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The proposed improvement to the dynamic evaluation method yields some performance gains on the considered benchmarks, but the authors are missing an evaluation on a larger data set.","rating":"7: Good paper, accept","review":"The authors provide an improved implementation of the idea of dynamic evaluation, where the update of the parameters used in the last time step proposed in (Mikolov et al. 2010) is replaced with a back-propagation through the last few time steps, and uses  RMSprop rather than vanilla SGD. The method is applied to word level and character level language modeling where it yields some gains in perplexity. The algorithm also appears able to perform domain adaptation, in a setting where a character-level language model trained mostly on English manages to quickly adapt to a Spanish test set. \n\nWhile the general idea is not novel, the implementation choices matter, and the authors provide one which appears to work well with recently proposed models. The character level experiments on the multiplicative LSTM make the most convincing point, providing a significant improvement over already good results on medium size data sets. Figure 2 also makes a strong case for the method's suitability for applications where domain adaptation is important.\n\nThe paper's weakest part is the word level language modeling section. Given the small size of the data sets considered, the results provided are of limited use, especially since the development set is used to fit the RMSprop hyper-parameters. How sensitive are the final results to this choice? Comparing dynamic evaluation to neural cache models is a good idea, given how both depend en medium-term history: (Grave et al. 2017) provide results on the larger text8 and wiki103, it would be useful to see results for dynamic evaluation at least on the former.\n\nAn indication of the actual additional evaluation time for word-level, char-level and sparse char-level dynamic evaluation would also be welcome.\n\nPros:\n- Good new implementation of an existing idea\n- Significant perplexity gains on character level language modeling\n- Good at domain adaptation\n\nCons:\n- Memory requirements of the method\n- Word-level language modeling experiments need to be run on larger data sets\n\n(Remark: the 7 rating is assuming that the authors respond to the concern about the size of the word-level data set)","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/1cb8c118b293cf605fdcd454c46ee638876f4131.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1512222828076,"tcdate":1511536809339,"number":1,"cdate":1511536809339,"id":"B1Z3O3HeG","invitation":"ICLR.cc/2018/Conference/-/Paper96/Official_Review","forum":"rkdU7tCaZ","replyto":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference/Paper96/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Dynamic Evaluation or is it Fast Weights at test time?","rating":"6: Marginally above acceptance threshold","review":"This paper takes AWD-LSTM, a recent, state of the art language model that was equipped with a Neural Cache, swaps the cache out for Dynamic Evaluation and improves the perplexities.\n\nDynamic Evaluation was the baseline that was most obviously missing from the original Neural Cache paper (Grave, 2016) and from the AWD-LSTM paper. In this sense, this work fills in a gap.\n\nLooking at the proposed update rule for Dynamic Evaluation though, the Global Prior seems to be an implementation of the Fast Weights idea. It would be great to explore that connection, or at least learn about how much the Global Prior helps.\n\nThe sparse update idea feels very much an afterthought and so do the experiments with Spanish.\n\nAll in all, this paper could be improved a lot but it is hard to argue with the strong results ...\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/1cb8c118b293cf605fdcd454c46ee638876f4131.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]}},{"tddate":null,"ddate":null,"tmdate":1509739486728,"tcdate":1508967247781,"number":96,"cdate":1509739484072,"id":"rkdU7tCaZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkdU7tCaZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Dynamic Evaluation of Neural Sequence Models","abstract":"We present methodology for using dynamic evaluation to improve neural sequence models. Models are adapted to recent history via a gradient descent based mechanism, causing them to assign higher probabilities to re-occurring sequential patterns. Dynamic evaluation outperforms existing adaptation approaches in our comparisons. Dynamic evaluation improves the state-of-the-art word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1 and 44.3 respectively, and the state-of-the-art character-level cross-entropies on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char respectively.","pdf":"/pdf/1cb8c118b293cf605fdcd454c46ee638876f4131.pdf","TL;DR":"Paper presents dynamic evaluation methodology for adaptive sequence modelling","paperhash":"anonymous|dynamic_evaluation_of_neural_sequence_models","_bibtex":"@article{\n  anonymous2018dynamic,\n  title={Dynamic Evaluation of Neural Sequence Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkdU7tCaZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper96/Authors"],"keywords":["sequence modelling","language","recurrent neural networks","adaptation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}