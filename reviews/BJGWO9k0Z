{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222576808,"tcdate":1512029859522,"number":3,"cdate":1512029859522,"id":"BkojC46xG","invitation":"ICLR.cc/2018/Conference/-/Paper149/Official_Review","forum":"BJGWO9k0Z","replyto":"BJGWO9k0Z","signatures":["ICLR.cc/2018/Conference/Paper149/AnonReviewer1"],"readers":["everyone"],"content":{"title":"novel but difficult to read and hard to assess. ","rating":"6: Marginally above acceptance threshold","review":"This paper studies a toy problem: a random binary image is generated, and treated as a maze (1=wall, 0=freely moveable space). A random starting point is generated. The task is to learn whether the center pixel is reachable from the starting point.\n\nA deep architechture is proposed to solve the problem: see fig 1. A conv net on the image is combined with that on a state image, the state being interpreted as rechable pixels. This can work if each layer expands the reachable region (the state) by one pixel if the pixel is not blocked.\n\nTwo local minima are observed: 1) the network ignores stucture and guesses if the task is solvable by aggregate statistics 2) it works as described above but propagates the rechable region on a checkerboard only.\n\nThe paper is chiefly concerned with analysing these local minima by expanding the cost function about them. This analysis is hard to follow for non experts graph theory. This is partly because many non-trivial results are mentioned with little or no explanation.\n\nThe paper is hard to evaluate. The actual setup seems somewhat arbitrary, but the method of analysing the failure modes is interesting. It may inspire more useful research in the future.\n\nIf we trust the authors, then the paper seems good because it is fairly unusual. But it is hard to determine whether the analysis is correct.","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Percolation as a Framework to Analyze the Training of Deep Networks","abstract":"In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to $128$ layers.","pdf":"/pdf/49a4d6b9bcae00e0ec511fd03471eaa095837949.pdf","TL;DR":"A toy dataset based on critical percolation in a planar graph provides an analytical window to the training dynamics of deep neural networks  ","paperhash":"anonymous|critical_percolation_as_a_framework_to_analyze_the_training_of_deep_networks","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Percolation as a Framework to Analyze the Training of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJGWO9k0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper149/Authors"],"keywords":["Deep Convolutional Networks","Loss function landscape","Graph Structured Data","Training Complexity","Theory of deep learning","Percolation theory","Anderson Localization"]}},{"tddate":null,"ddate":null,"tmdate":1512222576851,"tcdate":1511810236977,"number":2,"cdate":1511810236977,"id":"HJraEkqlz","invitation":"ICLR.cc/2018/Conference/-/Paper149/Official_Review","forum":"BJGWO9k0Z","replyto":"BJGWO9k0Z","signatures":["ICLR.cc/2018/Conference/Paper149/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good contribution, paper needs to be made clearer","rating":"7: Good paper, accept","review":"This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution. They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing \"bugs\" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths. This analysis is conducted by drawing on results from the field of critical percolation in physics.\n\nOverall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays. The analysis is very thorough and the methods described may find use in analyzing other tasks. In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms. It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one. This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions. I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds.  \n\nWith that being said, there is some work that needs to be done to make the paper clearer. In particular, many parts are quite technical and may not be accessible to a broader machine learning audience. It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix. Specifically:\n- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution. \n- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.\n- Section 6: there is lots of math here, but the main results don't obviously stand out. I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are. Interested readers can then work through the math if they want to. Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful. \n\nAlso, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs. This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs. Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary. However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks). \n\nThere are lots of small typos, please fix them. Here are a few:\n- \"For L=16, batch size of 20, ...\": not a complete sentence. \n- Right before 6.1.1: \"when the these such\" -> \"when such\"\n- Top of page 8: \"it also have a\" -> \"it also has a\", \"when encountering larger dataset\" -> \"...datasets\"\n-  First sentence of 6.2: \"we turn to the discuss a second\" -> \"we turn to the discussion of a second\"\n- etc. \n\nQuality: High\nClarity: medium-low\nOriginality: high\nSignificance: medium-high\n\nReferences:\n[1] https://arxiv.org/pdf/1602.02867.pdf\n[2] https://arxiv.org/pdf/1612.08810.pdf\n[3] https://arxiv.org/pdf/1707.03497.pdf","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Percolation as a Framework to Analyze the Training of Deep Networks","abstract":"In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to $128$ layers.","pdf":"/pdf/49a4d6b9bcae00e0ec511fd03471eaa095837949.pdf","TL;DR":"A toy dataset based on critical percolation in a planar graph provides an analytical window to the training dynamics of deep neural networks  ","paperhash":"anonymous|critical_percolation_as_a_framework_to_analyze_the_training_of_deep_networks","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Percolation as a Framework to Analyze the Training of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJGWO9k0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper149/Authors"],"keywords":["Deep Convolutional Networks","Loss function landscape","Graph Structured Data","Training Complexity","Theory of deep learning","Percolation theory","Anderson Localization"]}},{"tddate":null,"ddate":null,"tmdate":1512222576903,"tcdate":1511805959229,"number":1,"cdate":1511805959229,"id":"HJ1MEAYxG","invitation":"ICLR.cc/2018/Conference/-/Paper149/Official_Review","forum":"BJGWO9k0Z","replyto":"BJGWO9k0Z","signatures":["ICLR.cc/2018/Conference/Paper149/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting theoretical work on CNNs via toy example","rating":"7: Good paper, accept","review":"The authors are motivated by two problems: Inputting non-Euclidean data (such as graphs) into deep CNNs, and analyzing optimization properties of deep networks. In particular, they look at the problem of maze testing, where, given a grid of black and white pixels, the goal is to answer whether there is a path from a designated starting point to an ending point. \n\nThey choose to analyze mazes because they have many nice statistical properties from percolation theory. For one, the problem is solvable with breadth first search in O(L^2) time, for an L x L maze. They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem. Their architecture is a deep feedforward network where each layer takes as input two images: one corresponding to the original maze (a skip connection), and the output of the previous layer. Layers alternate between convolutional and sigmoidal. The authors discuss how this architecture can solve the problem exactly. The pictorial explanation for how the CNN can mimic BFS is interesting but I got a little lost in the 3 cases on page 4. For example, what is r? And what is the relation of the black/white and orange squares? I thought this could use a little more clarity. \n\nThough experiments, they show that there are two kinds of minima, depending on whether we allow negative initializations in the convolution kernels. When positive initializations are enforced, the network can more or less mimic the BFS behavior, but never when initializations can be negative. They offer a rigorous analysis into the behavior of optimization in each of these cases, concluding that there is an essential singularity in the cost function around the exact solution, yet learning succumbs to poor optima due to poor initial predictions in training. \n\nI thought this was an impressive paper that looked at theoretical properties of CNNs. The problem was very well-motivated, and the analysis was sharp and offered interesting insights into the problem of maze solving. What I thought was especially interesting is how their analysis can be extended to other graph problems; while their analysis was specific to the problem of maze solving, they offer an approach -- e.g. that of finding \"bugs\" when dealing with graph objects -- that can extend to other problems. I would be excited to see similar analysis of other toy problems involving graphs.\n\nOne complaint I had was inconsistent clarity: while a lot was well-motivated and straightforward to understand, I got lost in some of the details (as an example, the figure on page 4 did not initially make much sense to me). Also, in the experiments, the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initialization? Finally, there were various typos throughout (one example is \"neglect minimua\" on page 2 should be \"neglect minima\").\n\nPros: Rigorous analysis, well motivated problem, generalizable results to deep learning theory\nCons: Clarity ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Critical Percolation as a Framework to Analyze the Training of Deep Networks","abstract":"In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to $128$ layers.","pdf":"/pdf/49a4d6b9bcae00e0ec511fd03471eaa095837949.pdf","TL;DR":"A toy dataset based on critical percolation in a planar graph provides an analytical window to the training dynamics of deep neural networks  ","paperhash":"anonymous|critical_percolation_as_a_framework_to_analyze_the_training_of_deep_networks","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Percolation as a Framework to Analyze the Training of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJGWO9k0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper149/Authors"],"keywords":["Deep Convolutional Networks","Loss function landscape","Graph Structured Data","Training Complexity","Theory of deep learning","Percolation theory","Anderson Localization"]}},{"tddate":null,"ddate":null,"tmdate":1509739459224,"tcdate":1509038073897,"number":149,"cdate":1509739456573,"id":"BJGWO9k0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJGWO9k0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Critical Percolation as a Framework to Analyze the Training of Deep Networks","abstract":"In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to $128$ layers.","pdf":"/pdf/49a4d6b9bcae00e0ec511fd03471eaa095837949.pdf","TL;DR":"A toy dataset based on critical percolation in a planar graph provides an analytical window to the training dynamics of deep neural networks  ","paperhash":"anonymous|critical_percolation_as_a_framework_to_analyze_the_training_of_deep_networks","_bibtex":"@article{\n  anonymous2018critical,\n  title={Critical Percolation as a Framework to Analyze the Training of Deep Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJGWO9k0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper149/Authors"],"keywords":["Deep Convolutional Networks","Loss function landscape","Graph Structured Data","Training Complexity","Theory of deep learning","Percolation theory","Anderson Localization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}