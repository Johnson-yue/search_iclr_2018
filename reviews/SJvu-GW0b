{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222763528,"tcdate":1512124658524,"number":2,"cdate":1512124658524,"id":"Hy9gZ2CxM","invitation":"ICLR.cc/2018/Conference/-/Paper777/Official_Review","forum":"SJvu-GW0b","replyto":"SJvu-GW0b","signatures":["ICLR.cc/2018/Conference/Paper777/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but limited validation. Also sample complexity may be exponential in graph degree.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a novel way of embedding graph structure into a sequence that can have an unbounded length. \n\nThere has been a significant amount of prior work (e.g. d graph convolutional neural networks) for signals supported on a specific graph. This paper on the contrary tries to encode the topology of a graph using a dynamical system created by the graph and randomization. \n\nThe main theorem is that the created dynamical system can be used to reverse engineer the graph topology for any digraph. \nAs far as I understood, the authors are doing essentially reverse directed graphical model learning. In classical learning of directed graphical models (or causal DAGs) one wants to learn the structure of a graph from observed data created by this graph inducing conditional independencies on data. This procedure is creating a dynamical system that (following very closely previous work) estimates conditional directed information for every pair of vertices u,v and can find if an edge is present from the observed trajectory. \nThe recovery algorithm is essentially previous work (but the application to graph recovery is new).\n\nThe authors state:\n``Estimating conditional directed information efficiently from samples is itself an active area of research Quinn et al. (2011), but simple plug-in estimators with a standard kernel density estimator will be consistent.''\n\nOne thing that is missing here is that the number of samples needed could be exponential in the degrees of the graph. Therefore, it is not clear at all that high-dimensional densities or directed information can be estimated from a number of samples that is polynomial in the dimension (e.g. graph degree).\n\nThis is related to the second limitation, that there is no sample complexity bounds presented only an asymptotic statement. \n\nOne remark is that there are many ways to represent a finite graph with a sequence that can be decoded back to the graph (and of course if there is no bound on the graph size, there will be no bound on the size of the sequence). For example, one could take the adjacency matrix and sequentially write down one row after the other (perhaps using a special symbol to indicate 'next row'). Many other simple methods can be obtained also, with a size of sequence being polynomial (in fact linear) in the size of the graph. I understand that such trivial representations might not work well with RNNs but they would satisfy stronger versions of Theorem 1 with optimal size. \nOn the contrary it was not clear how the proposed sequence will scale in the graph size. \n\n\nAnother remark is that it seems that GCNN and this paper solve different problems. \nGCNNs want to represent graph-supported signals (on a fixed graph) while this paper tries to represent the topology of a graph, which seems different. \n\n\nThe experimental evaluation was somewhat limited and that is the biggest problem from a practical standpoint. It is not clear why one would want to use these sequences for solving MVC. There are several graph classification tasks that try to use the graph structure (as well as possibly other features) see eg the bioinformatics \nand other applications. Literature includes for example:\nGraph Kernels by S.V.N. Vishwanathan et al. \nDeep graph kernels (Yanardag & Vishwanathan and graph invariant kernels (Orsini et al.),\nwhich use counts of small substructures as features. \n\nThe are many benchmarks of graph classification tasks where the proposed representation could be useful but significantly more validation work would be needed to make that case. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph2Seq: Scalable Learning Dynamics for Graphs","abstract":"Neural networks are increasingly used as a general purpose approach to learning algorithms over graph structured data. However, techniques for representing graphs as real-valued vectors are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but as we show in this paper, these methods have difficulty generalizing to large graphs. In this paper we propose Graph2Seq, an embedding framework that represents graphs as an infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq naturally scales to graphs of arbitrary size. Moreover, through analysis of a formal computational model we show that an unbounded sequence is necessary for scalability. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequence. Experimental evaluations of Graph2Seq on a variety of combinatorial optimization problems show strong generalization and strict improvement over state of the art. ","pdf":"/pdf/5fd9800628deca2200033cee89cd2ee195c1350f.pdf","paperhash":"anonymous|graph2seq_scalable_learning_dynamics_for_graphs","_bibtex":"@article{\n  anonymous2018graph2seq:,\n  title={Graph2Seq: Scalable Learning Dynamics for Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvu-GW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper777/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222763575,"tcdate":1511802428219,"number":1,"cdate":1511802428219,"id":"HyNr86Ylz","invitation":"ICLR.cc/2018/Conference/-/Paper777/Official_Review","forum":"SJvu-GW0b","replyto":"SJvu-GW0b","signatures":["ICLR.cc/2018/Conference/Paper777/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The motivation for this paper is unclear. The writing is problematic and evaluations are not sufficient.","rating":"4: Ok but not good enough - rejection","review":"This paper proposes to represent nodes in graphs by time series. This is an interesting idea but the results presented in the paper are very preliminary.\nExperiments are only conducted on synthetic data with very small sizes.\nIn Section 5.1, I did not understand the construction of the graph. What means 'all the vertices are disjoint'? Then I do not understand why the vertices of G_i form the optimum.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Graph2Seq: Scalable Learning Dynamics for Graphs","abstract":"Neural networks are increasingly used as a general purpose approach to learning algorithms over graph structured data. However, techniques for representing graphs as real-valued vectors are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but as we show in this paper, these methods have difficulty generalizing to large graphs. In this paper we propose Graph2Seq, an embedding framework that represents graphs as an infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq naturally scales to graphs of arbitrary size. Moreover, through analysis of a formal computational model we show that an unbounded sequence is necessary for scalability. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequence. Experimental evaluations of Graph2Seq on a variety of combinatorial optimization problems show strong generalization and strict improvement over state of the art. ","pdf":"/pdf/5fd9800628deca2200033cee89cd2ee195c1350f.pdf","paperhash":"anonymous|graph2seq_scalable_learning_dynamics_for_graphs","_bibtex":"@article{\n  anonymous2018graph2seq:,\n  title={Graph2Seq: Scalable Learning Dynamics for Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvu-GW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper777/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739108772,"tcdate":1509134702756,"number":777,"cdate":1509739106109,"id":"SJvu-GW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJvu-GW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Graph2Seq: Scalable Learning Dynamics for Graphs","abstract":"Neural networks are increasingly used as a general purpose approach to learning algorithms over graph structured data. However, techniques for representing graphs as real-valued vectors are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but as we show in this paper, these methods have difficulty generalizing to large graphs. In this paper we propose Graph2Seq, an embedding framework that represents graphs as an infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq naturally scales to graphs of arbitrary size. Moreover, through analysis of a formal computational model we show that an unbounded sequence is necessary for scalability. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequence. Experimental evaluations of Graph2Seq on a variety of combinatorial optimization problems show strong generalization and strict improvement over state of the art. ","pdf":"/pdf/5fd9800628deca2200033cee89cd2ee195c1350f.pdf","paperhash":"anonymous|graph2seq_scalable_learning_dynamics_for_graphs","_bibtex":"@article{\n  anonymous2018graph2seq:,\n  title={Graph2Seq: Scalable Learning Dynamics for Graphs},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJvu-GW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper777/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}