{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222707577,"tcdate":1511697613839,"number":3,"cdate":1511697613839,"id":"Hk8Ah7_eG","invitation":"ICLR.cc/2018/Conference/-/Paper640/Official_Review","forum":"HJaDJZ-0W","replyto":"HJaDJZ-0W","signatures":["ICLR.cc/2018/Conference/Paper640/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Methods for inducing sparsity in terms of blocks of weights in neural networks.","rating":"5: Marginally below acceptance threshold","review":"This paper presents methods for inducing sparsity in terms of blocks of weights in neural networks which aims to combine benefits of sparsity and faster access based on computing architectures. This is achieved by pruning blocks of weights and group lasso regularization. It is demonstrated empirically that model size can be reduced by upto 10 times with some loss in prediction accuracy.\n\nThough the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be :\n\n- The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved. Also, the algorithm for computing the pruning threshold is based on a new set of hyper-parameters. It is not clear under what conditions the above settings will (not) work.\n\n - For the group lasso method, since there are many ways to group the variable, it is not clear how the variables are grouped. Is there a reasoning behind a particular grouping of the variables. Individually, group lasso does not seem to work, and gives much worse results. The reasons for worse performance could be investigated. It is possible that important weights are in different groups, and group sparsity is forcing some of them to be zero, and hence leading to worse results. It would be insightful to explain the kind of solver used for group lasso regularization, and if that works for large-scale problems.\n\n - The results for various kinds of sparsity are unclear in the sense that it is not clear how to set the block size a-priori for having minimum reduction in accuracy and still significant sparsity without having to repeat the process for various choices.\n\nOverall, the paper does not seem to present novel ideas, and is mainly focused on evaluating the impact of block-based sparsity instead of weight pruning by Han etal. As mentioned in Section 2, regularization has been used earlier to achieve sparsity in deep networks. In this view the significance over existing work is relatively narrow, and no explicit comparison with existing methods is provided. It is possible that an existing method leads to pruning method such as by Han etal. leads to 8x decrease in model size while retaining the accuracy, while the proposed method leads to 10x decrease while also decreasing the accuracy by 10%. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Block-Sparse Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80% to 90% with small loss in accuracy. This allows us to reduce the model size by roughly 10x. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32x32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.\n","pdf":"/pdf/106043f36180429a754e9211d80c8e09c7102f82.pdf","TL;DR":"We show the RNNs can be pruned to induce block sparsity which improves speedup for sparse operations on existing hardware","paperhash":"anonymous|blocksparse_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018block-sparse,\n  title={Block-Sparse Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJaDJZ-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper640/Authors"],"keywords":["Pruning","block sparsity","structured sparsity","Recurrent Neural Networks","Speech Recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222707613,"tcdate":1511331064674,"number":2,"cdate":1511331064674,"id":"rJWbr5zxM","invitation":"ICLR.cc/2018/Conference/-/Paper640/Official_Review","forum":"HJaDJZ-0W","replyto":"HJaDJZ-0W","signatures":["ICLR.cc/2018/Conference/Paper640/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper proposes to 0 out blocks of weights while training RNNs and further aid the process by utilizing group lasso regularization. As demonstrated empirically, the learned networks are sparse and can be run efficiently while showing minimal loss of accuracy.","rating":"7: Good paper, accept","review":"Compressing/pruning of neural networks is required to enable running on devices with limited compute resources. While previous works have proposed to 0 out weights, especially for the case of RNNs, in an unstructured way, the current paper proposes to 0 out weights blocks at a time via thresholding. The process is further aided by utilizing group lasso regularization. The resulting networks are sparse, memory efficient and can be run more efficiently while resulting in minimal loss in accuracy when compared to networks learned with full density. The proposed techniques are evaluated on RNNs for speech recognition and benefits clearly spelled out. Further experiments thresh out how much benefit is provided by thresholding (block sparsity) and regularizing via group lasso.\n\nThe paper quality seems high, presentation clarity sufficient, the ideas presented (especially the use of group lasso) well thought out and original, and the work seems significant. If I were to nitpick then I would suggest trying out these techniques on RNNs meant for something other than speech recognition (machine translation perhaps?).","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Block-Sparse Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80% to 90% with small loss in accuracy. This allows us to reduce the model size by roughly 10x. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32x32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.\n","pdf":"/pdf/106043f36180429a754e9211d80c8e09c7102f82.pdf","TL;DR":"We show the RNNs can be pruned to induce block sparsity which improves speedup for sparse operations on existing hardware","paperhash":"anonymous|blocksparse_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018block-sparse,\n  title={Block-Sparse Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJaDJZ-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper640/Authors"],"keywords":["Pruning","block sparsity","structured sparsity","Recurrent Neural Networks","Speech Recognition"]}},{"tddate":null,"ddate":null,"tmdate":1512222707653,"tcdate":1511295372747,"number":1,"cdate":1511295372747,"id":"ByH5FWfgf","invitation":"ICLR.cc/2018/Conference/-/Paper640/Official_Review","forum":"HJaDJZ-0W","replyto":"HJaDJZ-0W","signatures":["ICLR.cc/2018/Conference/Paper640/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good start, but needs better comparison against existing work. ","rating":"5: Marginally below acceptance threshold","review":"The authors propose a block sparsity pruning approach to compress RNNs. There are several ways. One is using  group LASSO to promote sparsity. The other is to prune, but with a very specialized schedule as to the pruning and pruning weight, motivated by the work of Narang et al 2017 for non-group sparsity.  The block sizes used in experiments are about 4x4, 8x8, up to 32 x 32. The relative performance degradation ranges between 10% to 96%, depending on the method, severity of compression, and task. The speedup for a matrix multiply is between 1.5x to 4x, and varies according to batch size.\n\nThis is certainly a well-motivated problem, and the procedure is simple but makes sense. Also, the paper contains a good overview of related work in compression, and is not hiding anything.  One paper that is missing is\n\nScardapane, S., Comminiello, D., Hussain, A., & Uncini, A. (2017). Group sparse regularization for deep neural networks. Neurocomputing, 241, 81-89.\n\nA major complaint is the lack of comparison of results against other compression techniques. Since it is a block sparsity approach, and the caching / fetching overhead is reduced, one does not need to have competitively superior results to basic pruning approaches, but one should come close on the same types of problems. This is not well presented. Additionally, the speedup should be superior to the sparse methods, which is also not shown (against previously published results, not personally run experiments.) \n\nAnother issue I find is the general writing, especially for the results section, is not entirely clear. For example, when showing a relative performance degradation of 96%, why is that happening? Is that significant? What should an implementer be aware of in order to avoid that? \n\nFinally, a meta issue to address is, if the block size is small (realistically, less than 64 x 64) usually I doubt there will be significant speedup. (4x is not considered terribly significant.) What we need to see is what happens when, say, block size is 256 x 256? What is the performance degradation? If you can give 10x speedup in the feedforward part (testing only) then if you have a 10% degradation in performance that might be acceptable in certain applications. \n\nOverall, I believe this is a very promising and well-motivated work, but needs to be \"marinated\" further to be publishable. Actually, with the inclusion of 2-3 tables against known, previously published results, and clearly stated benefits, I would change my review to accept. \n\nMinor complaints:\n\nThe axis labels/numbers in figure 2 are too small. \n\nAlso, please reread for some grammar / writing issues (last paragraph of 1st page, caption of figure 2, for example)\n\nI think also figure 2 should be rerun with more trials. The noise in the curves are not showing a highly interpretable trend. (Though, actually, batch size = 8 being super low might have a significance; can this be explained?)\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Block-Sparse Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80% to 90% with small loss in accuracy. This allows us to reduce the model size by roughly 10x. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32x32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.\n","pdf":"/pdf/106043f36180429a754e9211d80c8e09c7102f82.pdf","TL;DR":"We show the RNNs can be pruned to induce block sparsity which improves speedup for sparse operations on existing hardware","paperhash":"anonymous|blocksparse_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018block-sparse,\n  title={Block-Sparse Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJaDJZ-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper640/Authors"],"keywords":["Pruning","block sparsity","structured sparsity","Recurrent Neural Networks","Speech Recognition"]}},{"tddate":null,"ddate":null,"tmdate":1509739186402,"tcdate":1509130085109,"number":640,"cdate":1509739183742,"id":"HJaDJZ-0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJaDJZ-0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Block-Sparse Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are used in state-of-the-art models in domains such as speech recognition, machine translation, and language modelling. Sparsity is a technique to reduce compute and memory requirements of deep learning models. Sparse RNNs are easier to deploy on devices and high-end server processors. Even though sparse operations need less compute and memory relative to their dense counterparts, the speed-up observed by using sparse operations is less than expected on different hardware platforms. In order to address this issue, we investigate two different approaches to induce block sparsity in RNNs: pruning blocks of weights in a layer and using group lasso regularization to create blocks of weights with zeros. Using these techniques, we demonstrate that we can create block-sparse RNNs with sparsity ranging from 80% to 90% with small loss in accuracy. This allows us to reduce the model size by roughly 10x. Additionally, we can prune a larger dense network to recover this loss in accuracy while maintaining high block sparsity and reducing the overall parameter count. Our technique works with a variety of block sizes up to 32x32. Block-sparse RNNs eliminate overheads related to data storage and irregular memory accesses while increasing hardware efficiency compared to unstructured sparsity.\n","pdf":"/pdf/106043f36180429a754e9211d80c8e09c7102f82.pdf","TL;DR":"We show the RNNs can be pruned to induce block sparsity which improves speedup for sparse operations on existing hardware","paperhash":"anonymous|blocksparse_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018block-sparse,\n  title={Block-Sparse Recurrent Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJaDJZ-0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper640/Authors"],"keywords":["Pruning","block sparsity","structured sparsity","Recurrent Neural Networks","Speech Recognition"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}