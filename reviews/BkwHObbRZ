{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222721420,"tcdate":1511801683398,"number":3,"cdate":1511801683398,"id":"SJjI7pKlz","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer3"],"readers":["everyone"],"content":{"title":"An interesting tensor factorization-type method for learning one hidden-layer neural network","rating":"7: Good paper, accept","review":"This paper proposes a tensor factorization-type method for learning one hidden-layer neural network. The most interesting part is the Hermite polynomial expansion of the activation function. Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem. They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. At last, they also establish the sample complexity for recovery.\n\nThe organization and presentation of the paper need some improvement. For example, the authors defer many technical details. To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages.\n\nThere are also some typos: For example, the dimension of a is inconsistent. In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector. On Page 8, P(B) should be a degree-4 polynomial of B.\n\nThe paper does not contains any experimental results on real data.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/8831ca3f1bd4cb1518757767b08f89acf35b7792.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1512222721457,"tcdate":1511773338173,"number":2,"cdate":1511773338173,"id":"SyfsN8tef","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper and solid contribution: Accept.","rating":"9: Top 15% of accepted papers, strong accept","review":"This paper studies the problem of learning one-hidden layer neural networks and is a theory paper. A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix. Simulations confirm the findings.\n\nOverall, pretty interesting result and solid contribution. The paper also raises good questions for future works. For instance, is designing alternative loss function useful in practice? In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos.\n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows? If so they should clarify this otherwise it confuses the reader a bit.\n2) What can be said about rate of convergence in terms of network parameters? Currently a generic bound is employed which is not very insightful in my opinion.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/8831ca3f1bd4cb1518757767b08f89acf35b7792.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1512222721499,"tcdate":1511657270853,"number":1,"cdate":1511657270853,"id":"rk0Ek5vgM","invitation":"ICLR.cc/2018/Conference/-/Paper697/Official_Review","forum":"BkwHObbRZ","replyto":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference/Paper697/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall an interesting heuristic approach to solving SGD problems on one particular type of data, with limited practical applications","rating":"6: Marginally above acceptance threshold","review":"This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization). First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD\n\nOverall the paper is well written. The authors first introduce their suggested loss function and then go into details about what inspired its creation. I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful\n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated. On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used. I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework\n- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima. Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched. \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem\n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better.\n\nMinor: fix margins in formula 2.7. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/8831ca3f1bd4cb1518757767b08f89acf35b7792.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]}},{"tddate":null,"ddate":null,"tmdate":1509739154411,"tcdate":1509132350987,"number":697,"cdate":1509739151736,"id":"BkwHObbRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkwHObbRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning One-hidden-layer Neural Networks with Landscape Design","abstract":"We consider the problem of learning a one-hidden-layer neural network: we assume the input x is from Gaussian distribution and the label $y = a \\sigma(Bx) + \\xi$, where a is a nonnegative vector and  $B$ is a full-rank weight matrix, and $\\xi$ is a noise vector. We first give an analytic formula for the population risk of the standard squared loss and demonstrate that it implicitly attempts to decompose a sequence of low-rank tensors simultaneously. \n\t\nInspired by the formula, we design a non-convex objective function $G$ whose landscape is guaranteed to have the following properties:\t\n\n1. All local minima of $G$ are also global minima.\n2. All global minima of $G$ correspond to the ground truth parameters.\n3. The value and gradient of $G$ can be estimated using samples.\n\t\nWith these properties, stochastic gradient descent on $G$ provably converges to the global minimum and learn the ground-truth parameters. We also prove finite sample complexity results and validate the results by simulations. ","pdf":"/pdf/8831ca3f1bd4cb1518757767b08f89acf35b7792.pdf","TL;DR":"The paper analyzes the optimization landscape of one-hidden-layer neural nets and designs a new objective that provably has no spurious local minimum. ","paperhash":"anonymous|learning_onehiddenlayer_neural_networks_with_landscape_design","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning One-hidden-layer Neural Networks with Landscape Design},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkwHObbRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper697/Authors"],"keywords":["theory","non-convex optimization","loss surface"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}