{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222627915,"tcdate":1512000678303,"number":3,"cdate":1512000678303,"id":"rkCi3T3lG","invitation":"ICLR.cc/2018/Conference/-/Paper378/Official_Review","forum":"H1eJxngCW","replyto":"H1eJxngCW","signatures":["ICLR.cc/2018/Conference/Paper378/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Need some more analysis / clarifications.","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThe paper proposes a new dataset for reading comprehension, called DuoRC. The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story. The DuoRC dataset offers the following challenges compared to the existing reading comprehension (RC) datasets – 1) low lexical overlap between questions and their corresponding passages, 2) requires use of common-sense knowledge to answer the question, 3) requires reasoning across multiples sentences to answer the question, 4) consists of those questions as well that cannot be answered from the given passage. The paper experiments with two types of models – 1) a model which only predicts the span in a document and 2) a model which generates the answer after predicting the span. Both these models are built off of an existing model on SQuAD – the Bidirectional Attention Flow (BiDAF) model. The experimental results show that the span based model performs better than the model which generates the answers. But the accuracy of both the models is significantly lower than that of their base model (BiDAF) on SQuAD, demonstrating the difficulty of the DuoRC dataset. \n\t\nStrengths:\n\n1.\tThe data collection process is interesting. The challenges in the proposed dataset as outlined in the paper seem worth pushing for.\n2.\tThe paper is well written making it easy to follow.\n3.\tThe experiments and analysis presented in the paper are insightful.\n\nWeaknesses:\n\n1.\tIt would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset, other than the size.\n2.\tThe dataset is motivated as consisting of four challenges (described in the summary above) that do not exist in the existing RC datasets. However, the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges. Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset.\n3.\tIt is not clear from the paper how should the questions which are unanswerable be evaluated. As in, what should be the ground-truth answer against which the answers should such questions be evaluated. Clearly, string matching would not work because a model could say “don’t know” whereas some other model could say “unanswerable”. So, does the training data have a particular string as the ground truth answer for such questions, so that a model can just be trained to spit out that particular string when it thinks it can’t answer the questions?  \n4.\tOne of the observations made in the paper is that “training on one dataset and evaluating on the other results in a drop in the performance.” However, in table 4, evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC. This seems to be in conflict with the observation drawn in the paper. Could authors please clarify this? Also, could authors please throw some light on why this might be happening?\n5.\tIn the third phase of data collection (Paraphrase RC), was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2, or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3?\n6.\tTypo: Dataset section, phrases --> phases\n\nOverall: The challenges proposed in the DuoRC dataset are interesting. The paper is well written and the experiments are interesting. However, there are some questions (as mentioned in the Weaknesses section) which need to be clarified before I can recommend acceptance for the paper.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension","abstract":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize corresponding answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different level of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating background knowledge not available in the given text. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other Reading Comprehension style datasets to explore novel neural approaches for studying language understanding.","pdf":"/pdf/51649aa4d31bbf7471cbbc241f63154ecd9f8fcc.pdf","TL;DR":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) containing 186,089 human-generated QA pairs created from a collection of 7680 pairs of parallel movie plots and introduce a RC task of reading one version of the plot and answering questions created from the other version; thus by design, requiring complex reasoning and deeper language understanding to overcome the poor lexical overlap between the plot and the question.","paperhash":"anonymous|duorc_towards_complex_language_understanding_with_paraphrased_reading_comprehension","_bibtex":"@article{\n  anonymous2018duorc:,\n  title={DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1eJxngCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper378/Authors"],"keywords":["reading comprehension","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1512222627955,"tcdate":1511823386081,"number":2,"cdate":1511823386081,"id":"SJzXOG9eG","invitation":"ICLR.cc/2018/Conference/-/Paper378/Official_Review","forum":"H1eJxngCW","replyto":"H1eJxngCW","signatures":["ICLR.cc/2018/Conference/Paper378/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Useful dataset for reading comprehension","rating":"6: Marginally above acceptance threshold","review":"1) This paper proposes a new dataset for Reading Comprehension (RC). Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly. \n\nOverall, I think this dataset is very useful for RC. The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs.\n\n2) I have the questions as follows:\ni) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots. However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC. Is there any mistake in the description of the abstract?\n\nii) What is the standard for creating the questions? I noticed that the time and location information was used to generate questions sometime, but sometimes these kinds of questions are ignored.\n\niii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents? \n\niv) What is the average length of the answers in both ParaphraseRC and SelfRC? I found that the answers are usually very short, which is more like factoid QA. It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge. \n\nv) During NLP pre-processing (section 4), how do you prune the irrelevant documents?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension","abstract":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize corresponding answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different level of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating background knowledge not available in the given text. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other Reading Comprehension style datasets to explore novel neural approaches for studying language understanding.","pdf":"/pdf/51649aa4d31bbf7471cbbc241f63154ecd9f8fcc.pdf","TL;DR":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) containing 186,089 human-generated QA pairs created from a collection of 7680 pairs of parallel movie plots and introduce a RC task of reading one version of the plot and answering questions created from the other version; thus by design, requiring complex reasoning and deeper language understanding to overcome the poor lexical overlap between the plot and the question.","paperhash":"anonymous|duorc_towards_complex_language_understanding_with_paraphrased_reading_comprehension","_bibtex":"@article{\n  anonymous2018duorc:,\n  title={DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1eJxngCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper378/Authors"],"keywords":["reading comprehension","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1512222627994,"tcdate":1511813559615,"number":1,"cdate":1511813559615,"id":"B1Ja-l9gM","invitation":"ICLR.cc/2018/Conference/-/Paper378/Official_Review","forum":"H1eJxngCW","replyto":"H1eJxngCW","signatures":["ICLR.cc/2018/Conference/Paper378/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Useful dataset for reading comprehension","rating":"7: Good paper, accept","review":"This paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and document. The paper rightly mentions that existing reading comprehension datasets (e.g. SQuAD) where the current methods are already performing at the human level largely due to large lexical overlap between question and document. The authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from Wikipedia and IMDB. \n\nThis paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap. The authors also present an analysis of the data by applying one of the SOTA techniques on SQuAD to this data. They also analyze the effect of various span-identification steps and preprocessing steps on the performance. Overall, this paper contributes a useful new dataset that can be quite useful for reading comprehension.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension","abstract":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize corresponding answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different level of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating background knowledge not available in the given text. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other Reading Comprehension style datasets to explore novel neural approaches for studying language understanding.","pdf":"/pdf/51649aa4d31bbf7471cbbc241f63154ecd9f8fcc.pdf","TL;DR":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) containing 186,089 human-generated QA pairs created from a collection of 7680 pairs of parallel movie plots and introduce a RC task of reading one version of the plot and answering questions created from the other version; thus by design, requiring complex reasoning and deeper language understanding to overcome the poor lexical overlap between the plot and the question.","paperhash":"anonymous|duorc_towards_complex_language_understanding_with_paraphrased_reading_comprehension","_bibtex":"@article{\n  anonymous2018duorc:,\n  title={DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1eJxngCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper378/Authors"],"keywords":["reading comprehension","question answering"]}},{"tddate":null,"ddate":null,"tmdate":1509739335704,"tcdate":1509109720016,"number":378,"cdate":1509739333044,"id":"H1eJxngCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1eJxngCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension","abstract":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize corresponding answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different level of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating background knowledge not available in the given text. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other Reading Comprehension style datasets to explore novel neural approaches for studying language understanding.","pdf":"/pdf/51649aa4d31bbf7471cbbc241f63154ecd9f8fcc.pdf","TL;DR":"We propose DuoRC, a novel dataset for Reading Comprehension (RC) containing 186,089 human-generated QA pairs created from a collection of 7680 pairs of parallel movie plots and introduce a RC task of reading one version of the plot and answering questions created from the other version; thus by design, requiring complex reasoning and deeper language understanding to overcome the poor lexical overlap between the plot and the question.","paperhash":"anonymous|duorc_towards_complex_language_understanding_with_paraphrased_reading_comprehension","_bibtex":"@article{\n  anonymous2018duorc:,\n  title={DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1eJxngCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper378/Authors"],"keywords":["reading comprehension","question answering"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}