{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222768084,"tcdate":1511819438327,"number":3,"cdate":1511819438327,"id":"S1I3_bqgM","invitation":"ICLR.cc/2018/Conference/-/Paper792/Official_Review","forum":"Bya8fGWAZ","replyto":"Bya8fGWAZ","signatures":["ICLR.cc/2018/Conference/Paper792/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Useful extension to Value Iteration Networks to extend value of explicitly incorporate VI into problems with dynamic state","rating":"7: Good paper, accept","review":"ORIGINALITY & SIGNIFICANCE\n\nThe authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network. The authors point out that the original \"value iteration network” (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network.  It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps. Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems. The work is therefore original and significant.\n\nThe algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability. The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster.  The M in MVProp in particular seems to be very useful in scaling up to the large grids. The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time.\n\n\nQUALITY\n\nThe symbol d_{rew} is never defined — what does “new” stand for? It appears to be the number of latent convolutional filters or channels generated by the state embedding network. \n\nSection 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }.\nBased on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer. \n\nIn equation 1, it wasn’t obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation?\nThe h( \\Phi( x ), v^{k-1} ) sort of makes sense …  value is only calculated with respect to only the observation of the maze obstacles but the policy \\pi is calculated with respect to the joint  observation and agent state. \n\nThe expression \n\n   h_{aid} ( \\phi(0), v )   =   <  Wa,   [ \\phi(o) ; v ]   >   +   b\n\nmakes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions). I gather the tensor Wa = R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding \\phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time? \n\nI didn’t understand the r^in, r&out representation in section 4.1. These are given by the domain?\n\nI did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision.\n\nA bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful.\n\n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Value Propagation Networks","abstract":"We present Value Propagation (VProp), a parameter-efficient differentiable planning module built on Value Iteration which can successfully be trained in a reinforcement learning fashion to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We evaluate on configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes. Furthermore, we show that the module and its variants provide a simple way to learn to plan when adversarial agents are present and the environment is stochastic, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems.","pdf":"/pdf/bd7c05ab426844e16c2cc9b3382cbb206d494523.pdf","TL;DR":"We propose Value Propagation, a novel end-to-end planner which can learn to solve 2D navigation tasks via Reinforcement Learning, and that generalize to larger and dynamic environments.","paperhash":"anonymous|value_propagation_networks","_bibtex":"@article{\n  anonymous2018value,\n  title={Value Propagation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bya8fGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper792/Authors"],"keywords":["Learning to plan","Reinforcement Learning","Value Iteration","Navigation","Convnets"]}},{"tddate":null,"ddate":null,"tmdate":1512222768127,"tcdate":1511751446304,"number":2,"cdate":1511751446304,"id":"rJRfJZKxf","invitation":"ICLR.cc/2018/Conference/-/Paper792/Official_Review","forum":"Bya8fGWAZ","replyto":"Bya8fGWAZ","signatures":["ICLR.cc/2018/Conference/Paper792/AnonReviewer4"],"readers":["everyone"],"content":{"title":"An extension of Value Iteration Network; the writing needs to be greatly improved ","rating":"5: Marginally below acceptance threshold","review":"The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment. The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path. The approaches are evaluated in grid worlds with and without other agents.\n\nI had some difficulty to understand the paper because of its presentation and writing (see below). \n\nIn Tamar's work, a mapping from observation to reward is learned. It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5. As a consequence, those two methods need to take as input a new reward function for every new map. Is that correct?\nI think this could explain the better experimental results\n\nIn the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper. Why did you use your own implementation of VIN and not Tamar et al.'s, which is publicly shared as far as I know?\n\nI think the writing needs to be improved on the following points:\n- The abstract doesn't fit well the content of the paper. For instance, \"its variants\" is confusing because there is only other variant to VProp. \"Adversarial agents\" is also misleading because those agents act like automata.\n\n- The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable.\n\n- The writing should be careful, e.g., \nvalue iteration is presented as a learning algorithm (which in my opinion is not) \n\\pi^* is defined as a distribution over state-action space and then \\pi is defined as a function; ...\n\n- The mathematical writing should be more rigorous, e.g., \np.2:\nT: s \\to a \\to s', \\pi : s \\to a\nA denotes a set and its cardinal\nIn (1), shouldn't it be \\Phi(o)? all the new terms should be explained\np. 3:\ndefinition of T and R \nshouldn't V_{ij}^k depend on Q_{aij}^k?\nT_{::aij} should be defined\nIn the definition of h_{aij}, should \\Phi and b be indexed by a?\n\n- The typos and other issues should be fixed:\np. 3:\nK iteration\nwith capable\np.4:\nclose 0\np.5:\nour our\ns^{t+1} should be defined like the other terms\n\"The state is represented by the coordinates of the agent and 2D environment observation\" should appear much earlier in the paper. \n\"\\pi_\\theta described in the previous sections\", notation \\pi_\\theta appears the first time here...\n3x3 -> 3 \\times 3\nofB\nV_{\\theta^t w^t}\np.6:\nthe the\nFig.2's caption:\nWhat does \"both cases\" refer to? They are three models.\nReferences:\net al.\nYI WU\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Value Propagation Networks","abstract":"We present Value Propagation (VProp), a parameter-efficient differentiable planning module built on Value Iteration which can successfully be trained in a reinforcement learning fashion to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We evaluate on configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes. Furthermore, we show that the module and its variants provide a simple way to learn to plan when adversarial agents are present and the environment is stochastic, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems.","pdf":"/pdf/bd7c05ab426844e16c2cc9b3382cbb206d494523.pdf","TL;DR":"We propose Value Propagation, a novel end-to-end planner which can learn to solve 2D navigation tasks via Reinforcement Learning, and that generalize to larger and dynamic environments.","paperhash":"anonymous|value_propagation_networks","_bibtex":"@article{\n  anonymous2018value,\n  title={Value Propagation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bya8fGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper792/Authors"],"keywords":["Learning to plan","Reinforcement Learning","Value Iteration","Navigation","Convnets"]}},{"tddate":null,"ddate":null,"tmdate":1512222768170,"tcdate":1511749714464,"number":1,"cdate":1511749714464,"id":"Sy5I_xKgM","invitation":"ICLR.cc/2018/Conference/-/Paper792/Official_Review","forum":"Bya8fGWAZ","replyto":"Bya8fGWAZ","signatures":["ICLR.cc/2018/Conference/Paper792/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An extension to value-iteration networks that improves performance on grid-worlds","rating":"4: Ok but not good enough - rejection","review":"The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training.\n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way\n\nThe paper is fairly clear and these extensions are reasonable.  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact.  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.   So, making improvements to help solve grid-worlds better is not so motivating.  It may be possible to motivate and demonstrate the methods of this paper in other domains, however.  The work on dynamic environments was an interesting step:  it would have been interesting to see how the \"models\" learned for the dynamic environments differed from those for static environments.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Value Propagation Networks","abstract":"We present Value Propagation (VProp), a parameter-efficient differentiable planning module built on Value Iteration which can successfully be trained in a reinforcement learning fashion to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We evaluate on configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes. Furthermore, we show that the module and its variants provide a simple way to learn to plan when adversarial agents are present and the environment is stochastic, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems.","pdf":"/pdf/bd7c05ab426844e16c2cc9b3382cbb206d494523.pdf","TL;DR":"We propose Value Propagation, a novel end-to-end planner which can learn to solve 2D navigation tasks via Reinforcement Learning, and that generalize to larger and dynamic environments.","paperhash":"anonymous|value_propagation_networks","_bibtex":"@article{\n  anonymous2018value,\n  title={Value Propagation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bya8fGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper792/Authors"],"keywords":["Learning to plan","Reinforcement Learning","Value Iteration","Navigation","Convnets"]}},{"tddate":null,"ddate":null,"tmdate":1509739098553,"tcdate":1509134933264,"number":792,"cdate":1509739095877,"id":"Bya8fGWAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bya8fGWAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Value Propagation Networks","abstract":"We present Value Propagation (VProp), a parameter-efficient differentiable planning module built on Value Iteration which can successfully be trained in a reinforcement learning fashion to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We evaluate on configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes. Furthermore, we show that the module and its variants provide a simple way to learn to plan when adversarial agents are present and the environment is stochastic, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems.","pdf":"/pdf/bd7c05ab426844e16c2cc9b3382cbb206d494523.pdf","TL;DR":"We propose Value Propagation, a novel end-to-end planner which can learn to solve 2D navigation tasks via Reinforcement Learning, and that generalize to larger and dynamic environments.","paperhash":"anonymous|value_propagation_networks","_bibtex":"@article{\n  anonymous2018value,\n  title={Value Propagation Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bya8fGWAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper792/Authors"],"keywords":["Learning to plan","Reinforcement Learning","Value Iteration","Navigation","Convnets"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}