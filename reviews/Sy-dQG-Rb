{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222770138,"tcdate":1511808889271,"number":3,"cdate":1511808889271,"id":"rkZtyy5gf","invitation":"ICLR.cc/2018/Conference/-/Paper806/Official_Review","forum":"Sy-dQG-Rb","replyto":"Sy-dQG-Rb","signatures":["ICLR.cc/2018/Conference/Paper806/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A review of \"Neural Speed Reading via Skim-RNN\"","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes a skim-RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference.\n\nPros.\n-\tThe idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive.\n-\tThe paper is clearly written with enough explanations about the proposal method and the novelty.\n-\tOne of the most difficult problems of this approach (non-differentiable) is elegantly solved by employing gumbel-softmax\n-\tThe effectiveness (mainly inference speed improvement with CPU) is validated by various experiments. The examples (Table 3 and Figure 6) show that the skimming process is appropriately performed (skimmed unimportant words while fully read relevant words etc.)\nCons.\n-\tThe idea is quite simple and the novelty is incremental by considering the difference from skip-RNN.\n-\tNo comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel-softmax may require more iterations).\n\nComments:\n-\tSection 1, Introduction, 2nd paragraph: ‘peed’ -> ‘speed’(?)\n-\tEquation (5): It would be better to explain why it uses the Gumbel distribution. To make (5) behave like argmax, only temperature parameter seems to be enough.\n-\tSection 4.1: What is “global training step”?\n-\tSection 4.2, “We also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost.”: This seems to be very interesting phenomena. Is there some discussion of why skim-LSTM is more stable?\n-\tSection 4.2, the last paragraph: “Table 6 shows” -> “Figure 6 shows”\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1512222770176,"tcdate":1511802100640,"number":2,"cdate":1511802100640,"id":"HJpgrTKxf","invitation":"ICLR.cc/2018/Conference/-/Paper806/Official_Review","forum":"Sy-dQG-Rb","replyto":"Sy-dQG-Rb","signatures":["ICLR.cc/2018/Conference/Paper806/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A model that makes intuitive sense, with solid experimentation","rating":"7: Good paper, accept","review":"Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn’t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient.\n\nContribution:\n- The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN.\n\nPros:\n- Models that dynamically decide the amount of computation make intuitive sense and are of general interests.\n- The paper presents solid experimentation on various text classification and question answering datasets.\n- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks).\n- The paper is well written, and the presentation is good.\n\nCons:\n- Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.\n- The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps.\n- Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental.\n\nQuestions:\n- Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.\n- One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary.\n\nConclusion:\n- Based on the comments above, I recommend Accept","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1512222770218,"tcdate":1511779859359,"number":1,"cdate":1511779859359,"id":"r1izCPYlG","invitation":"ICLR.cc/2018/Conference/-/Paper806/Official_Review","forum":"Sy-dQG-Rb","replyto":"Sy-dQG-Rb","signatures":["ICLR.cc/2018/Conference/Paper806/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neural Speed Reading via Skim-RNN","rating":"7: Good paper, accept","review":"The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. While the proposed idea might be too simple, the authors show the importance of it via thorough experiments. It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments. \n\n* One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence. It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state (updated + copied) or a first few dimensions of the hidden state. I assume that the full hidden states are used for prediction. It is somehow interesting because it may mean the prediction heavily depends on small (d') part of the hidden state. In the second and third figures of Figure 10, the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed, and it might be related to the above assumption. In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token).\n\n* This paper would gain more attention from practitioners because of its practical purpose. In a similar vein, it would be also good to have some comments on training time as well. In a general situation where there is no need of re-training, training time would be meaningless, however, if one requires updating the model on the fly, it would be also meaningful to have some intuition on training time.\n\n* One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. In this sense, it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited-sized hidden dimensions (say 10 or 20). So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1510380430999,"tcdate":1510380430999,"number":2,"cdate":1510380430999,"id":"SJDcXGVJM","invitation":"ICLR.cc/2018/Conference/-/Paper806/Public_Comment","forum":"Sy-dQG-Rb","replyto":"SytImi7kG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Response","comment":"Hi,\nThanks for your response."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1510351818649,"tcdate":1510351697523,"number":1,"cdate":1510351697523,"id":"SytImi7kG","invitation":"ICLR.cc/2018/Conference/-/Paper806/Official_Comment","forum":"Sy-dQG-Rb","replyto":"BycR50MyG","signatures":["ICLR.cc/2018/Conference/Paper806/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper806/Authors"],"content":{"title":"Response","comment":"Hi,\nThank you for your interest in our paper and your suggestion! \nI think copying 1 to d-d' all the time is equivalent to copying d'+1 to d (without loss of generality).\nSo I believe what you are suggesting is something like having two small RNNs that operate on different parts of the hidden state.\nIn fact, we also think it is an interesting direction to explore (we also mention this as potential future work in the conclusion), though we did not report it mainly because we have not observed clear advantage by doing so in our experiments yet."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1510300369721,"tcdate":1510300369721,"number":1,"cdate":1510300369721,"id":"BycR50MyG","invitation":"ICLR.cc/2018/Conference/-/Paper806/Public_Comment","forum":"Sy-dQG-Rb","replyto":"Sy-dQG-Rb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Skimming part of the proposed model","comment":"It is a very interesting paper. I really enjoyed reading it.  Actually, I have a naive question about skimming part. According to Figure 1, the hidden state (d’ + 1 to d) of the word “and” is copied from the previous hidden state directly, while the first part (1 to d’) is updated by a smaller RNN. I am curious about this setting. Why did you update this model in this way? Can I copy the (1 to d - d’) part and update the remaining part of this model? "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]}},{"tddate":null,"ddate":null,"tmdate":1509739091181,"tcdate":1509135208880,"number":806,"cdate":1509739088516,"id":"Sy-dQG-Rb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy-dQG-Rb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neural Speed Reading via Skim-RNN","abstract":"Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significant reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also show that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.","pdf":"/pdf/8c29c50279ea58c7a61ad3ece742510c35faff8b.pdf","paperhash":"anonymous|neural_speed_reading_via_skimrnn","_bibtex":"@article{\n  anonymous2018neural,\n  title={Neural Speed Reading via Skim-RNN},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-dQG-Rb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper806/Authors"],"keywords":["Natural Language Processing","RNN","Inference Speed"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}