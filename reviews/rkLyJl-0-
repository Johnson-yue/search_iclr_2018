{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222681832,"tcdate":1511759486660,"number":3,"cdate":1511759486660,"id":"ByPYAMtgG","invitation":"ICLR.cc/2018/Conference/-/Paper539/Official_Review","forum":"rkLyJl-0-","replyto":"rkLyJl-0-","signatures":["ICLR.cc/2018/Conference/Paper539/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea, nice experiments, good motivation but lacks theoretical understanding","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper.\n\nTo me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it.  Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works.\n\nOtherwise, the paper is clearly written.\nThe topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to).\n\nI am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \\alpha, \\beta, \\gamma, \\mu, \\eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters?\n\n\n ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","abstract":"Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","pdf":"/pdf/99d43825c5c560a5664d2849a7cd438caa718c49.pdf","TL;DR":"We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.","paperhash":"anonymous|neumann_optimizer_a_practical_optimization_algorithm_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018neumann,\n  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkLyJl-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper539/Authors"],"keywords":["Deep Learning","Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222681870,"tcdate":1511639067628,"number":2,"cdate":1511639067628,"id":"BkEmOSvef","invitation":"ICLR.cc/2018/Conference/-/Paper539/Official_Review","forum":"rkLyJl-0-","replyto":"rkLyJl-0-","signatures":["ICLR.cc/2018/Conference/Paper539/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","rating":"5: Marginally below acceptance threshold","review":"Summary: \nThe paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training. The paper also provides the effectiveness of the algorithm by training ImageNet models (Inception-V3, Resnet-50, Resnet-101, and Inception-Resnet-V2). \n \nComments:\nI really appreciate the author(s) by providing experiments using real models on the ImageNet dataset. The algorithm seems to be easily used in practice. \n\nI do not have many comments for this paper since it focuses only in practical view without theory guarantee rigorously. \n\nAs you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? As we know, Adam is currently very well-known algorithm to train DNN. Do you think it would be interesting if you could compare the efficiency of Neumann optimizer with Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.  \n\nThe question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? Why should people use Neumann optimizer but not Adam, which is already very well-known? If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published.  \n \nMinor comments:\nPage 3, in eq. (3): missing “-“ sign\nPage 3, in eq. (6): missing “transpose” on \\nabla \\hat{f}\nPage 4, first equation: O(|| \\eta*mu_t ||^2)\nPage 5, in eq. (9): m_{k-1}\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","abstract":"Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","pdf":"/pdf/99d43825c5c560a5664d2849a7cd438caa718c49.pdf","TL;DR":"We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.","paperhash":"anonymous|neumann_optimizer_a_practical_optimization_algorithm_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018neumann,\n  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkLyJl-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper539/Authors"],"keywords":["Deep Learning","Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222681908,"tcdate":1511557762235,"number":1,"cdate":1511557762235,"id":"Hy5t5WIxG","invitation":"ICLR.cc/2018/Conference/-/Paper539/Official_Review","forum":"rkLyJl-0-","replyto":"rkLyJl-0-","signatures":["ICLR.cc/2018/Conference/Paper539/AnonReviewer3"],"readers":["everyone"],"content":{"title":"See below.","rating":"6: Marginally above acceptance threshold","review":" \nThis paper presents a new 2nd-order algorithm that implicitly uses curvature information, and it shows the intuition behind the approximation schemes in the algorithms and also validates the heuristics in various experiments.  The method involves using Neumann Series and Richardson iteration to avoid Hessian-vector product in second order method for NN.  In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization.  The numerical examples are relatively clear and easy to figure out details.\n\n1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer.  For example, one simple experiment would be showing how it works for convex problems, e.g., logistic regression.  Realistic DNN systems are very complex, and evaluating the method in a simple setting would help a lot in determining what if anything is novel about the method.\n\n2. Also, for deep learning problems, it would be more convincing to see how different initialization can affect the performances. \n\n3. Although the authors present their algorithm as a second order method at beginning, the final algorithm is kind of like a complex momentum SGD with limited memory.  Rather than simply throwing out a new method with a new name, it would be helpful to understand what the steps of this method are implicitly doing.  Please explain more about this.\n\n4. It said that the algorithm is hyperparameter free except for learning rate.  However, it is hard to see why there is no need to tune other hyperparameters, e.g., Cubic Regularizer, Repulsive Regularizer.  The effect/sensitivity of hyperparameters for second order methods are quite different than hyperparameters for first order methods, and it is of interest to know how hyperparameters for implicit second order methods perform.\n\n5. For Section 4.2, the well know benefit by using large batch size to train models is that it could reduce training time and epochs.  However, from Table 3, there is no such phenomenon.  Please explain.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","abstract":"Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","pdf":"/pdf/99d43825c5c560a5664d2849a7cd438caa718c49.pdf","TL;DR":"We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.","paperhash":"anonymous|neumann_optimizer_a_practical_optimization_algorithm_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018neumann,\n  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkLyJl-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper539/Authors"],"keywords":["Deep Learning","Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509484966603,"tcdate":1509484966603,"number":1,"cdate":1509484966603,"id":"HJ1hYPI0-","invitation":"ICLR.cc/2018/Conference/-/Paper539/Public_Comment","forum":"rkLyJl-0-","replyto":"rkLyJl-0-","signatures":["~Boris_Ginsburg1"],"readers":["everyone"],"writers":["~Boris_Ginsburg1"],"content":{"title":"Excellent paper! Questions and comments ...  ","comment":"Thanks very much for this outstanding paper! It is very interesting, both from the theoretical and practical points of view. \nWe have several questions:\n1.\tPage 5: The transition from Eq. 7 to eq. 9 intuitively makes sense, but we would appreciate more rigorous derivation (maybe as Appendix B?)\n2.\tPage 6: can you please clarify the transition from Alg. 1 to Alg. 2. ?  \n* Alg. 1 line 5 uses fixed w_t and fixed mini-batch. \n* Alg 2, line 9-11 uses different w_t and different batches (similar to regular SGD). \nWhat are the assumptions / requirements which make it possible to use different w_t?\n3.\tPage 6, Alg. 2: is periodic reset of m_t necessary? \nWhat will be the impact on performance if we don’t reset m_t and avoid K at all?\n4.\tPage 6, Alg 2: Is Alg. 2 w/o regularization equivalent to regular SGD with adaptive momentum?\n5.\tPage 8, section 4.2: Did you try to use Neumann optimizer for training networks w/o batch norm (e.g. Alexnet or Googlenet) with large batch?\n6.\tPage 8, section 4.3 “Regularization”: Did you try to use L2-regularizer (weight decay) instead of “cubic + repulsive term”? \n\nTypos:\n1.\tPage 3 , eq. 3: ‘-“ sign is missing\n2.\tPage 3 , after eq. 6: variable z is not defined (is z:= (w-w_t)/ \\nu ? )\n3.\tPage 4 , first equation (line 4): should be O(|\\nu m_t|^ 2 ?\n4.\tPage 5, eq.9: last term should be m_{k-1}\n5.\tPage 6, Alg. 2, line 11: should be w_t=w_{t-1} + m_t ?\n6.\tPage 6, Alg. 2, line 13:  should be return w_T?\n7.\tPage 7: can you set a right reference to data augmentation strategy?  \n\nThanks again for an excellent paper!\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","abstract":"Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","pdf":"/pdf/99d43825c5c560a5664d2849a7cd438caa718c49.pdf","TL;DR":"We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.","paperhash":"anonymous|neumann_optimizer_a_practical_optimization_algorithm_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018neumann,\n  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkLyJl-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper539/Authors"],"keywords":["Deep Learning","Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739247362,"tcdate":1509125854147,"number":539,"cdate":1509739244689,"id":"rkLyJl-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkLyJl-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks","abstract":"Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.","pdf":"/pdf/99d43825c5c560a5664d2849a7cd438caa718c49.pdf","TL;DR":"We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.","paperhash":"anonymous|neumann_optimizer_a_practical_optimization_algorithm_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018neumann,\n  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkLyJl-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper539/Authors"],"keywords":["Deep Learning","Optimization"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}