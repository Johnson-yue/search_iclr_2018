{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222700630,"tcdate":1511816041773,"number":3,"cdate":1511816041773,"id":"rkMOsg5ez","invitation":"ICLR.cc/2018/Conference/-/Paper607/Official_Review","forum":"B1O3OgbRW","replyto":"B1O3OgbRW","signatures":["ICLR.cc/2018/Conference/Paper607/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper, but incremental contribution.","rating":"6: Marginally above acceptance threshold","review":"This is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural networks. The contributions here are: 1) experiments testing the DTP algorithm on more difficult datasets, 2) proposing a minor modification of the DTP algorithm at the output layer, and 3) testing the DTP algorithm on locally-connected architectures. These are all novel contributions, but each one seems incremental in the context of previous work on this and similar algorithms (E.G. Nokland, Direct Feedback Alignment Provides Learning in Deep Neural Networks, 2016; Baldi et al, Learning in the Machine: The Symmetries of the Deep Learning Channel, 2017). \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Assessing the scalability of biologically-motivated deep learning algorithms and architectures","abstract":"The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired a number of proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up a biologically motivated model of deep learning to datasets which need deep networks with  appropriate architectures to achieve good performance. We present results on CIFAR-10 and ImageNet.  For CIFAR-10 we show that our algorithm, a straightforward, weight-transport-free variant of difference target-propagation (DTP) modified to remove backpropagation from the penultimate layer, is competitive with BP in training deep networks with locally defined receptive fields that have untied weights.  For ImageNet we find that both DTP and our algorithm perform significantly worse than BP, opening questions about whether different architectures or algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","pdf":"/pdf/737a8ebf404bb5b34d37481716b2c62133be3f6a.pdf","TL;DR":"Benchmarks for biologically plausible learning algorithms on complex datasets and architectures","paperhash":"anonymous|assessing_the_scalability_of_biologicallymotivated_deep_learning_algorithms_and_architectures","_bibtex":"@article{\n  anonymous2018assessing,\n  title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1O3OgbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper607/Authors"],"keywords":["target propagation","biologically-plausible learning","benchmark","neuroscience"]}},{"tddate":null,"ddate":null,"tmdate":1512222700672,"tcdate":1511808738786,"number":2,"cdate":1511808738786,"id":"BysJyk5xz","invitation":"ICLR.cc/2018/Conference/-/Paper607/Official_Review","forum":"B1O3OgbRW","replyto":"B1O3OgbRW","signatures":["ICLR.cc/2018/Conference/Paper607/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The work presents an empirical analysis that emphasises the ineffectively of non-convolutional architectures on image classification tasks and demonstrates a worse performance of target propagation compared to backpropagation on ImageNet. While the paper is an interesting read its novelty may be a bit limited. ","rating":"5: Marginally below acceptance threshold","review":"Summary of the paper: The paper analysis how well difference target probation (DTP) - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like CIFAR-10 and ImageNet.  The DTP algorithm is slightly adapted to make it more biologically plausible, by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation (leading to the variant SDTP), and by making the optimisation of both involved losses parallel.  Furthermore, only feedforward and locally connected networks (CNN) are considered since their architecture is considered more biologically plausible than convolutional neural networks. While on MNIST and CIFAR, DTP and SDTP performed as well as backprop, they perform worse on ImageNet. Furthermore, it becomes clear, that without CNN structure no really good performance is achieved neither on CIFAR nor on ImageNet \n\nPros:\n- The paper is nicely written and good to follow.\n- Suggested modifications from DTP to STDP increase its biological plausibility without making its performance worse. \n- The worse performance compared to backprop and CNNs underlines the open question how to yield biologically plausible AND efficient algorithms and network architectures. \n\nCons:\n- The title of the paper seems to general to me, since target propagation is the only algorithm compared against backpropagation. \n- Since the adaptions to DTP are rather small, the work does not contain much novelty. It can rather be seen as an interesting empirical study, with \"negative result\".\n\n\nMinor comments:\n- page 1: The reference list could also include  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00934 and  https://arxiv.org/abs/1510.02777\n- page 5: “the the degree” , “specified as (….) followed by” -> , “as (….) followed by” ?,\n- This notation probably stems from the code, but SAME and VALID could be nicer described as “0 padding” and “no padding” for example.\n- page 8:  “applying BP to the brain” sounds strange to me.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Assessing the scalability of biologically-motivated deep learning algorithms and architectures","abstract":"The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired a number of proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up a biologically motivated model of deep learning to datasets which need deep networks with  appropriate architectures to achieve good performance. We present results on CIFAR-10 and ImageNet.  For CIFAR-10 we show that our algorithm, a straightforward, weight-transport-free variant of difference target-propagation (DTP) modified to remove backpropagation from the penultimate layer, is competitive with BP in training deep networks with locally defined receptive fields that have untied weights.  For ImageNet we find that both DTP and our algorithm perform significantly worse than BP, opening questions about whether different architectures or algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","pdf":"/pdf/737a8ebf404bb5b34d37481716b2c62133be3f6a.pdf","TL;DR":"Benchmarks for biologically plausible learning algorithms on complex datasets and architectures","paperhash":"anonymous|assessing_the_scalability_of_biologicallymotivated_deep_learning_algorithms_and_architectures","_bibtex":"@article{\n  anonymous2018assessing,\n  title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1O3OgbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper607/Authors"],"keywords":["target propagation","biologically-plausible learning","benchmark","neuroscience"]}},{"tddate":null,"ddate":null,"tmdate":1512222700711,"tcdate":1511554450468,"number":1,"cdate":1511554450468,"id":"rk95plLez","invitation":"ICLR.cc/2018/Conference/-/Paper607/Official_Review","forum":"B1O3OgbRW","replyto":"B1O3OgbRW","signatures":["ICLR.cc/2018/Conference/Paper607/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review of 'Assessing the scalability of biologically-motivated deep learning...'","rating":"8: Top 50% of accepted papers, clear accept","review":"I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP). The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible. There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments. In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer.  Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible.  Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop. The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently).\n\nIn the negatives, the paper should mention in the discussion and intro that all the TP variants ignore the issue of dynamics. We know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one (or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward, which seems not very plausible).\n\nIn the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture). Also, training errors should be shown, since I suspect that underfitting may be happening especially in the case of ImageNet. If that was the case, future work should first explore higher capacity (which may require larger-memory GPUs...). Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME).","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Assessing the scalability of biologically-motivated deep learning algorithms and architectures","abstract":"The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired a number of proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up a biologically motivated model of deep learning to datasets which need deep networks with  appropriate architectures to achieve good performance. We present results on CIFAR-10 and ImageNet.  For CIFAR-10 we show that our algorithm, a straightforward, weight-transport-free variant of difference target-propagation (DTP) modified to remove backpropagation from the penultimate layer, is competitive with BP in training deep networks with locally defined receptive fields that have untied weights.  For ImageNet we find that both DTP and our algorithm perform significantly worse than BP, opening questions about whether different architectures or algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","pdf":"/pdf/737a8ebf404bb5b34d37481716b2c62133be3f6a.pdf","TL;DR":"Benchmarks for biologically plausible learning algorithms on complex datasets and architectures","paperhash":"anonymous|assessing_the_scalability_of_biologicallymotivated_deep_learning_algorithms_and_architectures","_bibtex":"@article{\n  anonymous2018assessing,\n  title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1O3OgbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper607/Authors"],"keywords":["target propagation","biologically-plausible learning","benchmark","neuroscience"]}},{"tddate":null,"ddate":null,"tmdate":1509739204946,"tcdate":1509128368279,"number":607,"cdate":1509739202278,"id":"B1O3OgbRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1O3OgbRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Assessing the scalability of biologically-motivated deep learning algorithms and architectures","abstract":"The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired a number of proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up a biologically motivated model of deep learning to datasets which need deep networks with  appropriate architectures to achieve good performance. We present results on CIFAR-10 and ImageNet.  For CIFAR-10 we show that our algorithm, a straightforward, weight-transport-free variant of difference target-propagation (DTP) modified to remove backpropagation from the penultimate layer, is competitive with BP in training deep networks with locally defined receptive fields that have untied weights.  For ImageNet we find that both DTP and our algorithm perform significantly worse than BP, opening questions about whether different architectures or algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","pdf":"/pdf/737a8ebf404bb5b34d37481716b2c62133be3f6a.pdf","TL;DR":"Benchmarks for biologically plausible learning algorithms on complex datasets and architectures","paperhash":"anonymous|assessing_the_scalability_of_biologicallymotivated_deep_learning_algorithms_and_architectures","_bibtex":"@article{\n  anonymous2018assessing,\n  title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1O3OgbRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper607/Authors"],"keywords":["target propagation","biologically-plausible learning","benchmark","neuroscience"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}