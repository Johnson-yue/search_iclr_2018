{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222593686,"tcdate":1512214285372,"number":3,"cdate":1512214285372,"id":"SJHM1GxbG","invitation":"ICLR.cc/2018/Conference/-/Paper242/Official_Review","forum":"BJNRFNlRW","replyto":"BJNRFNlRW","signatures":["ICLR.cc/2018/Conference/Paper242/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Important problem with well evaluated solution. ","rating":"7: Good paper, accept","review":"In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization. Their technique can be applied on top of existing GANs and can address issues such as mode collapse. The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN. Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse. This is a strong contribution\n\nIn Table 2 the difference between inception scores for DCGAN and this approach seems significant to ignore. The authors should explain more possibly.\nThere is a typo in Page 2 – For all these varaints, -variants.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS","abstract":"We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.","pdf":"/pdf/8923ef29307de52d61eeae4519bc601985e7f761.pdf","TL;DR":"We propose a primal-dual subgradient method for training GANs and this method effectively alleviates mode collapse.","paperhash":"anonymous|training_generative_adversarial_networks_via_primaldual_subgradient_methods","_bibtex":"@article{\n  anonymous2018training,\n  title={TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJNRFNlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper242/Authors"],"keywords":["GAN","Primal-Dual Subgradient","Mode Collapse","Saddle Point"]}},{"tddate":null,"ddate":null,"tmdate":1512222593724,"tcdate":1512000991141,"number":2,"cdate":1512000991141,"id":"SkvyRa3lG","invitation":"ICLR.cc/2018/Conference/-/Paper242/Official_Review","forum":"BJNRFNlRW","replyto":"BJNRFNlRW","signatures":["ICLR.cc/2018/Conference/Paper242/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Clarity analysis, very good motivation, but relatively limited novelty","rating":"6: Marginally above acceptance threshold","review":"This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem.\nAs a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point.\nAdditionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models.\nBut this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1.\nAs follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem.\nMost important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it. But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS","abstract":"We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.","pdf":"/pdf/8923ef29307de52d61eeae4519bc601985e7f761.pdf","TL;DR":"We propose a primal-dual subgradient method for training GANs and this method effectively alleviates mode collapse.","paperhash":"anonymous|training_generative_adversarial_networks_via_primaldual_subgradient_methods","_bibtex":"@article{\n  anonymous2018training,\n  title={TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJNRFNlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper242/Authors"],"keywords":["GAN","Primal-Dual Subgradient","Mode Collapse","Saddle Point"]}},{"tddate":null,"ddate":null,"tmdate":1512222593826,"tcdate":1511607675241,"number":1,"cdate":1511607675241,"id":"SkmK6TUxz","invitation":"ICLR.cc/2018/Conference/-/Paper242/Official_Review","forum":"BJNRFNlRW","replyto":"BJNRFNlRW","signatures":["ICLR.cc/2018/Conference/Paper242/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting insights although gap between theory and practice should be discussed","rating":"7: Good paper, accept","review":"This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem. They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.\n\nTechnically, the paper is sound. It mostly leverages the existing literature on primal-dual subgradient methods to modify the GAN training procedure. I think this is a nice contribution that does yield to some interesting insights. However I do have some concerns about the way the paper is currently written and I find some claims misleading.\n\nPrior convergence proofs: I think the way the paper is currently written is misleading. The authors quote the paper from Ian Goodfellow: “For GANs, there is no theoretical prediction as to\nwhether simultaneous gradient descent should converge or not.”. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf. A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent. Another problem is of course the assumptions required for the proof that typically don’t hold in practice (see comment below).\n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution. This typically yields a non-convex non-concave optimization problem. This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm.\n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2. This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf\nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this? I think this would also shed some light as to why this approach alleviates the problem of mode collapse.\n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality. For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work but I’m not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse. The inception score fails to capture this property. Could the authors explore other quantitative measure? Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)?\n\nExperiments\nTypo: Should say “The data distribution is p_d(x) = 1{x=1}”.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS","abstract":"We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.","pdf":"/pdf/8923ef29307de52d61eeae4519bc601985e7f761.pdf","TL;DR":"We propose a primal-dual subgradient method for training GANs and this method effectively alleviates mode collapse.","paperhash":"anonymous|training_generative_adversarial_networks_via_primaldual_subgradient_methods","_bibtex":"@article{\n  anonymous2018training,\n  title={TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJNRFNlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper242/Authors"],"keywords":["GAN","Primal-Dual Subgradient","Mode Collapse","Saddle Point"]}},{"tddate":null,"ddate":null,"tmdate":1509739410681,"tcdate":1509079499987,"number":242,"cdate":1509739408022,"id":"BJNRFNlRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJNRFNlRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS","abstract":"We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.","pdf":"/pdf/8923ef29307de52d61eeae4519bc601985e7f761.pdf","TL;DR":"We propose a primal-dual subgradient method for training GANs and this method effectively alleviates mode collapse.","paperhash":"anonymous|training_generative_adversarial_networks_via_primaldual_subgradient_methods","_bibtex":"@article{\n  anonymous2018training,\n  title={TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJNRFNlRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper242/Authors"],"keywords":["GAN","Primal-Dual Subgradient","Mode Collapse","Saddle Point"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}