{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222619459,"tcdate":1512195144006,"number":2,"cdate":1512195144006,"id":"SJxIVpkZM","invitation":"ICLR.cc/2018/Conference/-/Paper317/Official_Review","forum":"BJIgi_eCZ","replyto":"BJIgi_eCZ","signatures":["ICLR.cc/2018/Conference/Paper317/AnonReviewer1"],"readers":["everyone"],"content":{"title":"state of the art on SQuAD with FusionNet","rating":"8: Top 50% of accepted papers, clear accept","review":"The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive. That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level. The network proposed here, FusionHet, fixes problem. Importantly, the model achieves state-of-the-art performance of the SQuAD dataset.\n\nThe paper is very well-written and easy to follow. I found the architecture very intuitively laid out, even though this is not my area of expertise. Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work! What most impressed me, however, was the literature review. Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work. Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2. \n\nAll in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222619503,"tcdate":1511651270095,"number":1,"cdate":1511651270095,"id":"r1ApvdPxG","invitation":"ICLR.cc/2018/Conference/-/Paper317/Official_Review","forum":"BJIgi_eCZ","replyto":"BJIgi_eCZ","signatures":["ICLR.cc/2018/Conference/Paper317/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice analysis of literature, interesting model and good results, but a little lack of substance","rating":"7: Good paper, accept","review":"The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \"fully-aware\" of all levels of abstraction, e.g. word-level, phrase-level, etc. In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD. They also propose an attention mechanism that works better than others (Symmetric + ReLU).\n\nStrengths:\n- The paper is well-written and clear.\n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field.\n- The multi-level attention is novel and indeed seems to work, with convincing ablations.\n- Nice engineering achievement, reaching the top of the leaderboard (in early October).\n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances. Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA).\n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard. Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution.\n\n\nMinor:\n- Probably figure 4 can be drawn better. Not easy to understand nor concrete.\n- Section 3.2 GRU citation should be Cho et al. [2].\n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too. Could you perform ablation without contextualized embedding (CoVe)?\n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511023037342,"tcdate":1511023037342,"number":2,"cdate":1511023037342,"id":"rJBpWyR1z","invitation":"ICLR.cc/2018/Conference/-/Paper317/Official_Comment","forum":"BJIgi_eCZ","replyto":"Sy1MCpzJf","signatures":["ICLR.cc/2018/Conference/Paper317/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper317/Authors"],"content":{"title":"Re: Nice.","comment":"Thank you for your compliment!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511022994760,"tcdate":1511022994760,"number":1,"cdate":1511022994760,"id":"H1sqbJ01G","invitation":"ICLR.cc/2018/Conference/-/Paper317/Official_Comment","forum":"BJIgi_eCZ","replyto":"SJftaCpyM","signatures":["ICLR.cc/2018/Conference/Paper317/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper317/Authors"],"content":{"title":"Training Parameters","comment":"Thank you for being interested in reproducing our work!\n\n- Which components are used without dropout?\nDropout is applied before every linear transform, including the input for each layer of LSTM and Attention. For fast implementation, we do not use hidden state dropout in LSTM. Also, an additional dropout is applied after the GloVe and CoVe embedding layer.\nThe dropout is shared across time step (i.e., variational dropout). And different linear layers use different dropout masks.\n\n- Attention dimensions for the various fusions present.\nFor all the fully-aware attention S(HoW_i, HoW_j), we used an attention dimension k = 250 (the same as the output size of BiLSTM)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1511022581125,"tcdate":1511021946392,"number":2,"cdate":1511021946392,"id":"SJftaCpyM","invitation":"ICLR.cc/2018/Conference/-/Paper317/Public_Comment","forum":"BJIgi_eCZ","replyto":"BJIgi_eCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Reproductive Study - Training Parameters","comment":"I set up a reproduction experiment for which I need a little clarification on the following.\n\n- Which components are used/set up without dropout?\n- Attention dimensions for the various fusions present."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1510297094879,"tcdate":1510297094879,"number":1,"cdate":1510297094879,"id":"Sy1MCpzJf","invitation":"ICLR.cc/2018/Conference/-/Paper317/Public_Comment","forum":"BJIgi_eCZ","replyto":"BJIgi_eCZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Nice.","comment":"Nice simple model."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739367437,"tcdate":1509096173704,"number":317,"cdate":1509739364781,"id":"BJIgi_eCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJIgi_eCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension","abstract":"This paper introduces a new neural structure called FusionNet, which extends existing attention approaches from three perspectives. First, it puts forward a novel concept of \"History of Word\" to characterize attention information from the lowest word-level embedding up to the highest semantic-level representation. Second, it introduces an improved attention scoring function that better utilizes the \"History of Word\" concept. Third, it proposes a fully-aware multi-level attention mechanism to capture the complete information in one text (such as a question) and exploit it in its counterpart (such as context or passage) layer by layer. We apply FusionNet to the Stanford Question Answering Dataset (SQuAD) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing (Oct. 4th, 2017). Meanwhile, we verify the generalization of FusionNet with two adversarial SQuAD datasets and it sets up the new state-of-the-art on both datasets: on AddSent, FusionNet increases the best F1 metric from 46.6% to 51.4%; on AddOneSent, FusionNet boosts the best F1 metric from 56.0% to 60.7%.","pdf":"/pdf/51bdc52c4b39149b4104832f675837dac6fa81b5.pdf","TL;DR":"We propose a light-weight enhancement for attention and a neural architecture, FusionNet, to achieve STOA on SQuAD and adversarial SQuAD.","paperhash":"anonymous|fusionnet_fusing_via_fullyaware_attention_with_application_to_machine_comprehension","_bibtex":"@article{\n  anonymous2018fusionnet:,\n  title={FusionNet: Fusing via Fully-aware Attention with Application to Machine Comprehension},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJIgi_eCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper317/Authors"],"keywords":["Attention Mechanism","Machine Comprehension","Natural Language Processing","Deep Learning"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}