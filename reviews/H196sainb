{"notes":[{"tddate":null,"ddate":null,"tmdate":1511868855534,"tcdate":1511868855534,"number":3,"cdate":1511868855534,"id":"rkJTKT9lz","invitation":"ICLR.cc/2018/Conference/-/Paper7/Public_Comment","forum":"H196sainb","replyto":"BJ4hFZ5ez","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Answer","comment":"I find the answer from the authors quite satisfying. In particular, the missing result that was provided in the answer (and should be definitely added to the paper) addresses my main concerns regarding the comparability with previous work. While this result is not as spectacular as the others (almost at par with the supervised system, possibly because the comparability of Wikipedia is playing a role as pointed in my previous comment) and I think that some of the claims in the paper should be reworded accordingly, it does convincingly show that the proposed method can achieve SOTA results in a standard dataset without any supervision.\n\nRegarding the work of Zhang et al. (2017b) and Artetxe et al. (2017), I agree on most of the comments on the former, and this new result shows that the proposed method works better than the latter. However, I still think that some of the claims regarding these papers (e.g. Zhang et al. (2017b) \"is significantly below supervised methods\" or Artetxe et al. (2017) does not work for en-ru and en-zh) are unfounded and need to either be supported experimentally or reconsidered."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1512222721809,"tcdate":1511828138705,"number":3,"cdate":1511828138705,"id":"H1Qhqm9ez","invitation":"ICLR.cc/2018/Conference/-/Paper7/Official_Review","forum":"H196sainb","replyto":"H196sainb","signatures":["ICLR.cc/2018/Conference/Paper7/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Well-rounded contribution, nice read, incomplete related work","rating":"8: Top 50% of accepted papers, clear accept","review":"An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning.\n\nThe paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general: it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times.\n\nThere are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).\n\nThe evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings. Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.\n\nIn my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1511819692278,"tcdate":1511819692278,"number":2,"cdate":1511819692278,"id":"BJ4hFZ5ez","invitation":"ICLR.cc/2018/Conference/-/Paper7/Official_Comment","forum":"H196sainb","replyto":"SkMy4hKlz","signatures":["ICLR.cc/2018/Conference/Paper7/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper7/Authors"],"content":{"title":"comparison","comment":"The paper by Dinu et al. provides embeddings and dictionaries for the English-Italian language pair. The embeddings they provide have become pretty standard and we found at least 5 previous methods that used this dataset:\nMikolov et al., Faruqui et al., Dinu et al., Smith et al., Artetxe et al.\nThese previous papers provide strong supervised SOTA baselines on the word translation task, and in Table 2 we show results of our supervised method compared to these 5 papers. The row “Procrustes + CSLS” is a supervised baseline, training our method with supervision using exactly the same word embeddings and dictionaries as in Dinu et al. These results show that our supervised baseline works better than all these previous approaches (reaching 44.9% P@1 en-it).\nThe requested unsupervised configuration “Adv - Refine - CSLS” using the same embeddings and dictionary as in Dinu et al. obtains 45.1% on en-it, which is better than our supervised baseline (and SOTA by more than 2%). \n\nHowever, this information is redundant with Table 1, which shows that our unsupervised approach is better than our supervised baseline on European languages. We therefore decided not to incorporate this result, but we will add it back as suggested.\n\nMoreover, using the Wacky datasets (non comparable corpora) to learn embeddings, we improved the SOTA by 11.5% and 26.6% on the sentence retrieval task using our CSLS method, see table 3. Again, these experiments use the very same setting as previously reported in the literature.\n\nMore generally, regarding your comment “they inexplicably use a different set of embeddings, trained in a different corpus”, note that:\n- As noted above, we did compare using the very same embeddings and settings as others.\n- We did study the effect of using different corpora: see fig. 3\n- As shown in the paper, using our method on Wikipedia improves the results by more than 20%\n- Wikipedia is available in most languages, pretrained embeddings were already released and publicly available, we just downloaded them (while the Wacky datasets are only available for a few languages)\n- We found that the monolingual quality of these pretrained embeddings is better than the one obtained on the Wacky datasets\n\nAs opposed to the 5 methods we compare ourselves against in the paper, Zhang et al. (2017):\n1) used different embeddings and dictionaries which they do not provide, \n2) used a lexicon of 50 or 100 word pairs only in their supervised baseline, which is different than standard practice since Mikolov et al. (2013b) (see Dinu et al., Faruqui et al., Smith et al., etc.) and which is also what we did, namely considering dictionaries with 5000 pairs. As a result, they compare themselves to a very weak baseline.\n3) in the retrieval task they consider a very simplistic settings, with only a few thousands words, as opposed to large dictionaries of 200k words (as done by Dinu et al., Smith et al. and us). \n4) they. do not provide a validation set and, as shown in Figure 2 in our paper, their stopping criterion does not work well.\nWe did try to run their code, but we have not been successful yet.\n\nAs for comparing against Artetxe et al., as reported in table 2 they obtain a P@1 of 39.7% while we obtain 45.1% using the same Dinu’s embeddings.\n\nFinally, we have released our code, along with our embeddings / dictionaries for reproducibility. We will share the link here as soon as the decisions are out in order to preserve anonymity."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1512222721848,"tcdate":1511803884453,"number":2,"cdate":1511803884453,"id":"rJEg3TtxM","invitation":"ICLR.cc/2018/Conference/-/Paper7/Official_Review","forum":"H196sainb","replyto":"H196sainb","signatures":["ICLR.cc/2018/Conference/Paper7/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"3: Clear rejection","review":"The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique. The task is interesting and relevant, especially for in low-resource language pair settings.\n\nThe paper, however, misses comparison against important work from the literature that is very relevant to their task — decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA. \n\nThe former set of works, while focused on machine translation also learns a translation table in the process. Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods. While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods. Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach.\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works. \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach.\n\nThe paper mentions that the approach is “unsupervised”. However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages. How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1511797722369,"tcdate":1511797722369,"number":2,"cdate":1511797722369,"id":"SkMy4hKlz","invitation":"ICLR.cc/2018/Conference/-/Paper7/Public_Comment","forum":"H196sainb","replyto":"H196sainb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comparison with previous work should be improved","comment":"I think that the paper does not do a good job at comparing the proposed method with previous work.\n\nWhile most of the experiments are run in a custom dataset and do not include results from previous authors, the paper also reports some results in the standard dataset from Dinu et al. (2015) \"to allow for a direct comparison with previous approaches\", which I think that is necessary. However, they inexplicably use a different set of embeddings, trained in a different corpus, for their unsupervised method in these experiments, so their results are not actually comparable with the rest of the systems. While I think that these results are also interesting, as they shows that the training corpus and embedding hyperparameters can make a huge difference, I see no reason not to also report the truly comparable results with the standard embeddings used by previous work. In other words, Table 2 is missing a row for \"Adv - Refine - CSLS\" using the same embeddings as the rest of the systems.\n\nMoreover, I think that the choice of training the embeddings in Wikipedia is somewhat questionable. Wikipedia is a document-aligned comparable corpus, and it seems reasonable that the proposed method could somehow benefit from that, even if it was not originally designed to do so. In other words, while the proposed method is certainly unsupervised in design, I think that it was not tested in truly unsupervised conditions. In fact, there is some previous work that learns cross-lingual word embeddings from Wikipedia by exploiting this document alignment information (http://www.aclweb.org/anthology/P15-1165), which shows that this cross-lingual signal in Wikipedia is actually very strong.\n\nApart from that, I think that the paper is a bit unfair with some previous work. In particular, the proposed adversarial method is very similar to that of Zhang et al. (2017b), and the authors simply state that the performance of the latter \"is significantly below supervised methods\", without any experimental evidence that supports this claim. Considering that the implementation of Zhang et al. (2017b) is public (http://nlp.csai.tsinghua.edu.cn/~zm/UBiLexAT/), the authors could have easily tested it in their experiments and show that the proposed method is indeed better than that of Zhang et al. (2017b), but they don't.\n\nI also think that the authors are a bit unfair in their criticism of Artetxe et al. (2017). While the proposed method has the clear advantage of not requiring any cross-lingual signal, not even the assumption of shared numerals in Artetxe et al. (2017), it is not true that the latter is \"just not applicable\" to \"languages that do not share a common alphabet (en-ru and en-zh)\", as both Russian and Chinese, as well as many other languages that do not use a latin alphabet, do use arabic numerals. In relation to that, the statement that \"the method of Artetxe et al. (2017) on our dataset does not work on the word translation task for any of the language pairs, because the digits were filtered out from the datasets used to train the fastText embeddings\" clearly applies to the embeddings they use, and not to the method of Artetxe et al. (2017) itself. Once again, considering that the implementation of Artetxe et al. (2017) is public (https://github.com/artetxem/vecmap), the authors could have easily supported their claims experimentally, but they also fail to do so."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1512222721888,"tcdate":1511181995673,"number":1,"cdate":1511181995673,"id":"SyE3AHgxG","invitation":"ICLR.cc/2018/Conference/-/Paper7/Official_Review","forum":"H196sainb","replyto":"H196sainb","signatures":["ICLR.cc/2018/Conference/Paper7/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good and interesting work (a few issues on the paper's claims)","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages. The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping). The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\n\nThe paper presents an interesting approach which achieves good performance. The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\n\nMy one concern is that the supervised approach that the paper compares to is limited: it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words. I think the paper's comparisons are valid, but the abstract and introduction make very strong claims about outperforming \"state-of-the-art supervised approaches\". I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. The same holds for statements like \"... our method is a first step ...\", which is very hard to justify. I also do not think it is necessary to over-sell, given the solid work in the paper.\n\nFurther comments, questions and suggestions:\n- It might be useful to add more details of your actual approach in the Abstract, not just what it achieves.\n- Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way. The actual word embedding method, therefore, has a big influence on performance (as you show). Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains/data with similar co-occurrence statistics, in order for your method to be appropriate?\n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly?\n- How were the 5k source words for Procrustes supervised baseline selected?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces?\n- Do you think your approach would benefit from having a few parallel training points?\n\nSome minor grammatical mistakes/typos (nitpicking):\n- \"gives a good performance\" -> \"gives good performance\"\n- \"Recent works\", \"several works\", \"most works\", etc. -> \"recent studies\", \"several studies\", etc.\n- \"i.e, the improvements\" -> \"i.e., the improvements\"\n\nThe paper is well-written, relevant and interesting. I therefore recommend that the paper be accepted.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1510092427041,"tcdate":1509724786274,"number":1,"cdate":1509724786274,"id":"rJcdzzcCb","invitation":"ICLR.cc/2018/Conference/-/Paper7/Official_Comment","forum":"H196sainb","replyto":"HkD8ivF0-","signatures":["ICLR.cc/2018/Conference/Paper7/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper7/Authors"],"content":{"title":"response","comment":"Thank you for the pointer, we were aware of this work and we will add a citation. Note however that our focus is not to learn to a machine translation system (we just gave a simple example of this application, together with others like sentence retrieval, word similarity, etc.), but to infer a bilingual dictionary without using any labeled data. Unlike Ravi et al. we use monolingual data on both side at training time, and we infer a large bilingual dictionary (200K words). When we say \"this is a first step towards fully unsupervised machine translation\" it does not mean we are the first to look at this problem, we simply meant that our method could be used as a first step in a more complex pipeline. We will rephrase this sentence to avoid confusion.\nIn other words, the two works look at different things: this one is focussed on learning a bilingual dictionary, while the other is focussed on the problem of machine translation."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1509681999501,"tcdate":1509681999501,"number":1,"cdate":1509681999501,"id":"HkD8ivF0-","invitation":"ICLR.cc/2018/Conference/-/Paper7/Public_Comment","forum":"H196sainb","replyto":"H196sainb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Is it really the first step towards unsupervised MT?","comment":"Saying that the method is a first step towards fully unsupervised machine translation seems like a bold (if not false) statement. In particular, this has been done before using deciphering:\n\nRavi & Knight, \"Deciphering Foreign Language\", ACL 2011, http://aclweb.org/anthology/P/P11/P11-1002.pdf\n\nThere are plenty of other similar previous work besides this one. I think any claims on MT without parallel corpora should at least mention deciphering as related work."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]}},{"tddate":null,"ddate":null,"tmdate":1509739534447,"tcdate":1507740609915,"number":7,"cdate":1509739531793,"id":"H196sainb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H196sainb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Word translation without parallel data","abstract":"State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent works showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally show that our method is a first step towards fully unsupervised machine translation and describe experiments on the English-Esperanto language pair, on which there only exists a limited amount of parallel data.","pdf":"/pdf/55c7f48a02bb1de6a93877d8da834997d529fd17.pdf","TL;DR":"Aligning languages without the Rosetta Stone: with no parallel data, we construct bilingual dictionaries using adversarial training, cross-domain local scaling, and an accurate proxy criterion for cross-validation.","paperhash":"anonymous|word_translation_without_parallel_data","_bibtex":"@article{\n  anonymous2018word,\n  title={Word translation without parallel data},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H196sainb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper7/Authors"],"keywords":["unsupervised learning","machine translation","multilingual embeddings","parallel dictionary induction","adversarial training"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}