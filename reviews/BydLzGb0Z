{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222767799,"tcdate":1511820784708,"number":3,"cdate":1511820784708,"id":"B1Fe0Zqxz","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Simple way to regularize recurrent sequence generators, limited applicability.","rating":"6: Marginally above acceptance threshold","review":"The paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. A penalty term is added that forces an agreement between the hidden states of the two decoders. During model evaluation only the forward decoder is used, with the backward operating decoder discarded. The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states.\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network. Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy.\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks.\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)? \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage? In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.\n\nClarity:\nThe paper is clearly written.\n\nOriginality:\nThe presented idea seems novel.\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods. Also, as the authors note the method seems to be limited to conditional sequence generators.\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used.\nCons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead.\nWe train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model.\nThe backward network is used only during training, and plays no role during sampling or inference.\nWe hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states).\nWe show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves 0.8 CIDEr points improvement on a COCO caption generation task.","pdf":"/pdf/eede789e34c27e83c8f9eeb2c903e6f153a40867.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1512222767838,"tcdate":1511723938937,"number":2,"cdate":1511723938937,"id":"HyciX9dxM","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"\n1) Summary\nThis paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably. The authors propose to train a forward and backward RNN in parallel. The forward RNN predicts forward in time and the backward RNN predicts backwards in time. While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step. In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition.\n\n\n2) Pros:\n+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.\n+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.\n+ Improved performance in speech recognition task.\n+ The idea is clearly explained and well motivated.\n\n\n3) Cons:\nImage captioning experiment:\nIn the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines. This experiment shows improvement over such baselines, however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016. It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done.\n\n\nUnconditioned generation experiments:\nIn these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help. Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation. The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case. For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that itâ€™s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is. If the authors believe that the 50% case is not necessary, please feel free to explain why.\n\n\nAdditional comments:\nThe method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN).\nIt would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).\n\nExperiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).\n\n\n4) Conclusion\nThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse. Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past. The proposed method presents a possible way of better modeling the future, however, some the results do not clearly back up the claim yet. The given score will improve if the authors are able to address the stated issues.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead.\nWe train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model.\nThe backward network is used only during training, and plays no role during sampling or inference.\nWe hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states).\nWe show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves 0.8 CIDEr points improvement on a COCO caption generation task.","pdf":"/pdf/eede789e34c27e83c8f9eeb2c903e6f153a40867.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1512222767877,"tcdate":1511648889841,"number":1,"cdate":1511648889841,"id":"HJzYCPDlf","invitation":"ICLR.cc/2018/Conference/-/Paper790/Official_Review","forum":"BydLzGb0Z","replyto":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference/Paper790/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper reads well, has sufficient reference. The idea is simple and well explained. Positive empirial results support the proposed regularizer.","rating":"8: Top 50% of accepted papers, clear accept","review":"Twin Networks: Using the Future as a Regularizer\n\n** PAPER SUMMARY **\n\nThe authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN. Both RNNs are trained jointly and only the forward model is used at test time. Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer.\n\n** REVIEW SUMMARY **\n\nThe paper reads well, has sufficient reference. The idea is simple and well explained. Positive empirial results support the proposed regularizer.\n\n** DETAILED REVIEW **\n\nOverall, this is a good paper. I have a few suggestions along the text but nothing major.\n\nIn related work, I would cite co-training approaches. In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu. \"A survey on multi-view learning.\" arXiv preprint arXiv:1304.5634 (2013). I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained? \n\nIn section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix. I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings. Could you comment why this would not happen?\n\nIn Section 4.2, you need to refer to Table 2 in the text. You also need to define the evaluation metrics used. In this section, why are you not reporting the results from the original Show&Tell paper? How does your implementation compare to the original work?\n\nOn unconditional generation, your hypothesis on uncertainty is interesting and could be tested. You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates. Would your regularizer still be helpful there? At which point would it break?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead.\nWe train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model.\nThe backward network is used only during training, and plays no role during sampling or inference.\nWe hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states).\nWe show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves 0.8 CIDEr points improvement on a COCO caption generation task.","pdf":"/pdf/eede789e34c27e83c8f9eeb2c903e6f153a40867.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]}},{"tddate":null,"ddate":null,"tmdate":1510092386650,"tcdate":1509134928036,"number":790,"cdate":1510092364410,"id":"BydLzGb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BydLzGb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Twin Networks: Matching the Future for Sequence Generation","abstract":"We propose a simple technique for encouraging generative RNNs to plan ahead.\nWe train a ``backward'' recurrent network to generate a given sequence in reverse order, and we encourage states of the forward model to predict cotemporal states of the backward model.\nThe backward network is used only during training, and plays no role during sampling or inference.\nWe hypothesize that our approach eases modeling of long-term dependencies by implicitly forcing the forward states to hold information about the longer-term future (as contained in the backward states).\nWe show empirically that our approach achieves 9% relative improvement for a speech recognition task, and achieves 0.8 CIDEr points improvement on a COCO caption generation task.","pdf":"/pdf/eede789e34c27e83c8f9eeb2c903e6f153a40867.pdf","TL;DR":"The paper introduces a method of training generative recurrent networks that helps to plan ahead. We run a second RNN in a reverse direction and make a soft constraint between cotemporal forward and backward states.","paperhash":"anonymous|twin_networks_matching_the_future_for_sequence_generation","_bibtex":"@article{\n  anonymous2018twin,\n  title={Twin Networks: Matching the Future for Sequence Generation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BydLzGb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper790/Authors"],"keywords":["generative rnns","long term dependencies","speech recognition","image captioning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}