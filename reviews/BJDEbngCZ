{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222628198,"tcdate":1511820844566,"number":2,"cdate":1511820844566,"id":"Hy0XRb5lG","invitation":"ICLR.cc/2018/Conference/-/Paper380/Official_Review","forum":"BJDEbngCZ","replyto":"BJDEbngCZ","signatures":["ICLR.cc/2018/Conference/Paper380/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"I find this paper not suitable for ICLR. All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Global Convergence of Policy Gradient Methods for Linearized  Control Problems","abstract":"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular\napproach for a variety of reasons: \n1) they are easy to implement without explicit knowledge of the underlying model;\n2) they are an \"end-to-end\" approach, directly optimizing the performance metric of interest;\n3) they inherently allow for richly parameterized policies.\nA notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties.  This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities. ","pdf":"/pdf/388865faa198a3aa44171e4d48ea5a1ada0a7f11.pdf","TL;DR":"This paper shows that model-free policy gradient methods can converge to the global optimal solution for non-convex linearized control problems.","paperhash":"anonymous|global_convergence_of_policy_gradient_methods_for_linearized_control_problems","_bibtex":"@article{\n  anonymous2018global,\n  title={Global Convergence of Policy Gradient Methods for Linearized  Control Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJDEbngCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper380/Authors"],"keywords":["linear quadratic regulator","policy gradient","natural gradient","reinforcement learning","non-convex optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222628235,"tcdate":1511807766991,"number":1,"cdate":1511807766991,"id":"ry1QoRKlf","invitation":"ICLR.cc/2018/Conference/-/Paper380/Official_Review","forum":"BJDEbngCZ","replyto":"BJDEbngCZ","signatures":["ICLR.cc/2018/Conference/Paper380/AnonReviewer2"],"readers":["everyone"],"content":{"title":"GLOBAL CONVERGENCE OF POLICY GRADIENT METHODS FOR LINEARIZED CONTROL PROBLEMS","rating":"6: Marginally above acceptance threshold","review":"The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective. I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved.\n2) The authors state in the abstract (and elsewhere): \"... showing that (model free) policy gradient methods globally converge to the optimal solution ...\". This is misleading and NOT true. The authors show the convergence of the objective but not of the iterates sequence. This should be rephrased elsewhere.\n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Global Convergence of Policy Gradient Methods for Linearized  Control Problems","abstract":"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular\napproach for a variety of reasons: \n1) they are easy to implement without explicit knowledge of the underlying model;\n2) they are an \"end-to-end\" approach, directly optimizing the performance metric of interest;\n3) they inherently allow for richly parameterized policies.\nA notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties.  This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities. ","pdf":"/pdf/388865faa198a3aa44171e4d48ea5a1ada0a7f11.pdf","TL;DR":"This paper shows that model-free policy gradient methods can converge to the global optimal solution for non-convex linearized control problems.","paperhash":"anonymous|global_convergence_of_policy_gradient_methods_for_linearized_control_problems","_bibtex":"@article{\n  anonymous2018global,\n  title={Global Convergence of Policy Gradient Methods for Linearized  Control Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJDEbngCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper380/Authors"],"keywords":["linear quadratic regulator","policy gradient","natural gradient","reinforcement learning","non-convex optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739334620,"tcdate":1509110062565,"number":380,"cdate":1509739331958,"id":"BJDEbngCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJDEbngCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Global Convergence of Policy Gradient Methods for Linearized  Control Problems","abstract":"Direct policy gradient methods for reinforcement learning and continuous control problems are a popular\napproach for a variety of reasons: \n1) they are easy to implement without explicit knowledge of the underlying model;\n2) they are an \"end-to-end\" approach, directly optimizing the performance metric of interest;\n3) they inherently allow for richly parameterized policies.\nA notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties.  This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities. ","pdf":"/pdf/388865faa198a3aa44171e4d48ea5a1ada0a7f11.pdf","TL;DR":"This paper shows that model-free policy gradient methods can converge to the global optimal solution for non-convex linearized control problems.","paperhash":"anonymous|global_convergence_of_policy_gradient_methods_for_linearized_control_problems","_bibtex":"@article{\n  anonymous2018global,\n  title={Global Convergence of Policy Gradient Methods for Linearized  Control Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJDEbngCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper380/Authors"],"keywords":["linear quadratic regulator","policy gradient","natural gradient","reinforcement learning","non-convex optimization"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}