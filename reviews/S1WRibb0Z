{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222728434,"tcdate":1511935032461,"number":3,"cdate":1511935032461,"id":"ryxBhTjgM","invitation":"ICLR.cc/2018/Conference/-/Paper716/Official_Review","forum":"S1WRibb0Z","replyto":"S1WRibb0Z","signatures":["ICLR.cc/2018/Conference/Paper716/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Some gap between theory and practice","rating":"5: Marginally below acceptance threshold","review":"The authors of this paper first present a class of networks inspired by various tensor decomposition models. Then they focus on one particular decompostion known as the tensor train decomposition and points out an analogy between tensor train networks and recurrent neural networks. Finally the authors show that almost all tensor train networks (exluding a set of measure zero) require exponentially large width to represent in CP networks, which is analogous to shallow networks.\n\nWhile I enjoyed reading the gentle introduction, nice overview of past work, and the theoretical analysis that relates the rank of tensor train networks to that of CP netowkrs, I wasn't sure how to translate the finding into the corresponding neural network models, namely, recurrent neural networks and shallow MLPs.\n\nFor example, \n * How does the \"bad\" example (low TT-rank but exponentially large CP-rank) translate into a recurrent neural network?\n * For both TT-networks and CP-networks, there are multilinear interaction of the inputs/previous hidden states. How precise is the analogy? Can we somehow restrict the interactions to additive ones so that we can exactly recover MLPs or RNNs?\n\nI also did not find the experiments illuminating. First of all the authors need to provide more details about how CP or TT networks are applies to MNIST and CIFAR-10 datasets. For example, the number of input patches and the number of hidden units, etc. In addition, I would like to see the performance of RNNs and MLPs with the same number of units/rank in order to validate the analogy between these networks. Finally I think it makes sense to try some sequence datasets for which RNNs are typically used.\n\nMinor comments:\n * In p7 it would help readers to point out that B^{(s,t)} is an algebraic subset because it is an intersection of M_r and the set of matrices of rank at most q^{d/2} - 1, which is known to be algebraic.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Expressive power of recurrent neural networks","abstract":"Deep neural networks are surprisingly efficient at solving practical tasks,\nbut the theory behind this phenomenon is only starting to catch up with\nthe practice. Numerous works show that depth is the key to this efficiency.\nA certain class of deep convolutional networks – namely those that correspond\nto the Hierarchical Tucker (HT) tensor decomposition – has been\nproven to have exponentially higher expressive power than shallow networks.\nI.e. a shallow network of exponential width is required to realize\nthe same score function as computed by the deep architecture. In this paper,\nwe prove the expressive power theorem (an exponential lower bound on\nthe width of the equivalent shallow network) for a class of recurrent neural\nnetworks – ones that correspond to the Tensor Train (TT) decomposition.\nThis means that even processing an image patch by patch with an RNN\ncan be exponentially more efficient than a (shallow) convolutional network\nwith one hidden layer. Using theoretical results on the relation between\nthe tensor decompositions we compare expressive powers of the HT- and\nTT-Networks. We also implement the recurrent TT-Networks and provide\nnumerical evidence of their expressivity.","pdf":"/pdf/7da7346579ca899d57e3696f3721d6227dd100af.pdf","TL;DR":"We prove the exponential efficiency of recurrent-type neural networks over shallow networks.","paperhash":"anonymous|expressive_power_of_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018expressive,\n  title={Expressive power of recurrent neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1WRibb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper716/Authors"],"keywords":["Recurrent Neural Networks","Tensor Train","tensor decompositions","expressive power"]}},{"tddate":null,"ddate":null,"tmdate":1512222731314,"tcdate":1511623299154,"number":2,"cdate":1511623299154,"id":"HyjYq-DgM","invitation":"ICLR.cc/2018/Conference/-/Paper716/Official_Review","forum":"S1WRibb0Z","replyto":"S1WRibb0Z","signatures":["ICLR.cc/2018/Conference/Paper716/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Important result, but some room for improvement.","rating":"6: Marginally above acceptance threshold","review":"This paper investigates an expressive power of the tensor train decomposition relative to the CP-decomposition. The result of this paper is interesting and also important from a viewpoint on analysis for the tensor train decomposition.\n\nHowever, I think there is some room for improvement on this paper. Comments are as follow.\n\nC1.\nCould you describe more details about the importance of an irreducible algebraic variety? Especially, it will be nice if authors provide practical examples of tensors in $\\mathcal{M}_r$ and tensors not in $\\mathcal{M}_r$. The present description about $\\mathcal{M}_r$ is too simple and thus I cannot judge whether the restriction on $\\mathcal{M}_r$ is critical or not.\n\nC2. \nI wonder that the experiment for comparing TT-decomposition and CP-decomposition is fair, since CP-decomposition does not have the universal approximation property. Is it possible to conduct numerical experiments for comparing the ranks directly? For example, given a tensor with known CP-rank, could you measure the TT-rank of the tensor? Such experiments will improve persuasiveness of the main result presented in this paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Expressive power of recurrent neural networks","abstract":"Deep neural networks are surprisingly efficient at solving practical tasks,\nbut the theory behind this phenomenon is only starting to catch up with\nthe practice. Numerous works show that depth is the key to this efficiency.\nA certain class of deep convolutional networks – namely those that correspond\nto the Hierarchical Tucker (HT) tensor decomposition – has been\nproven to have exponentially higher expressive power than shallow networks.\nI.e. a shallow network of exponential width is required to realize\nthe same score function as computed by the deep architecture. In this paper,\nwe prove the expressive power theorem (an exponential lower bound on\nthe width of the equivalent shallow network) for a class of recurrent neural\nnetworks – ones that correspond to the Tensor Train (TT) decomposition.\nThis means that even processing an image patch by patch with an RNN\ncan be exponentially more efficient than a (shallow) convolutional network\nwith one hidden layer. Using theoretical results on the relation between\nthe tensor decompositions we compare expressive powers of the HT- and\nTT-Networks. We also implement the recurrent TT-Networks and provide\nnumerical evidence of their expressivity.","pdf":"/pdf/7da7346579ca899d57e3696f3721d6227dd100af.pdf","TL;DR":"We prove the exponential efficiency of recurrent-type neural networks over shallow networks.","paperhash":"anonymous|expressive_power_of_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018expressive,\n  title={Expressive power of recurrent neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1WRibb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper716/Authors"],"keywords":["Recurrent Neural Networks","Tensor Train","tensor decompositions","expressive power"]}},{"tddate":null,"ddate":null,"tmdate":1512222731354,"tcdate":1511592844891,"number":1,"cdate":1511592844891,"id":"SJr9X58lz","invitation":"ICLR.cc/2018/Conference/-/Paper716/Official_Review","forum":"S1WRibb0Z","replyto":"S1WRibb0Z","signatures":["ICLR.cc/2018/Conference/Paper716/AnonReviewer2"],"readers":["everyone"],"content":{"title":"An interesting theoretical paper","rating":"6: Marginally above acceptance threshold","review":"In this paper, the expressive power of neural networks characterized by tensor train (TT) decomposition, a chain-type tensor decomposition, is investigated. Here, the expressive power refers to the rank of tensor decomposition, i.e., the number of latent components. The authors compare the complexity of TT-type networks with networks structured by CP decomposition, which corresponds to shallow networks. It is proved that the space of TT-type networks with rank O(r)  can be complex as the same as the space of CP-type networks with rank poly(r).\n\nThe paper is clearly written and easy to follow. \n\nThe contribution is clear and it is distinguished from previous studies.\n\nThough I enjoyed reading this paper, I have several concerns.\n\n1. The authors compare the complexity of TT representation with CP representation (and HT representation). However, CP representation does not have universality (i.e., some tensors cannot be expressed by CP representation with finite rank, see [1]), this comparison may not make sense. It seems the comparison with Tucker-type representation makes much more sense because it has universality. \n\n2. Connecting RNN and TT representation is a bit confusing. Specifically, I found two gaps.\n   (a) RNNs reuse the same parameter against all the input x_1 to x_d. This means that G_1 to G_d in Figure 1 are all the same. That's why RNNs can handle size-varying sequences. \n   (b) Standard RNNs do not use the multilinear units shown in Figure 3, but use a simple addition of an input and the output from the previous layer (i.e., h_t = f(Wx_t + Vh_{t-1}), where h_t is the t-th hidden unit, x_t is the t-th input, W and V are weights, and f is an activation function.) \nDue to the gaps, the analysis used in this paper seems not applicable to RNNs. If this is true, the story of this paper is somewhat misleading. Or, is your theory still applicable?\n\n[1] Hackbusch, Wolfgang. Tensor spaces and numerical tensor calculus. Vol. 42. Springer Science & Business Media, 2012.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Expressive power of recurrent neural networks","abstract":"Deep neural networks are surprisingly efficient at solving practical tasks,\nbut the theory behind this phenomenon is only starting to catch up with\nthe practice. Numerous works show that depth is the key to this efficiency.\nA certain class of deep convolutional networks – namely those that correspond\nto the Hierarchical Tucker (HT) tensor decomposition – has been\nproven to have exponentially higher expressive power than shallow networks.\nI.e. a shallow network of exponential width is required to realize\nthe same score function as computed by the deep architecture. In this paper,\nwe prove the expressive power theorem (an exponential lower bound on\nthe width of the equivalent shallow network) for a class of recurrent neural\nnetworks – ones that correspond to the Tensor Train (TT) decomposition.\nThis means that even processing an image patch by patch with an RNN\ncan be exponentially more efficient than a (shallow) convolutional network\nwith one hidden layer. Using theoretical results on the relation between\nthe tensor decompositions we compare expressive powers of the HT- and\nTT-Networks. We also implement the recurrent TT-Networks and provide\nnumerical evidence of their expressivity.","pdf":"/pdf/7da7346579ca899d57e3696f3721d6227dd100af.pdf","TL;DR":"We prove the exponential efficiency of recurrent-type neural networks over shallow networks.","paperhash":"anonymous|expressive_power_of_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018expressive,\n  title={Expressive power of recurrent neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1WRibb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper716/Authors"],"keywords":["Recurrent Neural Networks","Tensor Train","tensor decompositions","expressive power"]}},{"tddate":null,"ddate":null,"tmdate":1509739144428,"tcdate":1509133257194,"number":716,"cdate":1509739141756,"id":"S1WRibb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1WRibb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Expressive power of recurrent neural networks","abstract":"Deep neural networks are surprisingly efficient at solving practical tasks,\nbut the theory behind this phenomenon is only starting to catch up with\nthe practice. Numerous works show that depth is the key to this efficiency.\nA certain class of deep convolutional networks – namely those that correspond\nto the Hierarchical Tucker (HT) tensor decomposition – has been\nproven to have exponentially higher expressive power than shallow networks.\nI.e. a shallow network of exponential width is required to realize\nthe same score function as computed by the deep architecture. In this paper,\nwe prove the expressive power theorem (an exponential lower bound on\nthe width of the equivalent shallow network) for a class of recurrent neural\nnetworks – ones that correspond to the Tensor Train (TT) decomposition.\nThis means that even processing an image patch by patch with an RNN\ncan be exponentially more efficient than a (shallow) convolutional network\nwith one hidden layer. Using theoretical results on the relation between\nthe tensor decompositions we compare expressive powers of the HT- and\nTT-Networks. We also implement the recurrent TT-Networks and provide\nnumerical evidence of their expressivity.","pdf":"/pdf/7da7346579ca899d57e3696f3721d6227dd100af.pdf","TL;DR":"We prove the exponential efficiency of recurrent-type neural networks over shallow networks.","paperhash":"anonymous|expressive_power_of_recurrent_neural_networks","_bibtex":"@article{\n  anonymous2018expressive,\n  title={Expressive power of recurrent neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1WRibb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper716/Authors"],"keywords":["Recurrent Neural Networks","Tensor Train","tensor decompositions","expressive power"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}