{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222563781,"tcdate":1511867661006,"number":3,"cdate":1511867661006,"id":"HJBzB65xf","invitation":"ICLR.cc/2018/Conference/-/Paper117/Official_Review","forum":"BJ_UL-k0b","replyto":"BJ_UL-k0b","signatures":["ICLR.cc/2018/Conference/Paper117/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Non-trivial hierarchical Bayes interpretation of MAML","rating":"7: Good paper, accept","review":"MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure. In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation. Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task.\n\nCasting MAML as HB seems a good idea. The paper does a good job of explaining the connection, but I think the presentation could be clarified. The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper. \nThe synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method. \nRegarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review. However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here. In addition, the point made by the TCML authors is fair (\"nothing prevented you from...\") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote. This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.\nThe experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.\n\nI am assessing this paper as \"7\" because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small. I think the approach is good and fruitful. \n\n\n# Suggestions on readability\n\n* I have the feeling the paper inverts $\\alpha, \\beta$ from their use in Finn 2017 (step size for meta- vs task-training). This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)\n\n* I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1. This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use. (could leave the output of ML-... implicit by writing ML-...(\\theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)\n\n* Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2).\n\n* sec6 \"Machine learning and deep learning\": I would definitely avoid this formulation, seems to tail in with all the media nonsense on \"what's the difference between ML and DL ?\". In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing.\n\n# Typos\n\n* sec1 second parag: did you really mean \"in the architecture or loss function\"? unclear.\n* sec2: over a family\n* \"common structure, so that\" (not such that)\n* orthgonal\n* sec2.1 suggestion: clarify that \\theta and \\phi are in the same space\n* sec2.2 suggestion: task-specific parameter $\\phi_j$ is distinct from ... parameters $\\phi_{j'}, j' \\neq j}\n* \"unless an approximate ... is provided\" (the use of the subjunctive here is definitely dated :-) )\n* sec3.1 task-specific parameters $\\phi_j$ (I would avoid writing just \\phi altogether to distinguish in usage from \\theta)\n* Gaussian-noised\n* approximation of the it objective\n* before eq9: \"that solves\": well, it doesn't really \"solve\" the minimisation, in that it is not a minimum; reformulate this?\n* sec4.1 innaccurate\n* well approximated\n* sec4.2 an curvature\n* (Amari 1989)\n* For the the Laplace\n* O(n^3) : what is n ?\n* sec5.2 (Ravi and L 2017)\n* for the the \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]}},{"tddate":null,"ddate":null,"tmdate":1511815893425,"tcdate":1511815893425,"number":1,"cdate":1511815893425,"id":"r1aR9l5lG","invitation":"ICLR.cc/2018/Conference/-/Paper117/Official_Comment","forum":"BJ_UL-k0b","replyto":"rJOy7ZYxG","signatures":["ICLR.cc/2018/Conference/Paper117/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper117/Authors"],"content":{"title":"Regarding architectural variation in the evaluation of meta-learning algorithms","comment":"We thank the authors of TCML for their comment regarding a comparison to TCML.\n\nWe emphasize that the primary focus of our work is not to perform an exhaustive exploration of neural network architectures for few-shot classification, but instead to reinterpret and propose improvements to the MAML algorithm from a probabilistic perspective. This is the reason why we have chosen to work with the model architecture that, to the best of our knowledge, all prior work in this area that reports on the miniImageNet task (see below for a list) uses with the exceptions of TCML, MetaNetworks and Relation Networks. We thus consider the exploration of more expressive architectures not an omission but a standardization choice.\n\nCertainly, there is likely to be a better architecture for MAML and L-MAML, just as there is almost certainly a better architecture for matching networks, prototypical networks, and the various other meta-learning methods. But the focus of evaluating such meta-learning algorithms is to decouple the question of algorithm design from the question of architecture design. Nevertheless, we agree that scalability is an important criterion for evaluating meta-learning methods.\n\nLastly, we note that a similar method to TCML (https://openreview.net/forum?id=B1DmUzWAW) reports results with a shallow miniImageNet embedding: \"5-way mini-Imagenet: 45.1% and 55.2% (1-shot, 5-shot).\" In future revisions of this work, we will treat this as the comparison point for a temporal-convolution-based meta-learner that employs the standard shallower architecture.\n\n-----------------------------------------------------------------------------------------------------------------------------------------\nPrevious meta-learning methods applied to miniImageNet that employ the architecture of Vinyals et al. (2016):\n\n- Vinyals et al. (2016). \"Matching Networks for One Shot Learning.\" (https://arxiv.org/abs/1606.04080)\n- Ravi & Larochelle (2017). \"Optimization as a Model for Few-Shot Learning.\" (https://openreview.net/forum?id=rJY0-Kcll) \n- Finn et al. (2017). \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" (https://arxiv.org/abs/1703.03400)\n- Snell et al. (2017). \"Prototypical Networks for Few-shot Learning.\" (https://arxiv.org/abs/1703.05175)\n- Triantafillou et al. (2017). \"Few-Shot Learning Through an Information Retrieval Lens.\" (https://arxiv.org/abs/1707.02610)\n- Li et al. (2017). \"Meta-SGD: Learning to Learn Quickly for Few-Shot Learning.\" (https://arxiv.org/abs/1707.09835)\n\n-----------------------------------------------------------------------------------------------------------------------------------------\nPrevious meta-learning methods applied to miniImageNet that make use of an alternative architecture:\n\n- Munkhdalai & Yu (2017). \"Meta Networks.\" (https://arxiv.org/abs/1703.00837)\n    - Use a \"CNN [with] 5 convolutional layers, each of which is a 3×3 convolution with 64 filters, followed by a ReLU non-linearity, a 2×2 max-pooling layer, a fully connected (FC) layer, and a softmax layer\" (App. A).\n\n- Mishra et al. (2017). \"Meta-Learning with Temporal Convolutions.\" (https://arxiv.org/abs/1707.03141)\n    - Use \"14 layers of 4 residual blocks [each with] a series of [three] convolution layers followed by a residual connection and then a 2×2 max-pooling operation\" (App. C).\n\n- Sung et al. (2017). \"Learning to Compare: Relation Network for Few-Shot Learning.\" (https://arxiv.org/abs/1711.06025)\n    - On top of the standard Vinyals et al. (2016) architecture, add a relation module that \"consists of two convolutional blocks and two fully-connected layers. Each of convolutional block [sic.] is a 3×3 convolution with 64 filters followed by batch normalisation, ReLU non-linearity and 2×2 maxpooling... The two fully-connected layers are 8 and 1 dimensional, respectively.\" (Section 3.4)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]}},{"tddate":null,"ddate":null,"tmdate":1512222563826,"tcdate":1511806094638,"number":2,"cdate":1511806094638,"id":"Hkv54AYeM","invitation":"ICLR.cc/2018/Conference/-/Paper117/Official_Review","forum":"BJ_UL-k0b","replyto":"BJ_UL-k0b","signatures":["ICLR.cc/2018/Conference/Paper117/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Novel view on MAML, well presented.","rating":"7: Good paper, accept","review":"The paper reformulates the model-agnostic meta-learning algorithm (MAML) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model. This provides an interesting and, as far as I can tell, novel view on MAML. The paper uses this view to improve the MAML algorithm. The writing of the paper is excellent. Experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method, except for TCML which has been exluded using a not so convincing argument. The overview of the literature is also very well done. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]}},{"tddate":null,"ddate":null,"tmdate":1512222563865,"tcdate":1511799633723,"number":1,"cdate":1511799633723,"id":"SycLo2FxG","invitation":"ICLR.cc/2018/Conference/-/Paper117/Official_Review","forum":"BJ_UL-k0b","replyto":"BJ_UL-k0b","signatures":["ICLR.cc/2018/Conference/Paper117/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"Summary\nThe paper presents an interesting view on the recently proposed MAML formulation of meta-learning (Finn et al). The main contribution is a) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors, b) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors, c) based on these insights, the paper proposes a variant of MALM using a Laplace approximation (with additional approximations for the covariance matrix. The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task.\n\nPro:\n-            The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning.\n-            Provides valuable insight into the MAML objective and its relation to probabilistic models\n\nCon:\n-            The paper is generally well-written but I find (as a non-meta-learner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details).\n-            The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method (compared to e.g. MAML)\n-            I do not think the empirical results provide enough evidence that it is a useful/robust method. Especially it does not provide insight into which types of problems (small/large, linear/ non-linear) the method is applicable to. \n\n\nDetailed comments/questions:\n-            The use of Laplace approximation is (in the paper) motivated from a probabilistic/Bayes and uncertainty point-of-view. It would, however, seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode. Could the authors perhaps comment on:\na) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization (given the psd approximation to the Hessian), or does it only make sense after convergence? \nb) the consequence of the approximation errors on the general convergence of the proposed method (consistency and rate)\n\n-            Sec 4.1, p5: Last equation: Perhaps useful to explain the term $log(\\phi_j^* | \\theta)$ and why it is not in subroutine 4 . Should $\\phi^*$  be $\\hat \\phi$ ?\n-            Sec 4.2: “A straightforward…”: I think it would improve readability to refer back to the to the previous equation (i.e. H) such that it is clear what is meant by “straightforward”.\n-            Sec 4.2: Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here; perhaps consider formalizing the actual computations in Subroutine 4 – and provide a clearer argument (preferably proof) that this leads to consistent and robust estimator of \\theta.\n-            It is not clear from the text or experiment how the learning parameters are set.\n-            Sec 5.1: It took some effort to understand exactly what was going on in the example and particular figure 5.1; e.g., in the model definition in the body text there is no mention of the NN mentioned/used in figure 5, the blue points are not defined in the caption, the terminology e.g.  “pre-update density” is new at this point. I think it would benefit the readability to provide the reader with a bit more guidance.\n-            Sec 5.1: While the qualitative example is useful (with a bit more text), I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem (in the linear case)\n-            Sec 5.2: The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ? I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance.\n-            Sec 5.2: The authors have left out “Mishral et al” from the comparison due to the model being significantly larger than others. Could the authors provide insight into why they did not use the ResNet structure from the  tcml paper in their L-MLMA scheme ?\n-            Sec 6+7: The paper clearly states that it is not the aim to (generally) formulate the MAML as a HB. Given the advancement in gradient based inference for HB the last couple of years (e.g. variational, nested laplace , expectation propagation etc) for explicit models, could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?\n\nMinor:\n-            Sec 4.1 “…each integral in the sum in (2)…” eq 2 is a product\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]}},{"tddate":null,"ddate":null,"tmdate":1511752416233,"tcdate":1511752416233,"number":1,"cdate":1511752416233,"id":"rJOy7ZYxG","invitation":"ICLR.cc/2018/Conference/-/Paper117/Public_Comment","forum":"BJ_UL-k0b","replyto":"BJ_UL-k0b","signatures":["~Mostafa_Rohaninejad1"],"readers":["everyone"],"writers":["~Mostafa_Rohaninejad1"],"content":{"title":"Comparison with Prior Work","comment":"Dear Authors,\n\nWe much appreciate the contributions made in this paper but would like to point out an issue with the writing / reporting of experiments.  In particular, with respect to the reporting on mini-ImageNet results, we wish to draw your attention to the omission of TCML from the results table (https://arxiv.org/abs/1707.03141).  To the best of our knowledge, TCML is in fact the current SOTA on this benchmark and seems important to be included to give the reader a complete picture.\n\nA footnote says “We omit TCML (Mishra et al., 2017) as their ResNet architecture has significantly more parameters than the other methods and is thus not comparable. Their 1-shot performance is 55.71 ± 0.99”.  We do not believe this is a judicious exclusion.  The ability of TCML to work well with a larger architecture is an indication of its ability to extract signal in a more difficult optimization setting, an important characteristic for meta-learning algorithms.  Nothing is preventing other work (including yours) from using more expressive architectures.  Or if there are underlying limitations that makes such methods limited to smaller models (such as computational complexity or overfitting), this is highly relevant to the study of meta-learning, rather than something to simply be omitted.  \n\nNote we personally did test the larger models with not only TCML but also with MAML.  Our finding was that MAML was not able to benefit from the larger models, and it ended up overfitting and in fact doing worse than MAML with smaller models.\n\nSincerely, \n\nMostafa, Nikhil, Peter, and Pieter (authors of TCML) \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]}},{"tddate":null,"ddate":null,"tmdate":1509739474869,"tcdate":1509000784317,"number":117,"cdate":1509739472160,"id":"BJ_UL-k0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ_UL-k0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes","abstract":"Meta-learning allows an intelligent agent to leverage prior learning episodes as a\nbasis for quickly improving performance on a novel task. Bayesian hierarchical\nmodeling provides a theoretical framework for formalizing meta-learning as inference\nfor a set of parameters that are shared across tasks. We reformulate the\nmodel-agnostic meta-learning algorithm (MAML) by Finn et al. (2017) as a method\nfor probabilistic inference in a hierarchical Bayesian model. In contrast to prior\nmethods for meta-learning via hierarchical Bayes, MAML is naturally applicable\nto complex function approximators through its use of a scalable gradient descent\nprocedure for posterior inference. Furthermore, the identification of MAML as\nprobabilistic inference provides a way to understand the algorithm’s operation as\na meta-learning procedure, as well as an opportunity to make use of computational\nstrategies from Bayesian methods. We use this opportunity to propose an\nimprovement to the MAML algorithm inspired by approximate Bayesian posterior\ninference, and show increased performance on a few-shot learning benchmark.","pdf":"/pdf/6f9935d97cfbe5e3ef879be693333252e5fe1070.pdf","TL;DR":"A specific gradient-based meta-learning algorithm, MAML, is equivalent to an inference procedure in a hierarchical Bayesian model. We use this connection to improve MAML via methods from Bayesian parameter estimation. ","paperhash":"anonymous|recasting_gradientbased_metalearning_as_hierarchical_bayes","_bibtex":"@article{\n  anonymous2018recasting,\n  title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ_UL-k0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper117/Authors"],"keywords":["meta-learning","learning to learn","hierarchical Bayes","approximate Bayesian methods"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}