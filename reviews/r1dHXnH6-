{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222578695,"tcdate":1511808077437,"number":3,"cdate":1511808077437,"id":"r1SU3CYeM","invitation":"ICLR.cc/2018/Conference/-/Paper16/Official_Review","forum":"r1dHXnH6-","replyto":"r1dHXnH6-","signatures":["ICLR.cc/2018/Conference/Paper16/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper explores interaction tensors (or attention weights) to capture semantic information to help solve NLI or NLI alike tasks, but contributions are not clear as the claims are not convincingly supported by experiments.","rating":"6: Marginally above acceptance threshold","review":"Pros: \nThe paper proposes a “Densely Interactive Inference Network (DIIN)” for NLI or NLI alike tasks. Although using tensors to capture high-order interaction and performing dimension reduction over that are both not novel, the paper explores them for NLI. The paper is written clearly and is very easy to follow. The ablation experiments in Table 5 give a good level of details to help observe different components' effectiveness.\nCons:\n1) The differences of performances between the proposed model and the previous models are not very clear. With regard to MultiNLI, since the previous results (e.g., those in Table 2) did not use cross-sentence attention and had to represent a premise or a hypothesis as a *fixed-length* vector, is it fair to compare DIIN with them? Note that the proposed DIIN model does represent a premise or a hypothesis by variable lengths (see interaction layer in Figure 1), and tensors provide some sorts of attention between them. Can this (Table 2) really shows the advantage of the proposed models? However, when a variable-length representation is allowed (see Table 3 on SNLI), the advantage of the model is also not observed, with no improvement as a single model (compared with ESIM) and being almost same as previous models (e.g., model 18 in Table 3) in ensembling.\n2) Method-wise, as discussed above, using tensors to capture high-order interaction and performing dimension reduction over that are both not novel.\n3) The paper mentions the use of untied parameters for premise and hypothesis, but it doesn’t compare it with tied version in the experiment section. \n4) In Table 6, for CONDITIONAL tag, why the baseline models (lower total accuracies) have a 100% accuracy, but DIIN only has about a 60% accuracy?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference over Interaction Space","abstract":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.","pdf":"/pdf/6fa4a0e2af92c43f1dac0ba6b65b41b9fb19bc67.pdf","TL;DR":"show multi-channel attention weight contains semantic feature to solve natural language inference task.","paperhash":"anonymous|natural_language_inference_over_interaction_space","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference over Interaction Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1dHXnH6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper16/Authors"],"keywords":["natural language inference","attention","SoTA","natural language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1512222579465,"tcdate":1511738679864,"number":2,"cdate":1511738679864,"id":"SJeSa6_ez","invitation":"ICLR.cc/2018/Conference/-/Paper16/Official_Review","forum":"r1dHXnH6-","replyto":"r1dHXnH6-","signatures":["ICLR.cc/2018/Conference/Paper16/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The result of this paper for recognizing textual entailment is very interesting; however, the paper is not well-written and it doesn't provide motivation and intuition for each component of their model and it needs major revision. ","rating":"4: Ok but not good enough - rejection","review":"This paper proposes Densely Interactive Inference Network to solve recognizing textual entailment via extracting a semantic feature from interaction tensor end-to-end. Their results show that this model has better performance than others.\n\nEven though the results of this paper is interesting, I have the problem with paper writing and motivation for their architecture:\n\n- Paper pages are well beyond 8-page limits for ICLR. The paper should be 8-pages + References. This paper has 11 pages excluding the references.\n-  The introduction text in the 2nd page doesn't have smooth flow and sometimes hard to follow.\n-  In my view section, 3.1 is redundant and text in section 3.2 can be improved\n-  Encoding layer in section 3.2 is really hard to follow in regards to equations and naming e.g p_{itr att} and why choose \\alpha(a,b,w)? \n-  Encoding layer in section 3.2, there is no motivation why it needs to use fuse gate.\n-  Feature Extraction Layer is very confusing again. What is FSDR or TSDR?\n-  Why the paper uses Eq. 8? the intuition behind it?\n-  One important thing which is missing in this paper, I didn't understand what is the motivation behind using each of these components? and how each of these components is selected?\n- How long does it take to train this network? Since it needs to works with other models (GLOV+ char features + POS tagging,..), it requires lots of effort to set up this network.\n\nEven though the paper outperforms others, it would be useful to the community by providing the motivation and intuition why each of these components was chosen. This is important especially for this paper because each layer of their architecture uses multiple components, i.e. embedding layer [Glov+ Character Features + Syntactical features].  In my view, having just good results are not enough and will not guarantee a publication in ICLR, the paper should be well-written and well-motivated in order to be useful for the future research and the other researchers.\nIn summary, I don't think the paper is ready yet and it needs significant revision.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference over Interaction Space","abstract":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.","pdf":"/pdf/6fa4a0e2af92c43f1dac0ba6b65b41b9fb19bc67.pdf","TL;DR":"show multi-channel attention weight contains semantic feature to solve natural language inference task.","paperhash":"anonymous|natural_language_inference_over_interaction_space","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference over Interaction Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1dHXnH6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper16/Authors"],"keywords":["natural language inference","attention","SoTA","natural language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1512222579511,"tcdate":1511737262176,"number":1,"cdate":1511737262176,"id":"HJIhPadgf","invitation":"ICLR.cc/2018/Conference/-/Paper16/Official_Review","forum":"r1dHXnH6-","replyto":"r1dHXnH6-","signatures":["ICLR.cc/2018/Conference/Paper16/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Does interaction tensor contains the required information?","rating":"6: Marginally above acceptance threshold","review":"Thank you for this paper! It is very nice piece of work and the problem of coding the \"necessary semantic information required for understanding the text\" is really a very important one.\n\nYet, as many papers, it fails to be clear in describing what is its real novelty and the introduction does not help in focussing what is this innovation. \n\nThe key point of the paper seems to demonstrate that the \"interaction tensor contains the necessary semantic information required for understanding the text\". This is a clear issue as this demostration is given only using 1) ablation studies removing gates and non capabilities; 2) analyzing the behavior of the model in the annotated subpart of the MultiNLI corpus; 3) a visual representation of the alignment produced by the model. Hence, there is not a direct analysis of what's inside the interaction tensors. This is the major limitation of the study. According to this analysis, DIIN seems to be a very good paraphrase detector and word aligner. In fact, Table 6 reports the astonishing 100% in paraphrase detection for the Mismatch examples. It seems also that examples where rules are necessary are not correctly modeled by DIIN: this is shown by the poor result on Conditional and Active Passive. Hence, DIIN seems not to be able to capture rules. \n\nFor a better demostration, there should be a clearer analysis of these \"interaction tensors\". The issue of the interpretability of what is in these tensors is gaining attention and should be taken into consideration if the main claim of the paper is that: \"interaction tensor contains the necessary semantic information required for understanding the text\". Some interesting attempts have been made in \"Harnessing Deep Neural Networks with Logic Rules\", ACL 2016 and in \"Can we explain natural language inference decisions taken with neural networks? Inference rules in distributed representations\", IJCNN 2017.\n\n\nMinor issues\n======\nCapital letters are used in the middle of some sentences, e.g. \"On the other hand, A mul\",  \"powerful capability, We hypothesize\"\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference over Interaction Space","abstract":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.","pdf":"/pdf/6fa4a0e2af92c43f1dac0ba6b65b41b9fb19bc67.pdf","TL;DR":"show multi-channel attention weight contains semantic feature to solve natural language inference task.","paperhash":"anonymous|natural_language_inference_over_interaction_space","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference over Interaction Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1dHXnH6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper16/Authors"],"keywords":["natural language inference","attention","SoTA","natural language understanding"]}},{"tddate":null,"ddate":null,"tmdate":1509739529185,"tcdate":1508389695710,"number":16,"cdate":1509739526530,"id":"r1dHXnH6-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1dHXnH6-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Natural Language Inference over Interaction Space","abstract":"Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI) dataset with respect to the strongest published system.","pdf":"/pdf/6fa4a0e2af92c43f1dac0ba6b65b41b9fb19bc67.pdf","TL;DR":"show multi-channel attention weight contains semantic feature to solve natural language inference task.","paperhash":"anonymous|natural_language_inference_over_interaction_space","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference over Interaction Space},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1dHXnH6-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper16/Authors"],"keywords":["natural language inference","attention","SoTA","natural language understanding"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}