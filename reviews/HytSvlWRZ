{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222699211,"tcdate":1511839181292,"number":3,"cdate":1511839181292,"id":"B1r0SU9gz","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper introduces a multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded.","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization. As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded.\n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop.\n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data)\n\nWeakness :\n1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature.\n2. The proof part(Section 2.2) can be extended with more details in Appendix.\n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others. \n4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong.\n5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation. \n6. Extra models like Deep Networks with/without matrix factorization could be added. ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)\n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin. Extensive experiments could be added.\n8. The performance on One-Layer Subspace Network (with only the input features) could be added. \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem, the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/07dce554fbe0e7ed70f79dd46c1359657bf90c6d.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1512222699257,"tcdate":1511703465551,"number":2,"cdate":1511703465551,"id":"r1z2QSOlz","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. The writing needs more elaboration. The experiments are unconvincing.","rating":"5: Marginally below acceptance threshold","review":"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem. The topic is important. Experiments on real data show improvements compared to several traditional approaches.\n\nMy major concerns are as follows.\n\n1. The paper is not self-contained. The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1. However, for some key steps in the proof, they refer to other references. If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix.\n\n2. The experiments are unconvincing. They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features. A major merit of DNN is that it can automatically extract useful features. However, in this experiment, the features are handcrafted before they are fed into the models. Thus, I would like to see a comparison between SN with vanilla DNN. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/07dce554fbe0e7ed70f79dd46c1359657bf90c6d.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1512222699307,"tcdate":1511448063449,"number":1,"cdate":1511448063449,"id":"SkwZAL4ef","invitation":"ICLR.cc/2018/Conference/-/Paper599/Official_Review","forum":"HytSvlWRZ","replyto":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference/Paper599/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea which is however not clearly developed. Incremental results.","rating":"4: Ok but not good enough - rejection","review":"This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases. \nDifferently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores. The framework is represented by a feed-forward deep architecture analogous to a residual network. At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables.\n\nThe idea of explicitly accounting for the boundedness of clinical scores is interesting, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales. For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application. \nFurthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets. \nThe proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, …). \n\nOverall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders. Moreover, the proposed experimental paradigm appears flawed. What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?\n\nOther remarks. \n\n- In section 2.2 and 4 there is some confusion between iteration indices and samples indices “i”. \n\n- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.  \n\n-  Figure 2, synthetic data. The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations. The loss function of Figure 2.b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure. These aspects may be a sign of convergence issues. \n\n- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V. This is a crucial parameters that is however not discussed nor analysed in the paper. \n\n- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model. The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting.\n\n- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations). Is there anything that I am missing?\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/07dce554fbe0e7ed70f79dd46c1359657bf90c6d.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]}},{"tddate":null,"ddate":null,"tmdate":1509739209317,"tcdate":1509128001453,"number":599,"cdate":1509739206660,"id":"HytSvlWRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HytSvlWRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases","abstract":"Over the past decade a wide spectrum of machine learning models have been developed to model the neurodegenerative diseases, associating biomarkers, especially non-intrusive neuroimaging markers, with key clinical scores measuring the cognitive status of patients. Multi-task learning (MTL) has been extensively explored in these studies to address challenges associated to high dimensionality and small cohort size. However, most existing MTL approaches are based on linear models and suffer from two major limitations: 1) they cannot explicitly consider upper/lower bounds in these clinical scores; 2) they lack the capability to capture complicated non-linear effects among the variables. In this paper, we propose the Subspace Network, an efficient deep modeling approach for non-linear multi-task censored regression. Each layer of the subspace network performs a multi-task censored regression to improve upon the predictions from the last layer via sketching a low-dimensional subspace to perform knowledge transfer among learning tasks. We show that under mild assumptions, for each layer the parametric subspace can be recovered using only one pass of training data. In addition, empirical results demonstrate that the proposed subspace network quickly picks up correct parameter subspaces, and outperforms state-of-the-arts in predicting neurodegenerative clinical scores using information in brain imaging. ","pdf":"/pdf/07dce554fbe0e7ed70f79dd46c1359657bf90c6d.pdf","paperhash":"anonymous|subspace_network_deep_multitask_censored_regression_for_modeling_neurodegenerative_diseases","_bibtex":"@article{\n  anonymous2018subspace,\n  title={Subspace Network: Deep Multi-Task Censored Regression for Modeling Neurodegenerative Diseases},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HytSvlWRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper599/Authors"],"keywords":["subspace","censor","multi-task","deep network"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}