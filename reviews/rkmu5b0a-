{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222793443,"tcdate":1511984131444,"number":3,"cdate":1511984131444,"id":"Hkib3t2lz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen-Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state-of-art results.\n\nThe paper is easy to follow.\n\nComment:\n\n1. Seems there still no principle to choose correct number of generators but try different setting. Although most parameters of generators are shared, the result various.\n2. Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/81bf7c75ce44785a1d1843d98b019338da486242.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1512222793486,"tcdate":1511802869885,"number":2,"cdate":1511802869885,"id":"rJAgO6KlM","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review for MGAN","rating":"7: Good paper, accept","review":"Summary:\n\nThe paper proposes a mixture of  generators to train GANs. The generators used have tied weights except the first layer that maps the random codes is generator specific, hence no extra computational cost is added.\n\n\nQuality/clarity:\n\nThe paper is well written and easy to follow.\n\nclarity: The appendix states how the weight tying is done , not the main paper, which might confuse the reader, would be better to state this weight tying that keeps the first layer free in the main text.\n\nOriginality:\n\n Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator.\n\nGeneral review:\n\n- when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). This angle should be stressed in the paper, it is in fine, *one generator* with a multimodal learned prior on z!\n\n- Taking the multimodal z further , can you try adding a mean to be learned, together with the covariances also? see if this also helps?  \n \n- in the tied weight case, in the synthetic example, can you show what each \"generator\" of the mixture learn? are they really learning modes of the data? \n\n- the theory is for general untied generators, can you comment on the tied case? I don't think the theory is any more valid, for this case, because again your implementation is one generator with a multimodal z prior.  would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/81bf7c75ce44785a1d1843d98b019338da486242.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1512222794827,"tcdate":1511799634517,"number":1,"cdate":1511799634517,"id":"Sy9Uo3Ygz","invitation":"ICLR.cc/2018/Conference/-/Paper84/Official_Review","forum":"rkmu5b0a-","replyto":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference/Paper84/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Attempt at solving mode collapse just moves the problem.","rating":"4: Ok but not good enough - rejection","review":"The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components.\n\nAll told the proposed method is quite incremental, as mixture GANs/multi-generators have been done before. The Inception scores are good but it's widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. If the generator were to generate one template per class for which the Inception network's p(y|x) had low entropy, the Inception score would be quite high even though the model had only memorized one image per class. For claims surrounding mode collapse in particular, evaluation against a parameter count matched baseline using the AIS log likelihood estimation procedure in Wu et al (2017) would be the gold standard. Frechet Inception distance has also been proposed which at least has some favourable properties relative to Inception score.\n\nThe mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious.\n\nFinally, their own qualitative results indicate that they've simply moved the problem, with clear evidence of mode collapse in one of their mixture components in figure 5c, 4th row from the bottom. Indeed, this does nothing to address the problem of mode collapse in general, as there is nothing preventing individual mixture component GANs from collapsing.\n\nUncited prior work includes Generative Adversarial Parallelization of Im et al (2016). Also, if I'm not mistaken this is quite similar to an AC-GAN, where the classes are instead randomly assigned and the generator conditioning is done in a certain way; namely the first layer activations are the sum of K embeddings which are gated by the active mixture component. More discussion of this would be warranted.\n\nOther notes:\n- The introduction contains no discussion of the ill-posedness of the GAN game as it is played in practice.\n- \"As a result, the optimization order in 1 can be reversed\" this does not accurately characterize the source of the issues, see, e.g. Goodfellow (2015) \"On distinguishability criteria...\".\n- Section 3: the second last sentence of the third paragraph is vague and doesn't really say anything. Of course parameter sharing leverages common information. How does this help to train the model effectively?\n- Section 3: Since JSD is defined between two distributions, it is not clear what JSD_pi(P_G1, P_G2, ...) refers to. The last line of the proof of theorem 2 leaps to calling this term a Jensen-Shannon divergence but it's not clear what the steps are; it looks like a regular KL divergence to me.\n- Section 3: Also, is the classifier being trained to maximize this divergence or just the generator? I assume the latter.\n- The proof of Theorem 3 makes unrealistic assumptions that we know the number of components a priori as well as their mixing proportions (pi).\n- \"... which further minimizes the objective value\" -- it minimizes a term that you introduced which is constant with respect to your learnable parameters. This is not a selling point, and I'm not sure why you bothered mentioning it.\n- There's no mention of the substitution of log (1 - D(x)) for -log(D(x)) and its effect on the interpretation as a Jensen-Shannon divergence (which I'm not sure was quite right in the first place)\n- Section 4: does the DAE introduced in DFM really introduce that much of a computational burden? \n- \"Symmetric Kullback Liebler divergence\" is not a well-known measure. The standard KL is asymmetric. Please define it.\n- Figure 2 is illegible in grayscale.\n- Improved-GAN score in Table 1 is misleading, as this was their no-label baseline. It's fine to include it but indicate it as such.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/81bf7c75ce44785a1d1843d98b019338da486242.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]}},{"tddate":null,"ddate":null,"tmdate":1509739496186,"tcdate":1508936299476,"number":84,"cdate":1509739493523,"id":"rkmu5b0a-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkmu5b0a-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"MGAN: Training Generative Adversarial Nets with Multiple Generators","abstract":"We propose in this paper a new approach to train the Generative Adversarial Nets (GANs) with a mixture of generators to overcome the mode collapsing problem. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. The idea is simple, yet proven to be extremely effective at covering diverse data modes, easily overcoming the mode collapsing problem and delivering state-of-the-art results. A minimax formulation was able to establish among a classifier, a discriminator, and a set of generators in a similar spirit with GAN. Generators create samples that are intended to come from the same distribution as the training data, whilst the discriminator determines whether samples are true data or generated by generators, and the classifier specifies which generator a sample comes from. The distinguishing feature is that internal samples are created from multiple generators, and then one of them will be randomly selected as final output similar to the mechanism of a probabilistic mixture model. We term our method Mixture Generative Adversarial Nets (MGAN). We develop theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon divergence (JSD) between the mixture of generators’ distributions and the empirical data distribution is minimal, whilst the JSD among generators’ distributions is maximal, hence effectively avoiding the mode collapsing problem. By utilizing parameter sharing, our proposed model adds minimal computational cost to the standard GAN, and thus can also efficiently scale to large-scale datasets. We conduct extensive experiments on synthetic 2D data and natural image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior performance of our MGAN in achieving state-of-the-art Inception scores over latest baselines, generating diverse and appealing recognizable objects at different resolutions, and specializing in capturing different types of objects by the generators.","pdf":"/pdf/81bf7c75ce44785a1d1843d98b019338da486242.pdf","TL;DR":"We propose a new approach to train GANs with a mixture of generators to overcome the mode collapsing problem.","paperhash":"anonymous|mgan_training_generative_adversarial_nets_with_multiple_generators","_bibtex":"@article{\n  anonymous2018mgan:,\n  title={MGAN: Training Generative Adversarial Nets with Multiple Generators},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmu5b0a-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper84/Authors"],"keywords":["GANs","Mode Collapse","Mixture","Jensen-Shannon Divergence","Inception Score","Generator","Discriminator","CIFAR-10","STL-10","ImageNet"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}