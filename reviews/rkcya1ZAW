{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222679474,"tcdate":1511818429772,"number":3,"cdate":1511818429772,"id":"Hy8TV-qgf","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer3"],"readers":["everyone"],"content":{"title":"No great novelty, but above the threshold.","rating":"6: Marginally above acceptance threshold","review":"The authors propose the use of first order Langevin dynamics as a way to transition from one latent variable to the next in the VAE setting, as opposed to the deterministic transitions of normalizing flow. The extremely popular Fokker-Planck equation is used to analyze the steady state distributions in this setting. The authors also propose the use of CTF in density estimation, as a generator of samples from the ''true'' distribution, and show competitive performance w.r.t. inception score for some common datasets.\n\nThe use of Langevin diffusion for latent transitions is a good idea in my opinion; though quite simple, it has the benefit of being straightforward to analyze with existing machinery. Though the discretized Langevin transitions in \\S 3.1 are known and widely used, I liked the motivation afforded by Lemma 2. \n\nI am not convinced that taking \\rho to be the sample distribution with equal probabilities at the z samples is a good choice in \\S 3.1; it would be better to incorporate the proximity of the langevin chain to a stationary point in the atom weights instead of setting them to 1/K. However to their credit the authors do provide an estimate of the error in the distribution stemming from their choice.   \n\nTo the best of my knowledge the use of CTF in density estimation as described in \\S 4 is new, and should be of interest to the community; though again it is fairly straightforward. Regarding the experiments, the difference in ELBO between the macVAE and the vanilla ones with normalizing flows is only about 2%; I wish the authors included a discussion on how the parameters of the discretized Langevin chain affects this, if at all.\n\nOverall I think the theory is properly described and has a couple of interesting formulations, in spite of being not particularly novel. I think CTFs like the one described here will see increased usage in the VAE setting, and thus the paper will be of interest to the community.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/00defff14c8a1f52a74a8565955b0a345a2402ee.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222679518,"tcdate":1511809807663,"number":2,"cdate":1511809807663,"id":"r1_fQy9lz","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Previous reviewer; interesting ideas furthering continuous-time flows","rating":"6: Marginally above acceptance threshold","review":"\nThe authors propose continuous-time flows as a flexible family of\ndistributions for posterior inference of latent variable models as\nwell as explicit density estimation. They build primarily on the work\nof normalizing flows from Rezende and Mohamed (2015). They derive an\ninteresting objective based on a sequence of sub-optimization\nproblems, following a variational formulation of the Fokker-Planck\nequations.\n\nI reviewed this paper for NIPS with a favorable decision toward weak\nacceptance; and the authors also addressed some of my questions in\nthis newer version (namely, some comparisons to related work; clearer\nwriting).\n\nThe experiments are only \"encouraging\"; they do not illustrate clear\nimprovements over previous methods. However, I think the work\ndemonstrates useful ideas furthering the idea of continuous-time\ntransformations that warrants acceptance.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/00defff14c8a1f52a74a8565955b0a345a2402ee.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1512222679563,"tcdate":1511512629821,"number":1,"cdate":1511512629821,"id":"HkCNqISxM","invitation":"ICLR.cc/2018/Conference/-/Paper522/Official_Review","forum":"rkcya1ZAW","replyto":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference/Paper522/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not convinced!","rating":"3: Clear rejection","review":"The authors try to use continuous time generalizations of normalizing flows for improving upon VAE-like models or for standard density estimation problems.\n\nClarity: the text is mathematically very sloppy / hand-wavy.\n\n1. I do not understand proposition (1). I do not think that the proof is correct (e.g. the generator L needs to be applied to a function -- the notation L(x) does not make too much sense): indeed, in the case when the volatility is zero (or very small), this proposition would imply that any vector field induces a volume preserving transformation, which is indeed false.\n\n2. I do not really see how the sequence of minimization Eq(5) helps in practice. The Wasserstein term is difficult to hand.\n\n3. in Equation (6), I do not really understand what $\\log(\\bar{\\rho})$ is if $\\bar{\\rho}$ is an empirical distribution. One really needs $\\bar{\\rho}$ to be a probability density to make sense of that.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/00defff14c8a1f52a74a8565955b0a345a2402ee.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]}},{"tddate":null,"ddate":null,"tmdate":1509739256786,"tcdate":1509125345906,"number":522,"cdate":1509739254129,"id":"rkcya1ZAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkcya1ZAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Continuous-Time Flows for Efficient Inference and Density Estimation","abstract":"Two fundamental problems in unsupervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. For efficient inference, normalizing flows have been recently developed to approximate a target distribution arbitrarily well. In practice, however, normalizing flows only consist of a finite number of deterministic transformations, and thus they possess no guarantee on the approximation accuracy. For density estimation, the generative adversarial network (GAN) has been advanced as an appealing model, due to its often excellent performance in generating samples. In this paper, we propose the concept of {\\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymptotically approach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adopted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an explicit  energy-based distribution with CTFs for density estimation. Experiments on various tasks demonstrate promising performance of the proposed CTF framework, compared to related techniques.","pdf":"/pdf/00defff14c8a1f52a74a8565955b0a345a2402ee.pdf","paperhash":"anonymous|continuoustime_flows_for_efficient_inference_and_density_estimation","_bibtex":"@article{\n  anonymous2018continuous-time,\n  title={Continuous-Time Flows for Efficient Inference and Density Estimation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkcya1ZAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper522/Authors"],"keywords":["continuous-time flows","efficient inference","density estimation","deep generative models"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}