{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222548705,"tcdate":1511859241630,"number":3,"cdate":1511859241630,"id":"Byf4Vs5gM","invitation":"ICLR.cc/2018/Conference/-/Paper108/Official_Review","forum":"Sk4w0A0Tb","replyto":"Sk4w0A0Tb","signatures":["ICLR.cc/2018/Conference/Paper108/AnonReviewer1"],"readers":["everyone"],"content":{"title":"On Rotational Unit of Memory","rating":"5: Marginally below acceptance threshold","review":"Summary:\nThis paper proposes a way to incorporate rotation memories into gated RNNs. They use a specific parametrization of the rotation matrices. They run experiments on several toy tasks and on language modelling with PTB character-level language modeling (which I would still consider to be toyish.)\n\n\nQuestion:\nCan the rotation proposed here cause unintentional forgetting by interleaving the memories? Because in some sense rotations are glorified summation in high-dimensions, if you do a full-rotation of a vector (360 degrees) you can end up in the same location. Thus the model might overwrite into its past memories.\n\nPros:\nProposes an interesting way to incorporate the rotation operations into the gated architectures.\n\nCons:\nThe specific choice of rotation operation is not very well justified.\nThis paper more or less uses the same architecture from Jing et al 2017 from EU-RNNs with a different parametrization for the rotation matrices.\nThe experiments are still limited to simple small-scale tasks.\n\n\nGeneral Comments:\n\nThe idea and the premise of this paper is interesting. In general the paper seems to be well-written. However the most important part of the paper section 3.1 is not very well justified. Why this particular parameterization of the rotation matrices is used and where does actually that come from? Can you point out to some citation? I think the RUM architecture section also requires better explanation on for instance why why R_t is parameterized that way (as a multiplicative function of R_{t-1}). A detailed ablation study would help too.\n\nThe model seems to perform really close to the GORU on Copying Task. I would be interested in seeing comparisons to GORU on “Associative Recall” as well. On QA task, which subset of bAbI dataset have you used? 1k or 10k training sets? \n\nOn language modelling there is only insignificant difference between the FS-LSTM-2 with FS-RUM model. This does not tell us much.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rotational Unit of Memory ","abstract":"The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM’s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.","pdf":"/pdf/c70646bd0f0e52534e894f0cafca3ea5ab0daabf.pdf","TL;DR":"A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.","paperhash":"anonymous|rotational_unit_of_memory","_bibtex":"@article{\n  anonymous2018rotational,\n  title={Rotational Unit of Memory },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk4w0A0Tb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper108/Authors"],"keywords":["RNN","unitary approach","associative memory","language modeling"]}},{"tddate":null,"ddate":null,"tmdate":1512222549478,"tcdate":1511834694883,"number":2,"cdate":1511834694883,"id":"B1yLVH5lM","invitation":"ICLR.cc/2018/Conference/-/Paper108/Official_Review","forum":"Sk4w0A0Tb","replyto":"Sk4w0A0Tb","signatures":["ICLR.cc/2018/Conference/Paper108/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice formulation of phase-coding memory for RNNs","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a RNN memory cell updating using an orthogonal rotation operator. This approach falls into the phase-encoding architectures. Overall the author's idea of generating a rotation operator using the embedded input and the transformed hidden state at the previous step is clever. Modelling this way makes the 'generating matrix' W_hh learn to couple the input to the hidden state (which contain information in the past) via the Rotation operator.\n\nI have several concerns:\n\n- The author should discuss the intuition why the rotation has to be from the generated memory target τ to the embeded input ε but not the other way around or other direction in this 2D subspace.\n- The description of parameter meter τ is not clear. Perhaps the author meant τ is the generated parameter via the parameter matrix W_hh acting upon the hidden state h_{t-1}\n- The idea of evolving the hidden state by an orthogonal matrix, of which the rotation is a special case, is similar to the GORU paper, which directly parametrizes the 'rotation' matrix. Therefore I am wondering if the better performance of this work than the GORU is because of the difference in parameterization or by limiting the orthogonal transform to only rotations (hence modelling only the phase of the hidden state). Perhaps an additional experiment is needed to verify this.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rotational Unit of Memory ","abstract":"The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM’s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.","pdf":"/pdf/c70646bd0f0e52534e894f0cafca3ea5ab0daabf.pdf","TL;DR":"A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.","paperhash":"anonymous|rotational_unit_of_memory","_bibtex":"@article{\n  anonymous2018rotational,\n  title={Rotational Unit of Memory },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk4w0A0Tb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper108/Authors"],"keywords":["RNN","unitary approach","associative memory","language modeling"]}},{"tddate":null,"ddate":null,"tmdate":1512222549514,"tcdate":1511818418254,"number":1,"cdate":1511818418254,"id":"Bkq3EZcxM","invitation":"ICLR.cc/2018/Conference/-/Paper108/Official_Review","forum":"Sk4w0A0Tb","replyto":"Sk4w0A0Tb","signatures":["ICLR.cc/2018/Conference/Paper108/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"4: Ok but not good enough - rejection","review":"The authors of this paper propose a new type of RNN architecture that modifies the reset gate of GRU with a rotational operator, where this rotational operator serves as an associative memory of their RNN model. The idea is sound, and the way they conduct experiments also make sense. The motivation and the details of the rotational memory are explained clearly. However, the experimental results reported in the paper seem to be a bit weak to support the claims made by the authors. \n\nThe performance improvements are not so clear to me. Especially, in the character level language modeling, the BPC improvement is only 0.001 when choosing the SOTA model of this dataset as the base architecture. The test BPC score is obtained as a single-run experiment on the PTB dataset, and the improvement seems to be too small. In the copying memory task shown in Section 4.1, how did GORU performed when T=200? \n\nOn the Q&A task, using the bAbI set (Section 4.3), RUM is said to be *significantly outperforming* GORU when the performance gap is 13.2%, and then, it is also said that RUM’s performance *is close to* the MeMN2N when the performance gap is 12.8%. Both performance gaps seem to be very close to each other, but the way they are interpreted in the paper is not.\n\nOverall, the writing is clear, and the idea sounds interesting, but the experimental results are not strongly correlated with the claims made in the paper. In the visual analysis, the authors assume that RUM architecture might be the architecture that utilizes the full representational power of models like RNNs. If this is the case, I would expect to see more impressive improvements in the performance, assuming that all the other conditions are properly controlled.\n\nI would suggest evaluating the model on more datasets. \n\nMinor comments: \nIn Section 2.2: Hopflied -> Hopfield\nIn Section 3.2: I believe the dimension of b_t should be 2*N_h","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Rotational Unit of Memory ","abstract":"The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM’s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.","pdf":"/pdf/c70646bd0f0e52534e894f0cafca3ea5ab0daabf.pdf","TL;DR":"A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.","paperhash":"anonymous|rotational_unit_of_memory","_bibtex":"@article{\n  anonymous2018rotational,\n  title={Rotational Unit of Memory },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk4w0A0Tb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper108/Authors"],"keywords":["RNN","unitary approach","associative memory","language modeling"]}},{"tddate":null,"ddate":null,"tmdate":1509739479739,"tcdate":1508990556480,"number":108,"cdate":1509739477082,"id":"Sk4w0A0Tb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sk4w0A0Tb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Rotational Unit of Memory ","abstract":"The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM’s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.","pdf":"/pdf/c70646bd0f0e52534e894f0cafca3ea5ab0daabf.pdf","TL;DR":"A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.","paperhash":"anonymous|rotational_unit_of_memory","_bibtex":"@article{\n  anonymous2018rotational,\n  title={Rotational Unit of Memory },\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sk4w0A0Tb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper108/Authors"],"keywords":["RNN","unitary approach","associative memory","language modeling"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}