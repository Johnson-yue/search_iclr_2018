{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222834210,"tcdate":1511995914280,"number":3,"cdate":1511995914280,"id":"ryfzqnhxz","invitation":"ICLR.cc/2018/Conference/-/Paper987/Official_Review","forum":"SkHl6MWC-","replyto":"SkHl6MWC-","signatures":["ICLR.cc/2018/Conference/Paper987/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Train improvement by adversarial trail examples generated with differential motion fields ","rating":"6: Marginally above acceptance threshold","review":"Summary:\nThe paper propose a method for generating adversarial examples in image recognition problems. The Adversarial scheme is inspired in the one proposed by Goodgellow  et al 2015 (AT) that introduces small perturbations to the data in the direction that increases the error. Such a perturbations are random (they have not structure) and lack of interpretation for a human user. The proposal is to limit the perturbations to just three kind of global motion fields: shift, centered rotation and scale (zoom in/out). Since the motions are small in scale, the authors use a first-order Taylor series approximation  (as in classical optical flow). This approximation allows to obtain close formulas for the perturbed examples; i.e. the correction factor of the Back-propagation computed derivatives w.r.t. original example. As result, the method is computational efficient respect to the AT and the perturbations are interpretable. \nExperiments demonstrate that with the MNIST database is not obtained an improvement in the error reduction but a reduction of the computational time. However, with ta more general recognition problem conducted with the CIFAR-10 database, the use of the proposed method improves both the error and the computational time, when compared with AT and Virtual Adversarial Train. \n\nComments:\n\n1. The paper presents a series os typos: FILED (title), obouve, freedm, nerual,; please check carfully.\n\n2. The Derivation of eq. (13) should be explained, It could be said that (12) can be casted as a eigenvalue problem [for example: $ max_{\\tilde v} \\| \\nabla_p L^T \\tilde v \\|^2 \\;\\; s.t. \\| v\\|=1  $] and (13) is the largest eigenvalue of $ \\nabla_p L \\nabla_p L^T $]\n\n3. The improvement in the error results in the db CIFAR-10 is good enough to see merit in the proposal approach. Maybe other perturbations with closed formula could be considered and linear combinations of them","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED","abstract":"We provide a novel thinking of regularization neural networks. We smooth the\nobjective of neural networks w.r.t small perturbations of the inputs. Different from\npreviously works, we assume the perturbations are caused by the movement field.\nWhen the magnitude of movement field approaches 0, we call it virtual movement\nfield. We measure the smoothness of the objective when virtual movement field is\napplied to inputs. By adding proper geometrical constraints to the movement field,\nthis smoothness can be approximated in close-form. We define this approximated\nsmoothness as the regularization term. By introducing the movement field, we\nprovide the geometric meaning of the perturbations and the regularization terms.\nWe derive three regularization terms which measure the smoothness w.r.t shift,\nrotation and scale respectively by adding different constraints into the movement\nfield. We evaluate our methods on synthetic data, MNIST dataset and CIFAR-10\ndataset. Experimental results show that our proposed method can significantly\nimprove the baseline neural networks.","pdf":"/pdf/e4af05ee205636e7cfcbf612220dafde4fe1170e.pdf","paperhash":"anonymous|regularization_neural_networks_via_constrained_virtual_movement_filed","_bibtex":"@article{\n  anonymous2018regularization,\n  title={REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHl6MWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper987/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222834255,"tcdate":1511893687748,"number":2,"cdate":1511893687748,"id":"H1g6cQsxM","invitation":"ICLR.cc/2018/Conference/-/Paper987/Official_Review","forum":"SkHl6MWC-","replyto":"SkHl6MWC-","signatures":["ICLR.cc/2018/Conference/Paper987/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper is well formalized and the idea is interesting. The regularization approach is novel compared to the methods of the literature. However, the experimental validation of the proposed approach is not consistent and the positioning of the proposed approach is not so clear. So, I  suggest to reject the paper in his actual form.","rating":"4: Ok but not good enough - rejection","review":"This paper tackles the overfitting problem when training neural networks based on regularization technique. More precisely, the authors propose new regularization terms that are related to the underlying virtual geometrical transformations (shift, rotation and scale) of the input data (signal, image and video). By formalizing the geometrical transformation process of a given image, the authors deduce constraints on the objective function which depend on the magnitude of the applied transformation. The proposed method is compared to three methods: one baseline and two methods of the literature (AT and VAT). The comparison is done on three datasets (synthetic data, MNIST and CIFAR10) in terms of test errors (for classification problems) and running time.\n\nThe paper is well formalized and the idea is interesting. The regularization approach is novel compared to the methods of the literature. \n\nMain concerns: \n1)\tThe experimental validation of the proposed approach is not consistent:\nThe description of the baseline method is not detailed in the paper. \nA priori, the baseline should naturally be the method without your regularization terms.\nBut, this seems to be contrary with what you displayed in Figure 3. \nIndeed, in Figure 3, there is three different graphs for the baseline method (i.e., one for each regularization term). It seems that the baseline method depends on the different kinds of regularization term, why? Same question for AT and VAT methods. \nIn practice, what is the magnitude of the perturbations? \nPlease, explain the axis of all the figures. \nPlease, explain how do you mix your different regularization terms in your method that you call VMT-all? \nAll the following points are related to the experiment for which you presented the results in Table 2: \nPlease, provide the results of all your methods on the synthetic dataset (only VMT-shift is provided). What is VMF? Do you mean VMT? \nFor the evaluations, it would be more rigorous to re-implement also the state-of-the-art methods for which you only give the results that they report in their paper. Especially, because you re-implemented AT with L-2 constraint, so, it seems straightforward to re-implement also AT with L-infinite constraint. Same remark for the dropout regularization technique, which is easy to re-implement on the dense layers of your neural networks, within the Tensorflow framework. \nAs you mentioned, your main contribution is related to running time, thus, you should give the running time in all experiments. \n\n2)\tThe method seems to be a tradeoff between accuracy and running time:\nThe VAT method performs better than all your methods in all the datasets. \nThe baseline method is faster than all the methods (Table 3). \nThis being said, the proposed method should be clearly presented in the paper as a tradeoff between accuracy and running time. \n3)\tThe positioning of the proposed approach is not so clear:  \nAs mentioned above, your method is a tradeoff between accuracy and running time. But you also mentioned (top of page 2) that the contribution of your paper is also related to the interpretability in terms of ‘’Human perception’’. Indeed, you clearly mentioned that the methods of the literature lacks interpretability. You also mentioned that your method is more ‘’geometrically’’ interpretable than methods of the literature. The link between interpretability in terms of “human perception” and “geometry” is not obvious. Anyway, the interpretability point is not sufficiently demonstrated, or at least, discussed in the paper. \n\n4)\tMany typos in the paper : \nSection 1: “farward-backward”\nSection 2.1: “we define the movement field V of as a n+1…”\nSection 2.2: “lable” - “the another” - “of how it are generated” – Sentence “Since V is normalized.” seems incomplete… - \\mathcal{L} not defined - Please, precise the simplifications like \\mathcal{L}_{\\theta} to \\mathcal{L} \nSection 3: “DISCUSSTION”\nSection 4.1: “negtive”\nFigure 2: “negetive”\nTable 2: “VMF”\nSection 4.2: “Tab 2.3” does not exist \nSection 4.3: “consists 9 convolutional” – “nerual networks”…\nPlease, always use the \\eqref latex command to refer to equations.\nThere is many others typos in the paper, so, please proofread the paper…\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED","abstract":"We provide a novel thinking of regularization neural networks. We smooth the\nobjective of neural networks w.r.t small perturbations of the inputs. Different from\npreviously works, we assume the perturbations are caused by the movement field.\nWhen the magnitude of movement field approaches 0, we call it virtual movement\nfield. We measure the smoothness of the objective when virtual movement field is\napplied to inputs. By adding proper geometrical constraints to the movement field,\nthis smoothness can be approximated in close-form. We define this approximated\nsmoothness as the regularization term. By introducing the movement field, we\nprovide the geometric meaning of the perturbations and the regularization terms.\nWe derive three regularization terms which measure the smoothness w.r.t shift,\nrotation and scale respectively by adding different constraints into the movement\nfield. We evaluate our methods on synthetic data, MNIST dataset and CIFAR-10\ndataset. Experimental results show that our proposed method can significantly\nimprove the baseline neural networks.","pdf":"/pdf/e4af05ee205636e7cfcbf612220dafde4fe1170e.pdf","paperhash":"anonymous|regularization_neural_networks_via_constrained_virtual_movement_filed","_bibtex":"@article{\n  anonymous2018regularization,\n  title={REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHl6MWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper987/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222834302,"tcdate":1511834799830,"number":1,"cdate":1511834799830,"id":"Sy_2ES9lG","invitation":"ICLR.cc/2018/Conference/-/Paper987/Official_Review","forum":"SkHl6MWC-","replyto":"SkHl6MWC-","signatures":["ICLR.cc/2018/Conference/Paper987/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to regularize neural networks by the invariance to certain types of transforms. This is framed into a minimax problem, which yields a closed form regularization when constrained to simple types of transforms. \n\nThe basic idea of using derivative to measure sensitivity has been widely known, and is related to tangent propagation and influence function. Please comment on the connection and difference. What is the substantial novelty of this current approach? \n\nThe empirical results are not particularly impressive. The performance is not as good as  (and seems significantly worse than) AT and VAT on MNIST. Could you provide an explanation? On CIFAR10, VMT-all is only comparable with VAT. Although VMT is faster than VAT, it seems not a significant advantage since is not faster in a magnitude. \n\nThe writing need to be significantly improved. Currently there are lot of typos and grammar errors, e.g., \\citep vs. \\citet; randon, abouve, batchszie;  \\mathcal{Z}^n is undefined when it first appears.\n\nIn VMT-all, how do you decide the relative importance of the three different regularizations?  \n\nIs Figure 3 the regularization on the training or testing set? Could you explain why it reflects generalization ability? ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED","abstract":"We provide a novel thinking of regularization neural networks. We smooth the\nobjective of neural networks w.r.t small perturbations of the inputs. Different from\npreviously works, we assume the perturbations are caused by the movement field.\nWhen the magnitude of movement field approaches 0, we call it virtual movement\nfield. We measure the smoothness of the objective when virtual movement field is\napplied to inputs. By adding proper geometrical constraints to the movement field,\nthis smoothness can be approximated in close-form. We define this approximated\nsmoothness as the regularization term. By introducing the movement field, we\nprovide the geometric meaning of the perturbations and the regularization terms.\nWe derive three regularization terms which measure the smoothness w.r.t shift,\nrotation and scale respectively by adding different constraints into the movement\nfield. We evaluate our methods on synthetic data, MNIST dataset and CIFAR-10\ndataset. Experimental results show that our proposed method can significantly\nimprove the baseline neural networks.","pdf":"/pdf/e4af05ee205636e7cfcbf612220dafde4fe1170e.pdf","paperhash":"anonymous|regularization_neural_networks_via_constrained_virtual_movement_filed","_bibtex":"@article{\n  anonymous2018regularization,\n  title={REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHl6MWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper987/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092382805,"tcdate":1509137665940,"number":987,"cdate":1510092360856,"id":"SkHl6MWC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkHl6MWC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED","abstract":"We provide a novel thinking of regularization neural networks. We smooth the\nobjective of neural networks w.r.t small perturbations of the inputs. Different from\npreviously works, we assume the perturbations are caused by the movement field.\nWhen the magnitude of movement field approaches 0, we call it virtual movement\nfield. We measure the smoothness of the objective when virtual movement field is\napplied to inputs. By adding proper geometrical constraints to the movement field,\nthis smoothness can be approximated in close-form. We define this approximated\nsmoothness as the regularization term. By introducing the movement field, we\nprovide the geometric meaning of the perturbations and the regularization terms.\nWe derive three regularization terms which measure the smoothness w.r.t shift,\nrotation and scale respectively by adding different constraints into the movement\nfield. We evaluate our methods on synthetic data, MNIST dataset and CIFAR-10\ndataset. Experimental results show that our proposed method can significantly\nimprove the baseline neural networks.","pdf":"/pdf/e4af05ee205636e7cfcbf612220dafde4fe1170e.pdf","paperhash":"anonymous|regularization_neural_networks_via_constrained_virtual_movement_filed","_bibtex":"@article{\n  anonymous2018regularization,\n  title={REGULARIZATION NEURAL NETWORKS VIA CONSTRAINED VIRTUAL MOVEMENT FILED},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkHl6MWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper987/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}