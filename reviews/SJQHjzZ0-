{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222819392,"tcdate":1512017333807,"number":3,"cdate":1512017333807,"id":"H1C2pZplz","invitation":"ICLR.cc/2018/Conference/-/Paper923/Official_Review","forum":"SJQHjzZ0-","replyto":"SJQHjzZ0-","signatures":["ICLR.cc/2018/Conference/Paper923/AnonReviewer3"],"readers":["everyone"],"content":{"title":"new evaluation metrics for GANs","rating":"7: Good paper, accept","review":"the paper proposes an evaluation method for training GANs using four standard distribution distances in literature namely:\n- JSD\n- Pearson-chi-square\n- MMD\n- Wasserstein-1\n\nFor each distance, a critic is initialized with parameters p. The critic is a neural network with the same architecture as the discriminator.\nThe critic then takes samples from the trained generator model, and samples from the groundtruth dataset. It trains itself to maximize the distance measure between these two distributions (trained via gradient descent).\n\nThese critics after convergence will then give a measure of the quality of the generator (lower the better).\n\nThe paper is easy to read, experiments are well thought out.\nFigure 3 is missing (e) and (f) sub-figures.\n\nWhen proposing a distance measure for GANs (which is a high standard, because everyone is looking forward to a robust measure), one has to have enough convincing to do. The paper only does experiments on two small datasets MNIST and CIFAR. If the paper has to convince me that this metric is good and should be used, I need to see experiments on one large-scale datset, such as Imagenet or LSUN. If one can clearly identify the good generators from bad generators using a weighted-sum of these 4 distances on Imagenet or LSUN, this is a metric that is going to stand well.\nIf such experiments are constructed in the rebuttal period, I shall raise my rating.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222819438,"tcdate":1511841919780,"number":2,"cdate":1511841919780,"id":"H1uFgwqeM","invitation":"ICLR.cc/2018/Conference/-/Paper923/Official_Review","forum":"SJQHjzZ0-","replyto":"SJQHjzZ0-","signatures":["ICLR.cc/2018/Conference/Paper923/AnonReviewer1"],"readers":["everyone"],"content":{"title":"review","rating":"4: Ok but not good enough - rejection","review":"This paper proposes using divergence and distance functions typically used for generative model training to evaluate the performance of various types of GANs. Through numerical evaluation, the authors observed that the behavior is consistent across various proposed metrics and the test-time metrics do not favor networks that use the same training-time criterion. \n\nMore specifically, the evaluation metric used in the paper are: 1) Jensen-Shannon divergence, 2) Constrained Pearson chi-squared, 3) Maximum Mean Discrepancy, 4) Wasserstein Distance, and 5) Inception Score. They applied those metrics to compare three different GANs: the standard DCGAN, Wasserstein DCGAN, and LS-DCGAN on MNIST and CIFAR-10 datasets. \n\nSummary:\n——\nIn summary, it is an interesting topic, but I think that the paper does not have sufficient novelty. Some empirical results are still preliminary. It is hard to judge the effectiveness of the proposed metrics for model selection and is not clear that those metrics are better qualitative descriptors to replace visual assessment. In addition, the writing should be improved. See comments below for details and other points.\n\nComments:\n——\n1.\tIn Section 3, the evaluation metrics are existing metrics and some of them have already been used in comparing GAN models.  Maximum mean discrepancy has been used before in work by Yujia Li et al. (2016, 2017)\n\n2.\tIn the experiments, the proposed metrics were only tested on small scale datasets; the authors should evaluate on larger datasets such as CIFAR-100, Toronto Faces, LSUN bedrooms or CelebA.\n\n3.\tIn the experiments, the authors noted that “Gaussian observable model might not be the ideal assumption for GANs. Moreover, we observe a high log-likelihood at the beginning of training, followed by a drop in likelihood, which then returns to the high value, and we are unable to explain why this happens.” Could the authors give explanation to this phenomenon? The authors should look into this more carefully.\n\n4.\tIn algorithm 1, it seems that the distance is computed via gradient decent. Is it possible to show that the optimization always converges? Is it meaningful to compare the metrics if some of them cannot be properly computed?\n\n5.     With many different metrics for assessing GANs, how should people choose? How do we trust the scores? Recently, Fréchet Inception Distance (FID) was proposed to evaluate the samples generated from GANs (Heusel et al. 2017), how are the above scores compared with FID?\n\nMinor Comments:\n——\n1.\tWriting should be fixed: “It seems that the common failure case of MMD is when the mean pixel intensities are a better match than texture matches (see Figure 5), and the common failure cases of IS happens to be when the samples are recognizable textures, but the intensity of the samples are either brighter or darker (see Figure 2).”\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1511230689920,"tcdate":1511230651190,"number":2,"cdate":1511230651190,"id":"SJ7anZWlM","invitation":"ICLR.cc/2018/Conference/-/Paper923/Official_Comment","forum":"SJQHjzZ0-","replyto":"ry8IOs_yM","signatures":["ICLR.cc/2018/Conference/Paper923/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper923/Authors"],"content":{"title":"Thank you for directing us to FID paper.","comment":"Thank you for directing us to FID paper.\n\nAs the reviewer stated, FID computes the distance between the Gaussian in Wasserstein-2 distance. The sufficient statistics of Gaussian comes from the first and second moment of the neural network features (convolutional neural network's feature maps). \n\nCompare to our proposed evaluation methods, FID has an advantage and disadvantage. The main advantage is the speed. The calculating of FID distance is much faster than our evaluation methods. The disadvantage of FID is that it only considers difference in first two moments of the samples, which can be insufficient unless the feature maps are Gaussian distributed. On the other hand, the four metrics that we consider does not make any assumptions about the distribution of the samples. \n\nWe agree with the reviewer on analyzing the experiments with FID! So, we have run experiments that includes FID:  We included the FID scores to Table 3, which shows the overall performance of DCGAN, LSGAN, WGAN on CIFAR10. We also evaluated on more recently proposed models, such as DRAGAN, BEGAN, EBGAN, WGAN_GP, based on the off-the-shelf package on MNIST and Fashion-MNIST.  The evaluation metric includes, LS, IW and FID. \n\n\nFor CIFAR10, the FID results agree with LS. The FID results are following:  \n         CIFAR10    FID\n          DCGAN : 0.112 +- 0.010; \n          W-DCGAN : 0.095 +- 0.003; \n          LS-DCGAN : 0.088 +- 0.008 \n(the smaller FID the better; see the Table 3 for other metric scores). According to FID, LS-DCGAN is the best among three models. \n\nFor MNIST, the three metrics, LS, IW, and FID, agree with each other on the rank as well. They all find that samples from DRAGAN is the best, then LSGAN, and so on. \n         MNIST       Metric        \n        DCGAN       IW: 0.111 +- 0.0074 LS: 0.4814 +- 0.0083  FID: 1.84 +- 0.15\n        EBGAN       IW: 0.029 +- 0.0026 LS: 0.7277 +- 0.0159  FID: 5.36 +- 0.32\n        WGAN GP  IW: 0.035 +- 0.0059 LS: 0.7314 +- 0.0194  FID: 2.67 +- 0.15\n        LSGAN        IW: 0.115 +- 0.0070 LS: 0.5058 +- 0.0117  FID: 2.20 +- 0.27\n        BEGAN       IW: 0.009 +- 0.0063 LS: -\t   \t\t            FID: 15.9 +- 0.48\n        DRAGAN    IW: 0.116 +- 0.0116 LS: 0.4632 +- 0.0247  FID: 1.09 +- 0.13\n\nFor Fashion-MNIST, LS, IW and FID agree on the rank of worst ones, but there are some subtle difference between LS and IW versus FID.   According to LS and IW, DRAGAN samples are ranked first and LSGAN samples are ranked second, and visa versa for FID. \n        Fashion-MNIST       Metric\n        DCGAN       IW: 0.69 +- 0.0057  LS: 0.0202 +- 0.00242 FID:   3.23 +- 0.34\n        EBGAN       IW: 0.99 +- 0.0001  LS: 2.2e-5 +- 5.3e-5  FID: 104.08 +- 0.56\n        WGAN GP     IW: 0.89 +- 0.0086  LS: 0.0005 +- 0.00037 FID:   2.56 +- 0.25\n        LSGAN       IW: 0.68 +- 0.0086  LS: 0.0208 +- 0.00290 FID:   0.62 +- 0.13\n        BEGAN       IW: 0.90 +- 0.0159  LS: 0.0016 \t  0.00047 FID:   1.51 +- 0.16\n        DRAGAN      IW: 0.66 +- 0.0108  LS: 0.0219 +- 0.00232 FID:   0.97 +- 0.14\n          \n                              \nWe will make sure to add these descriptions and experiment results in the paper for the next revision cycle.\n\nThank you!!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222819487,"tcdate":1511180650915,"number":1,"cdate":1511180650915,"id":"ryX_FSexG","invitation":"ICLR.cc/2018/Conference/-/Paper923/Official_Review","forum":"SJQHjzZ0-","replyto":"SJQHjzZ0-","signatures":["ICLR.cc/2018/Conference/Paper923/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Through evaluation of current popular GAN variants. ","rating":"6: Marginally above acceptance threshold","review":"Through evaluation of current popular GAN variants. \n  * useful AIS figure\n  * useful example of failure mode of inception scores\n   * interesting to see that using a metric based on a model’s distance does not make the model better at that distance\nthe main criticism that can be given to the paper is that the proposed metrics are based on trained models which do not have an independent clear evaluation metrics (as classifiers do for inception scores). However, the authors do show that the results are consistent when changing the critic architecture. Would be nice to see if this also holds for changes in learning rates. \n * nice to see an evaluation on how models scale with the increase in training data.\n\nUsing an Independent critic for evaluation has been proposed and used in practice before, see “Comparison of Maximum Likelihood and GAN-based training of Real NVPs”, Danihelka et all, as well as Variational Approaches for Auto-Encoding Generative Adversarial Networks, Rosca at all.\n\nImprovements to be added to the paper:\n   * How about overfitting? Would be nice to mention whether the proposed metrics are useful at detecting overfitting. From algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used. However, if the generator completely memorizes the training set, the critic will not be able to learn anything useful. In that case, the test measure will not provide any information either. A way to go around this is to use validation data to train the critic, not training data. In that case, the critic can learn the difference between training and validation data and at test time the test set can be used. \n  * Using the WGAN with weight clipping is not a good baseline. The improved WGAN method is more robust to hyper parameters and is the one currently  used by the community. The WGAN with weight clipping is quite sensitive to the clipping hyperparameter, but the authors do not report having changed it from the original paper, both for the critic or for the discriminator used during training. \n  *  Is there a guidance for  which metric should be used? \n\nFigure 3 needs to be made a bit larger, it is quite hard to read in the current set up. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510680653868,"tcdate":1510680653868,"number":2,"cdate":1510680653868,"id":"ry8IOs_yM","invitation":"ICLR.cc/2018/Conference/-/Paper923/Public_Comment","forum":"SJQHjzZ0-","replyto":"SJQHjzZ0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Fréchet Inception Distance (FID) for evaluating GANs","comment":"[1] proposed the Fréchet Inception Distance (FID) to evaluate GANs which is the Fréchet distance aka Wasserstein-2 distance between the real world and generated samples statistics.  [1] showed in their experiments clearly a much more  consistent behaviour of the FID compared to the Inception Score. It is now unclear if the analysis in this paper could be improved by using the FID for GAN evaluations.\n\n[1] https://arxiv.org/abs/1706.08500\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510371205916,"tcdate":1510371205916,"number":1,"cdate":1510371205916,"id":"SkCF1x4kG","invitation":"ICLR.cc/2018/Conference/-/Paper923/Official_Comment","forum":"SJQHjzZ0-","replyto":"r1GaH_NAW","signatures":["ICLR.cc/2018/Conference/Paper923/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper923/Authors"],"content":{"title":"Thank you for directing us to C2ST","comment":"Thank you for directing us to C2ST.\n\nThere is a relationship between the methods proposed and classifier two-sample tests (C2ST). C2ST proposes to train a classifier that can distinguish samples drawn from two distribution P and Q and accept/reject the null hypothesis.\n\nOne of the commonalities that shared between our proposed test methods and C2ST is that both require optimizing a function (training a neural network) at test time. C2ST trains neural network to maximize the classification between data and samples, whereas our method is training neural network to maximize the distance between the data and samples.\n\nIn our paper, we considered four distance metrics that belong to two class of metrics, $\\phi$-divergence and IPMs. Sriperumbudur et al. have shown that the optimal risk function is associated with a binary classifier with P and Q distribution conditioned on class when the discriminant function is restricted to certain F (Theorem 17 from Sriperumbudur et al).\n\nLet the optimal risk function be\n\n   R(L,F) = inf_{f \\in F} \\int L(y, f(x)) dp(x,y)\n\nwhere F is the set of discriminant functions (classifier), y \\in {-1,1}, and L is the loss function.\n\nBy following derivation, we can see that the optimal risk function becomes IPM:\n\nR(L,F) = inf_{f \\in F} \\int L(y, f(x)) du(x,y)\n           = inf_{f \\in F} [  eps \\int L(1, f(x)) dp(x) + (1 - eps)  \\int L(0, f(x)) dq(x)\n           = inf_{f \\in F} f dp(x) + \\inf_{f \\in F} f dq(x) \n           = - IPM\n\nwhere L(1, f(x)) = 1/eps and L(0,f(x)) = -1 / (1-eps). \n\nThe second equality is derived by separating the loss for class 1 and class 0. \nThird equality is from the way how we chose L(1,f(x)) and L(0,f(x)).\nThe last equality follow by the fact that F is symmetric around zero (f \\in F => -f \\in F). Hence, this shows that with appropriately choosing L, MMD, Wasserstein distance can be understood as the optimal L-risk associated with binary classifier with specific set of F functions. For example, Wasserstein distance and MMD distances are equivalent to optimal risk function with 1-Lipschitz classifiers and RKHS classifier with an unit length.\n\nSimilarly, since every binary classifier has a corresponding distance metric from IPM, C2ST binary classifier must have a distance function associated with it with specific set of F. For example, if the binary classifier is KNN, then we are considering IPM metric with the topology induced by KNN, as well if the classifier is neural networks then we are considering IPM metric with the topology induced by the neural network.\n\nThank you for asking relationship between our proposed testing methods and C2ST.\nWe will make sure to add these descriptions in the paper for the next revision cycle!!!\n\n\n[1] Sriperumbudur et al.  On Integral Probability Metrics, $\\phi$-divergence and binary classification."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509356985573,"tcdate":1509356985573,"number":1,"cdate":1509356985573,"id":"r1GaH_NAW","invitation":"ICLR.cc/2018/Conference/-/Paper923/Public_Comment","forum":"SJQHjzZ0-","replyto":"SJQHjzZ0-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"C2ST","comment":"What is the relationship between the methods proposed here and classifier two-sample tests?\n\nhttps://arxiv.org/abs/1610.06545"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1510092385901,"tcdate":1509137211691,"number":923,"cdate":1510092362424,"id":"SJQHjzZ0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJQHjzZ0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Quantitatively Evaluating GANs With Divergences Proposed for Training","abstract":"Generative adversarial networks (GANs) have made enormous progress in terms of theory and application in the machine learning and computer vision communities. Even though substantial progress has been made, a lack of quantitative model assessment is still a serious issue. This has led to a huge number of GAN variants being proposed, with relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics, and interestingly, the test-time metrics do not favour networks that use the same training-time criterion.","pdf":"/pdf/578a04b8857b7d7cc66ce81581dc871a67f6e3e4.pdf","TL;DR":"An empirical evaluation on generative adversarial networks","paperhash":"anonymous|quantitatively_evaluating_gans_with_divergences_proposed_for_training","_bibtex":"@article{\n  anonymous2018quantitatively,\n  title={Quantitatively Evaluating GANs With Divergences Proposed for Training},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJQHjzZ0-}\n}","keywords":["Generative adversarial networks"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper923/Authors"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}