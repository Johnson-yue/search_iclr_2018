{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222577318,"tcdate":1511983994776,"number":3,"cdate":1511983994776,"id":"SymYit2xf","invitation":"ICLR.cc/2018/Conference/-/Paper151/Official_Review","forum":"Sy21R9JAW","replyto":"Sy21R9JAW","signatures":["ICLR.cc/2018/Conference/Paper151/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Useful work showing the similarity between different neural network interpretation techniques","rating":"7: Good paper, accept","review":"The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results. The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function.\n\nA second part of the paper looks at whether explanations are global or local. The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case. The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets. Results further outline the resemblance between the compared methods.\n\nIn the appendix, the last step of the proof below Eq. 7 is unclear. As far as I can see, the variable g_i^LRP wasnâ€™t defined, and the use of Eq. 5 to achieve this last could be better explained. There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards better understanding of gradient-based attribution methods for Deep Neural Networks","abstract":"Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","pdf":"/pdf/604d428bc6f9e50bef1133d698ec58474259286f.pdf","TL;DR":"Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?","paperhash":"anonymous|towards_better_understanding_of_gradientbased_attribution_methods_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy21R9JAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper151/Authors"],"keywords":["Deep Neural Networks","Attribution methods","Theory of deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512260694201,"tcdate":1511820689942,"number":2,"cdate":1511820689942,"id":"Byt56W9lM","invitation":"ICLR.cc/2018/Conference/-/Paper151/Official_Review","forum":"Sy21R9JAW","replyto":"Sy21R9JAW","signatures":["ICLR.cc/2018/Conference/Paper151/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper, but discussion on methods which do not fit into the proposed framework could be extended.","rating":"6: Marginally above acceptance threshold","review":"The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space.\n\nThe main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'.\n\nThe unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks. However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work.\n\nSensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination. While the measure shows interesting trends towards a linear behaviour for simpler methods, it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods. Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017). Sensitivtiy-n seems to be an extension of the region perturbation idea to me.\n\nIt would be interesting to see the relation between the \"unified\" gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework. It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Towards better understanding of gradient-based attribution methods for Deep Neural Networks","abstract":"Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","pdf":"/pdf/604d428bc6f9e50bef1133d698ec58474259286f.pdf","TL;DR":"Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?","paperhash":"anonymous|towards_better_understanding_of_gradientbased_attribution_methods_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy21R9JAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper151/Authors"],"keywords":["Deep Neural Networks","Attribution methods","Theory of deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222577464,"tcdate":1511803966543,"number":1,"cdate":1511803966543,"id":"rJUrhpYxf","invitation":"ICLR.cc/2018/Conference/-/Paper151/Official_Review","forum":"Sy21R9JAW","replyto":"Sy21R9JAW","signatures":["ICLR.cc/2018/Conference/Paper151/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Ties together attribution methods in a unifying and systematic way","rating":"7: Good paper, accept","review":"This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks. The paper provides several advances:\n- \\epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training.\n- This gives a more unified way of understanding, and implementing the methods.\n- The paper points out situations when the methods are equivalent\n- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity\n- The paper proposes a new objective function to measure joint sensitivity\n\nOverall, I believe this paper to be a useful contribution to the literature. It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods. Especially the latter will be appreciated.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards better understanding of gradient-based attribution methods for Deep Neural Networks","abstract":"Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","pdf":"/pdf/604d428bc6f9e50bef1133d698ec58474259286f.pdf","TL;DR":"Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?","paperhash":"anonymous|towards_better_understanding_of_gradientbased_attribution_methods_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy21R9JAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper151/Authors"],"keywords":["Deep Neural Networks","Attribution methods","Theory of deep learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739458156,"tcdate":1509039588242,"number":151,"cdate":1509739455499,"id":"Sy21R9JAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy21R9JAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards better understanding of gradient-based attribution methods for Deep Neural Networks","abstract":"Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","pdf":"/pdf/604d428bc6f9e50bef1133d698ec58474259286f.pdf","TL;DR":"Four existing backpropagation-based attribution methods are fundamentally similar. How to assess it?","paperhash":"anonymous|towards_better_understanding_of_gradientbased_attribution_methods_for_deep_neural_networks","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards better understanding of gradient-based attribution methods for Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy21R9JAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper151/Authors"],"keywords":["Deep Neural Networks","Attribution methods","Theory of deep learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}