{"notes":[{"tddate":null,"ddate":null,"tmdate":1512250971360,"tcdate":1512249269538,"number":1,"cdate":1512249269538,"id":"H162w9x-z","invitation":"ICLR.cc/2018/Conference/-/Paper339/Public_Comment","forum":"HJDV5YxCW","replyto":"HJDV5YxCW","signatures":["~Angus_Galloway1"],"readers":["everyone"],"writers":["~Angus_Galloway1"],"content":{"title":"Comparison with BitNet would be helpful","comment":"A comparison with BitNet -- (https://arxiv.org/pdf/1708.04788.pdf) would be helpful. Although they do not go down to the very low precision (1-2 bit) case as with your paper, they do learn a unique precision for the parameters of each layer via SGD."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","abstract":"Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the “middle-out” criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.","pdf":"/pdf/997b30c27efab22a9ed7a4687462fd77508761ab.pdf","TL;DR":"We introduce fractional bitwidth approximation and show it has significant advantages.","paperhash":"anonymous|heterogeneous_bitwidth_binarization_in_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018heterogeneous,\n  title={Heterogeneous Bitwidth Binarization in Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDV5YxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper339/Authors"],"keywords":["Deep Learning","Computer Vision","Approximation"]}},{"tddate":null,"ddate":null,"tmdate":1512222622337,"tcdate":1512165832470,"number":3,"cdate":1512165832470,"id":"SklRZUJ-G","invitation":"ICLR.cc/2018/Conference/-/Paper339/Official_Review","forum":"HJDV5YxCW","replyto":"HJDV5YxCW","signatures":["ICLR.cc/2018/Conference/Paper339/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","rating":"4: Ok but not good enough - rejection","review":"This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension.\n\nThe description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages. \n\nExperiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain. Results are promising.\n\nOverall, I am leaning towards a rejection mostly due to limited novelty. \n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","abstract":"Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the “middle-out” criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.","pdf":"/pdf/997b30c27efab22a9ed7a4687462fd77508761ab.pdf","TL;DR":"We introduce fractional bitwidth approximation and show it has significant advantages.","paperhash":"anonymous|heterogeneous_bitwidth_binarization_in_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018heterogeneous,\n  title={Heterogeneous Bitwidth Binarization in Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDV5YxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper339/Authors"],"keywords":["Deep Learning","Computer Vision","Approximation"]}},{"tddate":null,"ddate":null,"tmdate":1512222622373,"tcdate":1511761574583,"number":2,"cdate":1511761574583,"id":"H1Jn8QYeG","invitation":"ICLR.cc/2018/Conference/-/Paper339/Official_Review","forum":"HJDV5YxCW","replyto":"HJDV5YxCW","signatures":["ICLR.cc/2018/Conference/Paper339/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The improvement is not significant","rating":"5: Marginally below acceptance threshold","review":"The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights.\n\n1.  The paper misses some more recent reference, e.g. [a,b]. The author should also have a discussion on them.\n\n2. Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. So, I wish to see a section on testing with Resnet and GoogleNet.\n\nIndeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.\" So, please show that.\n\n3. The paper wants to find a good trade-off on speed and accuracy. The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?\n\nMy concern is that one-bit system is already complicated to implement. Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice? One example is Section 4 in [Courbariaux et al. 2016].\n\n4. Is trade-off between 1 to 2 bits really important? \n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). Is such improvement really important?\n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b]. Extremely low bit neural network: Squeeze the last bit out with ADMM. arvix 2017","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","abstract":"Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the “middle-out” criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.","pdf":"/pdf/997b30c27efab22a9ed7a4687462fd77508761ab.pdf","TL;DR":"We introduce fractional bitwidth approximation and show it has significant advantages.","paperhash":"anonymous|heterogeneous_bitwidth_binarization_in_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018heterogeneous,\n  title={Heterogeneous Bitwidth Binarization in Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDV5YxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper339/Authors"],"keywords":["Deep Learning","Computer Vision","Approximation"]}},{"tddate":null,"ddate":null,"tmdate":1512222622412,"tcdate":1511529312814,"number":1,"cdate":1511529312814,"id":"SkYPj5Hez","invitation":"ICLR.cc/2018/Conference/-/Paper339/Official_Review","forum":"HJDV5YxCW","replyto":"HJDV5YxCW","signatures":["ICLR.cc/2018/Conference/Paper339/AnonReviewer1"],"readers":["everyone"],"content":{"title":"There are flaws in comparisons with previous works","rating":"4: Ok but not good enough - rejection","review":"This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase.\n\nThough this is an important direction to investigate, there are several issues:\n\n1. Comparison with previous results is misleading:\na.\t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%.\nb.\tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. \n\nTherefore, it is not clear that the proposed methods improve over previous approaches.\n\n2. It is not clear to me: in which dimension of the tensors are we saving the scale factor? If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3. The review of the literature is inaccurate. For example, it is not true that Courbariaux et al. (2016) “further improved accuracy on small datasets”: the main novelty there was binarizing the activations (which typically decreased the accuracy). Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \"a significant improvement over previous work\" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors).  Lastly, the statement “Typical approaches include linearly placing the quantization points” is inaccurate: it was observed that logarithmic quantization works better in various cases. For example, see Miyashita, Lee and Murmann 2016, and Hubara et al.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","abstract":"Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the “middle-out” criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.","pdf":"/pdf/997b30c27efab22a9ed7a4687462fd77508761ab.pdf","TL;DR":"We introduce fractional bitwidth approximation and show it has significant advantages.","paperhash":"anonymous|heterogeneous_bitwidth_binarization_in_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018heterogeneous,\n  title={Heterogeneous Bitwidth Binarization in Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDV5YxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper339/Authors"],"keywords":["Deep Learning","Computer Vision","Approximation"]}},{"tddate":null,"ddate":null,"tmdate":1509739356065,"tcdate":1509100079532,"number":339,"cdate":1509739353405,"id":"HJDV5YxCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJDV5YxCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Heterogeneous Bitwidth Binarization in Convolutional Neural Networks","abstract":"Recent work has shown that performing inference with fast, very-low-bitwidth\n(e.g., 1 to 2 bits) representations of values in models can yield surprisingly accurate\nresults. However, although 2-bit approximated networks have been shown to\nbe quite accurate, 1 bit approximations, which are twice as fast, have restrictively\nlow accuracy. We propose a method to train models whose weights are a mixture\nof bitwidths, that allows us to more finely tune the accuracy/speed trade-off. We\npresent the “middle-out” criterion for determining the bitwidth for each value, and\nshow how to integrate it into training models with a desired mixture of bitwidths.\nWe evaluate several architectures and binarization techniques on the ImageNet\ndataset. We show that our heterogeneous bitwidth approximation achieves superlinear\nscaling of accuracy with bitwidth. Using an average of only 1.4 bits, we are\nable to outperform state-of-the-art 2-bit architectures.","pdf":"/pdf/997b30c27efab22a9ed7a4687462fd77508761ab.pdf","TL;DR":"We introduce fractional bitwidth approximation and show it has significant advantages.","paperhash":"anonymous|heterogeneous_bitwidth_binarization_in_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018heterogeneous,\n  title={Heterogeneous Bitwidth Binarization in Convolutional Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJDV5YxCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper339/Authors"],"keywords":["Deep Learning","Computer Vision","Approximation"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}