{"notes":[{"tddate":null,"ddate":null,"tmdate":1512030625134,"tcdate":1512030625134,"number":2,"cdate":1512030625134,"id":"SyFo-HpeM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"Sy6BFU_eG","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"We are mostly concerned with learning high quality embeddings","comment":"We agree with the comment that the similarity is somewhat heuristic and inspired by the hyperbolic geometry. We care about the quality of the embeddings most of all. The use of heuristics to find good embeddings is quite common in the literature. For instance, negative sampling is often used, which abandons the strict maximization of the log probability of the softmax to learn more efficiently. What we are doing is in the same spirit as negative sampling."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511708997116,"tcdate":1511708997116,"number":2,"cdate":1511708997116,"id":"Sy6BFU_eG","invitation":"ICLR.cc/2018/Conference/-/Paper92/Public_Comment","forum":"S1xDcSR6W","replyto":"r1aJmPBef","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The similarity function still doesn't reflect the hyperbolic geometry.","comment":"What is misleading is to describe these embeddings as inheriting the geometrical properties of the hyperbolic space, which isn't true.  \n\nIt is true that the similarity function defined by the authors inherits some of the properties of the hyperbolic space, as they explained it above. \n\nHowever, it should be emphasized that it is only vaguely related to the hyperbolic metric via some heuristic, and that most properties characterizing a space endorsed with a hyperbolic structure will not be satisfied by the word-embedding space.\n\nNamely, with this similarity measure, one loses the possibility to use the conformality of the hyperbolic metric, which gave us closed forms to compute curvature tensors, volume elements, the metric tensor, the exponential map and geodesics... Which should be clearly stated at the beginning of the paper, in order to not mislead readers expecting a real exploitation of hyperbolic geometry in the embedding space. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511514852988,"tcdate":1511514852988,"number":1,"cdate":1511514852988,"id":"r1aJmPBef","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"SylAxmQeM","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"We define a similarity in the objective function for points in hyperbolic space. We do not claim that this is a dot product.","comment":"\nHyperbolic space is not a vector space and so does not have a globally defined inner-product. We have defined a measure of similarity between embedded points, which is a cosine similarity weighted by the distance in hyperbolic space. \nThe hyperbolic metric comes into this because it is the hyperbolic distances from the origin (using this metric) that weight the cosine distance.\nThe net effect is that when the coordinates of points are updated, the updates in angular directions (ie. perpendicular to the radial direction) are suppressed for points far away from the origin, by a factor related to their distance from the origin in the hyperbolic space. This has the desired effect of allowing many peripheral points to be mutually distant, while simultaneously close to central points, as, for example, in figures 3 and 4.\nThe intention is to be able to use the machinery of neural embeddings while also getting the useful geometric properties of hyperbolic space."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511366855619,"tcdate":1511366855619,"number":1,"cdate":1511366855619,"id":"SylAxmQeM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Public_Comment","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"These embeddings are not in the hyperbolic space as claimed...","comment":"This paper is completely wrong: by changing the dot-product, you cannot talk about a hyperbolic space anymore. \n\nThe dot-product given by the authors has nothing to do with the hyperbolic riemannian metric.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512222814763,"tcdate":1511304371649,"number":2,"cdate":1511304371649,"id":"rynh2mGgf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer3"],"readers":["everyone"],"content":{"title":"nice paper, good concepts, not enough experiments. ","rating":"5: Marginally below acceptance threshold","review":"This paper proposes tree vertex embeddings over hyperbolic space. The conditional predictive distribution is the softmax of <v1, v2>_H = ||v1|| ||v2|| cos(theta1-theta2), and v1, v2 are points  defined via polar coordinates (r1,theta1), and (r2,theta2).\nTo evaluate, the authors show some qualitative embeddings of graph and 2-d projections, as well as F1 scores in identifying the biggest cluster associated with a class. \n\nThe paper is well motivated, with an explanation of the technique as well as its applications in tree embedding in general. I also like the evaluations, and shows a clear benefit of this poincare embedding vs euclidean embedding.\n\nHowever, graph embeddings are now a very well explored space, and this paper does not seem to mention or compare against other hyperbolic (or any noneuclidean) embedding techniques. From a 2 second google search, I found several sources with very similar sounding concepts:\n\nMaximilian Nickel, Douwe Kiela, Poincaré Embeddings for Learning Hierarchical Representations\n\nA Cvetkovski, M Crovella, Hyperbolic Embedding and Routing for Dynamic Graphs\n\nYuval Shavitt, Tomar Tankel, Hyperbolic Embedding of Internet Graph for Distance Estimation and Overlay Construction\n\nThomas Bläsius, Tobias Friedrich, Anton Krohmer, andSören Laue. Efficient Embedding of Scale-Free Graphs in the Hyperbolic Plane\n\nI think this paper does have some novelty in applying it to the skip-gram model and using deep walk, but it should make more clear that using hyperbolic space embeddings for graphs is a popular and by now, intuitive construct. Along the same lines, the benefit of using the skip-gram and deep-walk techniques should be compared against some of the other graph embedding techniques out there, of which none are listed in the experiment section. \n\nOverall, a detailed comparison against 1 or 2 other hyperbolic graph embedding techniques would be sufficient for me to change my vote to accept. \n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512222814803,"tcdate":1511191891076,"number":1,"cdate":1511191891076,"id":"HyiISdgef","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Done before?","rating":"4: Ok but not good enough - rejection","review":"The paper considers embeddings of graph-structured data onto the hyperbolic Poincare ball. Focus is on word2vec style models but with hyperbolic embeddings. I am unable to determine how suitable an embedding space the Poincare ball really is, since I am not familiar enough with the type of data studied in the paper. I have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty:\nThe paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space. From what I can see, the paper\n\n  Poincaré Embeddings for Learning Hierarchical Representations\n  https://arxiv.org/abs/1705.08039\n\nconsider an almost identical model to the one proposed here with an almost identical motivation and application set. Some technicalities appear different, but (to me) it seems like the main claimed novelties of the present paper has already been out for a while. If this analysis is incorrect, then I encourage the authors to provide very explicit arguments for this in the rebuttal phase.\n\nOther comments:\n*) It seems to me that, by construction, most data will be pushed towards the boundary of the Poincare ball during the embedding. Is that a property you want?\n*) I found it rather surprising that the log-likelihood under consideration was pushed to an appendix of the paper, while its various derivatives are part of the main text. Given the not-so-tight page limits of ICLR, I'd recommend to provide the log-likelihood as part of the main text (it's rather difficult to evaluate the correctness of a derivative when its base function is not stated).\n*) In the introduction must energy is used on the importance of large data sets, but it appears that only fairly small-scale experiments are considered. I'd recommend a better synchronization.\n*) I find visual comparisons difficult on the Poincare ball as I am so trained at assuming Euclidean distances when making visual comparisons (I suspect most readers are as well). I think one needs to be very careful when making visual comparisons under non-trivial metrics.\n*) In the final experiment, a logistic regressor is fitted post hoc to the embedded points. Why not directly optimize a hyperbolic classifier?\n\nPros:\n+ well-written and (fairly) well-motivated.\n\nCons:\n- It appears that novelty is very limited as highly similar work (see above) has been out for a while.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1509739490519,"tcdate":1508952663717,"number":92,"cdate":1509739487864,"id":"S1xDcSR6W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks, including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly improve performance on vertex classification tasks for several real-world public datasets compared to Euclidean embeddings. ","pdf":"/pdf/d009d5fae51102f4b5a18e9a0b0bf7d2d0318c2a.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}