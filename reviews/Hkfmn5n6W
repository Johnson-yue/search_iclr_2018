{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222703971,"tcdate":1511778021972,"number":3,"cdate":1511778021972,"id":"ryA1wwKgz","invitation":"ICLR.cc/2018/Conference/-/Paper63/Official_Review","forum":"Hkfmn5n6W","replyto":"Hkfmn5n6W","signatures":["ICLR.cc/2018/Conference/Paper63/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Not very interesting/surprising. Results of similar flavor already exists. Doesn't provide additional insight.","rating":"4: Ok but not good enough - rejection","review":"This is a theory paper. The authors consider networks with single hidden layer. They assume gaussian input and binary labels. Compared to some of the existing literature, they study a more realistic model that allows for mild overparametrization and approximately speaking d_0=d_1=sqrt(N). The main result is that volume of suboptimal local minima exponentially decreases in comparison to global minima.\n\nIn my opinion, paper has multiple drawbacks.\n1) Lack of surprise factor: There are already multiple papers essentially saying similar things. I am not sure if this contributes substantially on top of existing literature.\n2) Lack of algorithmic results: While the volume of suboptimal DLM being small is an interesting result, it doesn't provide substantial algorithmic insight. Recent literature contains results that states not only all locals are global but also gradient descent provably converges to the global with a good rate. See Soltanolkotabi et al.\n3) Mean squared error for classification problem (discrete labels) does not sound reasonable to me. I believe there are already some zero error results for continuous labels. Logistic loss would have made a more compelling story.\n\nMinor comments:\ni) Results are limited to single hidden layer whereas the title states multilayer. While single hidden layer is multilayer, stating single hidden layer upfront might be more informative for the reader.\nii) Theorem 10 and Theorem 6 essentially has the same bound on the right hand side but Theorem 10 additionally divides local volume by global which decreases by exp(-2Nlog N). So it appears to me that Thm 10 is missing an additional exp(2Nlog N) factor on the right hand side.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exponentially vanishing sub-optimal local minima in multilayer neural networks","abstract":"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","pdf":"/pdf/983273b9cdfbdbb86e4c30ad433eab9db310d00e.pdf","TL;DR":"\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before","paperhash":"anonymous|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks","_bibtex":"@article{\n  anonymous2018exponentially,\n  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkfmn5n6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper63/Authors"],"keywords":["Neural Network","theory","optimization","local minima","loss landscape"]}},{"tddate":null,"ddate":null,"tmdate":1512222704016,"tcdate":1511762097948,"number":2,"cdate":1511762097948,"id":"Skq3uQKxG","invitation":"ICLR.cc/2018/Conference/-/Paper63/Official_Review","forum":"Hkfmn5n6W","replyto":"Hkfmn5n6W","signatures":["ICLR.cc/2018/Conference/Paper63/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Assumptions are more realistic, but the main results are less motivated.","rating":"6: Marginally above acceptance threshold","review":"## Summary\nThis paper aims to tackle the question: \"why does standard SGD based algorithms on neural network converge to 'good' solutions?\" \n\nPros: \nAuthors ask the question of convergence of optimization (ignoring generalization error): how \"likely\" is that an over-parameterized (d1d0 > N) single hidden layer binary classifier \"find\" a good (possibly over-fitted) local minimum. They make a set of assumptions (A1-A3) which are weaker (d1 > N^{1/2}) than the ones used earlier works. Previous works needed a wide hidden layer (d1 > N).\n\nAssumptions (d0=input dim, d1=hidden dim, N=n of datapoints, X=datapoints matrix):\nA1. Datapoints X come from a Gaussian distribution \nA2. N^{1/2} < d0 =< N\nA3. N polylog(N) < d0d1 (approximate n of. parameters)  and d1 =< N\n\nThis paper proves that total \"angular volume\" of \"regions\" (defined with respect to the piecewise linear regions of neuron activations) with differentiable bad-local minima are exponentially small when compared with to the total \"angular volume\" of \"regions\" containing only differentiable global-minimal. The proof boils down to counting arguments and concentration inequality.\n\nCons: \nNon-differentiable stationary points are left as a challenging future work on this paper. Non-differentiability aside, authors show a possible way by which shallow neural networks might be over-fitting the data. But this is only half the story and does not completely answer the question. First, exponentially vanishing (in N) volume of the \"regions\" containing bad-local minima doesn't mean that the number of bad local minima are exponentially small when compared to number global minima. \nSecondly, as the authors aptly pointed out in the discussion section, this results doesn't mean neural networks will converge to good local minima because these bad local minimas can have a large basins of attraction.\nLastly, appropriate comparisons with the existing literature is lacking. It is hinted that this paper is more general as the assumptions are more realistic. However, it comes at a cost of losing sharpness in the theoretical results. It is not well motivated why one should study the angular volume of the global and local minima. \n\n## Questions and comments\n1. How critical is Gaussian-datapoints assumption (A1)? Which part of the proof fails to generalize?  \n2. Can the proof be extended to scalar regression?  It seems hard to generalize to vector output neural networks. What about deep neural networks? \n3. Can you relate the results to other more recent works like: https://arxiv.org/pdf/1707.04926.pdf.\n4. Piecewise linear and positively homogeneous (https://arxiv.org/pdf/1506.07540.pdf) activation seem to be important assumption of the paper. It should probably be mentioned explicitly.\n5. In the experiments section, it is mentioned that \"...inputs to the hidden neurons converge to a distinctly non-zero value. This indicates we converged to DLMs.\" How can you guarantee that it is a local minimum and not a saddle point?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exponentially vanishing sub-optimal local minima in multilayer neural networks","abstract":"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","pdf":"/pdf/983273b9cdfbdbb86e4c30ad433eab9db310d00e.pdf","TL;DR":"\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before","paperhash":"anonymous|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks","_bibtex":"@article{\n  anonymous2018exponentially,\n  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkfmn5n6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper63/Authors"],"keywords":["Neural Network","theory","optimization","local minima","loss landscape"]}},{"tddate":null,"ddate":null,"tmdate":1512222704056,"tcdate":1511419703887,"number":1,"cdate":1511419703887,"id":"HkgrJeEgM","invitation":"ICLR.cc/2018/Conference/-/Paper63/Official_Review","forum":"Hkfmn5n6W","replyto":"Hkfmn5n6W","signatures":["ICLR.cc/2018/Conference/Paper63/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting result despite clear limitations","rating":"7: Good paper, accept","review":"This paper studies the question: Why does SGD on deep network is often successful, despite the fact that the objective induces bad local minima?\nThe approach in this paper is to study a standard MNN with one hidden layer. They show that in an overparametrized regime, where the number of parameters is logarithmically larger than the number of parameters in the input, the ratio between the number of (bad) local minima to the number of global minima decays exponentially. They show this for a piecewise linear activation function, and input drawn from a standard Normal distribution. Their improvement over previous work is that the required overparameterization is fairly moderate, and that the network that they considered is similar to ones used in practice. \n\nThis result seems interesting, although it is clearly not sufficient to explain even the success on the setting studied in this paper, since the number of minima of a certain type does not correspond to the probability of the SGD ending in one: to estimate the latter, the size of each basin of attraction should be taken into account. The authors are aware of this point and mention it as a disadvantage. However, since this question in general is a difficult one, any progress might be considered interesting. Hopefully, in future work it would be possible to also bound the probability of starting in one of the basins of attraction of bad local minima.\n\nThe paper is well written and well presented, and the limitations of the approach, as well as its advantages over previous work, are clearly explained. As I am not an expert on the previous works in this field, my judgment relies mostly on this work and its representation of previous work. I did not verify the proofs in the appendix. \n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exponentially vanishing sub-optimal local minima in multilayer neural networks","abstract":"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","pdf":"/pdf/983273b9cdfbdbb86e4c30ad433eab9db310d00e.pdf","TL;DR":"\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before","paperhash":"anonymous|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks","_bibtex":"@article{\n  anonymous2018exponentially,\n  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkfmn5n6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper63/Authors"],"keywords":["Neural Network","theory","optimization","local minima","loss landscape"]}},{"tddate":null,"ddate":null,"tmdate":1509739506569,"tcdate":1508842522390,"number":63,"cdate":1509739503916,"id":"Hkfmn5n6W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hkfmn5n6W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exponentially vanishing sub-optimal local minima in multilayer neural networks","abstract":"Background: Statistical mechanics results (Dauphin et al. (2014); Choromanska et al. (2015)) suggest that local minima with high error are exponentially rare in high dimensions. However, to prove low error guarantees for Multilayer Neural Networks (MNNs), previous works so far required either a heavily modified MNN model or training method, strong assumptions on the labels (e.g., “near” linear separability), or an unrealistically wide hidden layer with \\Omega\\(N) units. \n\nResults: We examine a MNN with one hidden layer of piecewise linear units, a single output, and a quadratic loss. We prove that, with high probability in the limit of N\\rightarrow\\infty datapoints, the volume of differentiable regions of the empiric loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input of dimension d_0=\\tilde{\\Omega}(\\sqrt{N}), and a more realistic number of d_1=\\tilde{\\Omega}(N/d_0) hidden units. We demonstrate our results numerically: for example, 0% binary classification training error on CIFAR with only N/d_0 = 16 hidden neurons.","pdf":"/pdf/983273b9cdfbdbb86e4c30ad433eab9db310d00e.pdf","TL;DR":"\"Bad\" local minima are vanishing in a multilayer neural net: a proof with more reasonable assumptions than before","paperhash":"anonymous|exponentially_vanishing_suboptimal_local_minima_in_multilayer_neural_networks","_bibtex":"@article{\n  anonymous2018exponentially,\n  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hkfmn5n6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper63/Authors"],"keywords":["Neural Network","theory","optimization","local minima","loss landscape"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}