{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222701679,"tcdate":1512131950169,"number":4,"cdate":1512131950169,"id":"H1IdT6AlG","invitation":"ICLR.cc/2018/Conference/-/Paper613/Official_Review","forum":"rytstxWAW","replyto":"rytstxWAW","signatures":["ICLR.cc/2018/Conference/Paper613/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Fast solution for the memory bottleneck issue in graph neural networks","rating":"7: Good paper, accept","review":"This paper addresses the memory bottleneck problem in graph neural networks and proposes a novel importance sampling scheme that is based on sampling vertices (instead of sampling local neighbors as in [1]). Experimental results demonstrate a significant speedup in per-batch training time compared to previous works while retaining similar classification accuracy on standard benchmark datasets.\n\nThe paper is well-written and proposes an elegant, and well-motivated for the memory bottleneck issue in graph neural networks.\n\nI think that this paper mostly looks solid, but I am a bit worried about the following assumption: “Specifically, we interpret that graph vertices are iid samples of some probability distribution”. As graph vertices are inter-connected and inter-dependent across edges of the graph, this iid assumption might be too strong. A short comment on why the authors take this particular interpretation would be helpful.\n\nIn the abstract the authors write: “Such a model [GCN], however, is transductive in nature because parameters are learned through convolutions with both training and test data.” — as demonstrated in Hamilton et al. (2017) [1], this class of models admits inductive learning as well as transductive learning, so the above statement is not quite accurate.\n\nFurthermore, a comment on whether this scheme would be useful for alternative graph neural network architectures, such as the one in MoNet [2] or the generic formulation of the original graph neural net [3] (nicely summarized in Gilmer et al. (2017) [4]) would be insightful (and would make the paper even stronger).\n\nI am very happy to see that the authors provide the code together with the submission (using an anonymous GitHub repository). The authors mention that “The code of GraphSAGE is downloaded from the accompany [sic] website, whereas GCN is self implemented.“ - Looking at the code it looks to me, however, as if it was based on the implementation by the authors of [5]. \n\nThe experimental comparison in terms of per-batch training time looks very impressive, yet it would be good to also include a comparison in terms of total training time per model (e.g. in the appendix). I quickly checked the provided implementation for FastGCN on Pubmed and compared it against the GCN implementation from [5], and it looks like the original GCN model is roughly 30% faster on my laptop (no batched training). This is not very surprising, as a fair comparison should involve batched training for both approaches. Nonetheless it would be good to include these results in the paper to avoid confusion.\n\nMinor issues:\n- The notation of the limit in Theorem 1 is a bit unclear. I assume the limit is taken to infinity with respect to the number of samples.\n- There are a number of typos throughout the paper (like “oppose to” instead of “opposed to”), these should be fixed in the revision.\n- It would be better to summarize Figure 3 (left) in a table, as the smaller values are difficult to read off the chart.\n\nOverall, I think that this paper can be accepted. The proposed scheme is a simple drop-in replacement for the way adjacency matrices are prepared in current implementations of graph neural nets and it promises to solve the memory issue of previous works while being substantially faster than the model in [1]. I expect the proposed approach to be useful for most graph neural network models.\n\n[1] W.L. Hamilton, R. Ying, J. Leskovec, Inductive Representation Learning on Large Graphs, NIPS 2017\n[2] F. Monti, D. Boscaini, J. Masci, E. Rodala, J. Svoboda, M.M. Bronstein, Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017\n[3] F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, G. Monfardini, The Graph Neural Network Model, IEEE Transactions on Neural Networks, 2009\n[4] J. Gilmer, S.S. Schoenholz, P.F. Riley, O. Vinyals, G.E. Dahl, Neural Message Passing for Quantum Chemistry, ICML 2017\n[5] T.N. Kipf, M. Welling, Semi-Supervised Classification with Graph Convolutional Networks, ICLR 2017","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1512222701723,"tcdate":1511766002084,"number":3,"cdate":1511766002084,"id":"SJce_4YlM","invitation":"ICLR.cc/2018/Conference/-/Paper613/Official_Review","forum":"rytstxWAW","replyto":"rytstxWAW","signatures":["ICLR.cc/2018/Conference/Paper613/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting ideas, but I have both theoretical and practical concerns","rating":"5: Marginally below acceptance threshold","review":"This work addresses a major shortcoming of recently popularized GCN. That is, when the data is equipped with the graph structure, classic SGD based methods are not  straightforward to apply. Hence it is not clear how to deal with large datasets (e.g., Reddit). Proposed approach uses an adjacency based importance sampling distribution to select only a subset of nodes on each GCN layer. Resulting loss estimate is shown to be consistent and its gradient is used to perform the weight updates.\n\nProposed approach is interesting and the direction of the work is important given recent popularity of the GCN. Nonetheless I have two major question and would be happy to revisit my score if at least one is addressed.\n\nTheory:\nSGD requires an unbiased estimate of the gradient to converge to the global optima in the convex loss case. Here, the loss estimate is shown to be consistent, but not guaranteed to be unbiased and nothing is said about the gradient in Algorithm 1. Could you please provide some intuition about the gradient estimate? I might not be familiar with some relevant results, but it appears to me that Algorithm 1 will not converge to the same solution as full data GD would.\n\nPractice:\nPer batch timings in Fig. 3 are not enough to argue that the method is faster as it might have poor convergence properties overall. Could you please show the train/test accuracies against training time for all compared methods?\n\nSome other concerns and questions:\n- It is not quite cleat what P is. You defined it as distribution over vertices of some (potentially infinite) population graph. Later on, sampling from P becomes equivalent to uniform sampling over the observed nodes. I don't see how you can define P over anything outside of the training nodes (without defining loss on the unobserved data), as then you would be sampling from a distribution with 0 mass on the parts of the support of P, and this would break the Monte Carlo assumptions.\n- Weights disappeared in the majority of the analysis. Could you please make the representation more consistent.\n- a(v,u) in Eq. 2 and A(v,u) in Eq. 5 are not defined. Do they both correspond to entries of the (normalized) adjacency?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1512222701766,"tcdate":1511765807138,"number":2,"cdate":1511765807138,"id":"HJDVPNYgf","invitation":"ICLR.cc/2018/Conference/-/Paper613/Official_Review","forum":"rytstxWAW","replyto":"rytstxWAW","signatures":["ICLR.cc/2018/Conference/Paper613/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Solid idea, excellent presentation, questions about experiments","rating":"7: Good paper, accept","review":"The paper focuses on the recently graph convolutional network (GCN) framework.\nThey authors identify a couple of issues with GCN: the fact that both training and test data need to be present at training time, making it transductive in nature and the fact that the notion of ‘neighborhood’ grows as the signal propagates through the network. The latter implies that GCNs can have a large memory footprint, making them impractical in certain cases. \nThe authors propose an alternative formulation that interprets the signals as vertex embedding functions; it also interprets  graph convolutions as integral transforms of said functions.\nStarting from mini-batches consisting purely of training data (during training) each layer performs Monte Carlo sampling on the vertices to approximate the embedding functions.\nThey show that this estimator is consistent and can be used for training the proposed architecture, FastGCN, via standard SGD. \nFinally, they analyze the estimator’s variance and propose an importance-sampling based estimator that has minimal layer-to-layer variance.\nThe experiments demonstrate that FastGCN is much faster than the alternatives, while suffering a small accuracy penalty.\n\nThis is a very good paper. The ideas are solid, the writing is excellent and the results convincing. I have a few comments and concerns listed below.\n\nComments:\n1. I agree with the anonymous commenter that the authors should provide detailed description of their experimental setup.\n2. The timing of GraphSAGE on Cora is bizarre. I’m even slightly suspicious that something might have been amiss in your setup. It is by far the smallest dataset. How do you explain GraphSAGE performing so much worse on Cora than on the bigger Pubmed and Reddit datasets? It is also on Cora that GraphSAGE seems to yield subpar accuracy, while it wins the other two datasets.\n3. As a concrete step towards grounding the proposed method on state of the art results, I would love to see at least one experiment with the same (original) data splits used in previous papers. I understand that semi-supervised learning is not the purpose of this paper, however matching previous results would dispel any concerns about setup/hyperparameter mismatch. \n4. Another thing missing is an exploration (or at least careful discussion) as to why FastGCN performs worse than the other methods in terms of accuracy and how much that relative penalty can be.\n\nMinor comments:\n5. Please add label axes to Figure 2; currently it is very hard to read. Also please label the y axis in Figure 3.\n6. The notation change in Section 3.1 was well intended, however I feel like it slowed me down significantly while reading the paper. I had already absorbed the original notation and had to go back and forth to translate to the new one. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1512222701809,"tcdate":1511449622942,"number":1,"cdate":1511449622942,"id":"B1ymVPEgM","invitation":"ICLR.cc/2018/Conference/-/Paper613/Official_Review","forum":"rytstxWAW","replyto":"rytstxWAW","signatures":["ICLR.cc/2018/Conference/Paper613/AnonReviewer3"],"readers":["everyone"],"content":{"title":"present a novel view of GCN that leads to scalable GCN further with importance sampling for variance reduction","rating":"7: Good paper, accept","review":"The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. It further explores variance reduction to speed up training via importance sampling.  The idea comes with theoretical support and experimental studies.\n\nSome questions are as follows:\n\n1) could you elaborate on n/t_l  in (5) that accounts for the normalization difference between matrix form (1) and the integral form (2) ?\n\n2) In Prop.2., there seems no essential difference between the two parts, as e(v) also depends on how the u_j's are sampled.\n\n3) what loss g is used in experiments?","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1510092427000,"tcdate":1509983750965,"number":1,"cdate":1509983750965,"id":"By1zUb00W","invitation":"ICLR.cc/2018/Conference/-/Paper613/Official_Comment","forum":"rytstxWAW","replyto":"r1grwep0Z","signatures":["ICLR.cc/2018/Conference/Paper613/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper613/Authors"],"content":{"title":"RE: experiment set up","comment":"Thank you very much for the query of the details. A small summary of the train/val/test split is in the following:\n\nCora: 2708 nodes in total. Original split 140/500/1000 -> we use 1208/500/1000.\nPubmed: 19717 nodes in total. Original split 60/500/1000 -> we use 18217/500/1000.\n\nThat is, the validation size and test size are unchanged, but we use all the rest data for training, instead of using only a small portion. More specifically, we used the same graph structure and the same input features. Then, we kept the test index unchanged, and selected 500 nodes for validation. All the remaining nodes were used for training.\n\nGCN was originally proposed as a semi-supervised (transductive) method. Hence, only a small portion of the nodes have their labels used for training. Our work, on the other hand, leans toward the supervised (inductive) setting. The main purpose is to demonstrate the scalability and speed of our method. If the training set had only a small number of nodes, the original GCN already works very well and it is not necessary to use our method. Hence, we enlarge the training set by using all available nodes (excluding validation and testing). Moreover, such a split is more coherent with that of the other data set, Reddit, used in another compared work, GraphSAGE.\n\nBecause more labels are used for training, it makes sense that the prediction results are better than those in the previous works.\n\nWe will edit the paper when allowed to address this question.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1509914424227,"tcdate":1509914424227,"number":1,"cdate":1509914424227,"id":"r1grwep0Z","invitation":"ICLR.cc/2018/Conference/-/Paper613/Public_Comment","forum":"rytstxWAW","replyto":"rytstxWAW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Please be more detailed with your experiment set up","comment":"Exactly how did you change the train/val/test split of the data sets? The accuracy values of GCN reported for Cora and Pubmed are much higher than in all previous work. Why did you not use one of the standard evaluation set ups? (Either the Planetoid split or 10/20 randomly sampled splits)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]}},{"tddate":null,"ddate":null,"tmdate":1509739200764,"tcdate":1509128609054,"number":613,"cdate":1509739198113,"id":"rytstxWAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rytstxWAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling","abstract":"The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. Such a model, however, is transductive in nature because parameters are learned through convolutions with both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.\n","pdf":"/pdf/c2a697bf5acc805876bb975b025cedd01fee3e44.pdf","paperhash":"anonymous|fastgcn_fast_learning_with_graph_convolutional_networks_via_importance_sampling","_bibtex":"@article{\n  anonymous2018fastgcn:,\n  title={FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rytstxWAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper613/Authors"],"keywords":["Graph convolutional networks","importance sampling"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}