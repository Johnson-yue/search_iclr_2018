{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222742596,"tcdate":1512115536976,"number":3,"cdate":1512115536976,"id":"BkFL6KCxf","invitation":"ICLR.cc/2018/Conference/-/Paper744/Official_Review","forum":"rJssAZ-0-","replyto":"rJssAZ-0-","signatures":["ICLR.cc/2018/Conference/Paper744/AnonReviewer2"],"readers":["everyone"],"content":{"title":"interesting direction, questions about the proposed approach","rating":"5: Marginally below acceptance threshold","review":"The authors present a new method for doing reverse curriculum training for reinforcement learning tasks with deterministic dynamics, a desired goal state at which reward is received, and the ability to teleport to any state. This covers a number of important cases of interest, including all simulated domains, and a number of robotics applications. The training proceeds in phases, where in each phase the initial starting set of states is expanded. The initial set of states used is close to the desired state goal. Each phase is initiated when 80% of the states in the current phase can reach the goal. Once the initial set of start states overlaps with the desired initial set of states for the task, training can terminate. During the training in a single phase, the algorithm uses a shaping reward (the tendency) which is based on a binary classifier that predicts if it will be possible to reach the goal from this state. This reward is combined in a hybrid reward signal. The authors suggest the use of a small number of checkpoints to guide the backwards state expansion to improve the search efficiency. Results are presented on several domains: maze, Super Mario, and Mujoco domains. \n\nThe topic of doing more sample efficient training is important and interesting, and the subset of settings the authors consider is still a good set. \n\nThe paper was clearly written though some details were relegated to the appendix which would’ve been useful to see in the main text.\n\nI’m not yet convinced about this method for the desired setting in terms of significance and quality.\n\nAn alternative to using tendency shaping reward would be (during phase expansion) make the new “goal” states any of the states in the previous phase of initial states P_{i} that did reach the goal. This should greatly reduce the decision making horizon needed in each  phase. Since the domain is deterministic, as soon as one can reach one of those states, we have a path to the goal. If we care about the number of steps to reach the goal (vs finding any path), then each of the states in P_{i} for which a successful path can be achieved to the goal can also be labeled by the cost / number of time steps to reach the goal. This should decompose the problem into a series of smaller problems. Perhaps I’m missing something-- could the authors please address this suggestion and/or explain why this wouldn’t be beneficial?\n\nThe authors currently use checkpoints to help guide the search towards the true task desired set of initial states. If those are lacking, it seems like the generation of the new P_{i+1} could be biased towards that desired set of states. One approach could be to randomly roll out from the start state and then bias P_{i+1} towards any states close to states along such trajectories. In general one could imagine a situation in which one both does forward learning/planning from the task start state and backwards learning from the goal state to obtain a significant benefit, similar to ideas that have been used in robot motion planning.\n\nWhy learn from pixels for the robot domains considered? Here it would be nice to compare to some robotics approaches. With the action space of the robot and motion planning, it seems like this problem could be tackled using existing techniques. It is interesting to have a method that can be used with pixels, but in cases where there are other approaches, it would be useful to compare to them.\n\nSmall point\nD.2 Why not compare to GAIL instead? \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRL: Discriminative Hints for Scalable Reverse Curriculum Learning","abstract":"Deep reinforcement learning algorithms have proven successful in a vast variety of domains. However, tasks with sparse rewards remain challenging when the state space is large. Goal-oriented tasks are among the most typical problems in this domain, where a reward can only be received when the final goal is accomplished. In this work, we propose a potential solution to such problems with the introduction of an experience-based tendency reward mechanism, which provides the agent with additional hints based on a discriminative learning on past experiences during an automated reverse curriculum. This mechanism not only provides dense additional learning signals on what states lead to success, but also allows the agent to retain only this tendency reward instead of the whole histories of experience during multi-phase curriculum learning. We extensively study the advantages of our method on the standard sparse reward domains like Maze and Super Mario Bros and show that our method performs more efficiently and robustly than prior approaches in tasks with long time horizons and large state space. In addition, we demonstrate that using an optional checkpoint scheme with very small quantity of key states, our approach can solve difficult robot manipulation challenges directly from perception and sparse rewards.","pdf":"/pdf/69e784dc2c3766f5e8a1fcfea501eb845724b46c.pdf","TL;DR":"We propose Tendency RL to efficiently solve goal-oriented tasks with large state space using automated curriculum learning and discriminative shaping reward, which has the potential to tackle robot manipulation tasks with perception.","paperhash":"anonymous|trl_discriminative_hints_for_scalable_reverse_curriculum_learning","_bibtex":"@article{\n  anonymous2018trl:,\n  title={TRL: Discriminative Hints for Scalable Reverse Curriculum Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJssAZ-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper744/Authors"],"keywords":["deep learning","deep reinforcement learning","robotics","perception"]}},{"tddate":null,"ddate":null,"tmdate":1512222742637,"tcdate":1511803376828,"number":2,"cdate":1511803376828,"id":"r1Kg9atxz","invitation":"ICLR.cc/2018/Conference/-/Paper744/Official_Review","forum":"rJssAZ-0-","replyto":"rJssAZ-0-","signatures":["ICLR.cc/2018/Conference/Paper744/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea, but approach seems limited. ","rating":"4: Ok but not good enough - rejection","review":"The authors extend the approach proposed in the  \"Reverse Curriculum Learning for Reinforcement Learning\" paper by adding a discriminator that gives a bonus reward to a state based on how likely it thinks the current policy is to reach the goal from said state. The discriminator is a potentially interesting mechanism to approximate multi-step backups in sparse-reward environments. \n\nThe approach of this paper seems severely severely limited by the assumptions made by the authors, mainly assuming a deterministic environment, known goal states and the ability to sample anywhere in the state space. Some of these assumptions may be reasonable in domains such as robotics, but they seem very restrictive in the domains like the games considered in the paper.\n\n\nAdditional Comments:\n\n-The authors demonstrate some benefits of using Tendency rewards, but made little attempt to explain why it leads to accelerated learning. Results are pure performance results.\n\n-The authors should probably structure the tendency reward as potential based instead of using the Gaussian kernel hack they introduce in section 4.2\n\n- Presentation: There are several mistakes and formatting issues in References\n\n- Assumption 2 transformations -> transitions?\n\n-Need to add assumption 3: advance knowledge of goal state\n\n- the use of gamma as  a scale factor in equation 2 is confusion, it was already introduced as the discount factor ( which is default notation in RL). It also isn't clear what the notation r_f denotes (is it the same as r^f in appendix?).\n\n-It is nice to see that the authors compare their method with alternative approaches. Unfortunately, the proposed method does not seem to offer many benefits. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRL: Discriminative Hints for Scalable Reverse Curriculum Learning","abstract":"Deep reinforcement learning algorithms have proven successful in a vast variety of domains. However, tasks with sparse rewards remain challenging when the state space is large. Goal-oriented tasks are among the most typical problems in this domain, where a reward can only be received when the final goal is accomplished. In this work, we propose a potential solution to such problems with the introduction of an experience-based tendency reward mechanism, which provides the agent with additional hints based on a discriminative learning on past experiences during an automated reverse curriculum. This mechanism not only provides dense additional learning signals on what states lead to success, but also allows the agent to retain only this tendency reward instead of the whole histories of experience during multi-phase curriculum learning. We extensively study the advantages of our method on the standard sparse reward domains like Maze and Super Mario Bros and show that our method performs more efficiently and robustly than prior approaches in tasks with long time horizons and large state space. In addition, we demonstrate that using an optional checkpoint scheme with very small quantity of key states, our approach can solve difficult robot manipulation challenges directly from perception and sparse rewards.","pdf":"/pdf/69e784dc2c3766f5e8a1fcfea501eb845724b46c.pdf","TL;DR":"We propose Tendency RL to efficiently solve goal-oriented tasks with large state space using automated curriculum learning and discriminative shaping reward, which has the potential to tackle robot manipulation tasks with perception.","paperhash":"anonymous|trl_discriminative_hints_for_scalable_reverse_curriculum_learning","_bibtex":"@article{\n  anonymous2018trl:,\n  title={TRL: Discriminative Hints for Scalable Reverse Curriculum Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJssAZ-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper744/Authors"],"keywords":["deep learning","deep reinforcement learning","robotics","perception"]}},{"tddate":null,"ddate":null,"tmdate":1512222742679,"tcdate":1511756436072,"number":1,"cdate":1511756436072,"id":"B129GzFxf","invitation":"ICLR.cc/2018/Conference/-/Paper744/Official_Review","forum":"rJssAZ-0-","replyto":"rJssAZ-0-","signatures":["ICLR.cc/2018/Conference/Paper744/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review: Concerns with regard to clarity and real-world applicability","rating":"4: Ok but not good enough - rejection","review":"This paper proposes a new method for reverse curriculum generation by gradually reseting the environment in phases and classifying states that tend to lead to success. It additionally proposes a mechanism for learning from human-provided \"key states\".\n\nThe ideas in this paper are quite nice, but the paper has significant issues with regard to clarity and applicability to real-world problems:\nFirst, it is unclear is the proposed method requires access only high-dimensional observations (e.g. images) during training or if it additionally requires low-dimensional states (e.g. sufficient information to reset the environment). In most compelling problems settings where a low-dimensional representation that sufficiently explains the current state of the world is available during training, then it is also likely that one can write down a nicely shaped reward function using that state information during training, in which case, it makes sense to use such a reward function. This paper seems to require access to low-dimensional states, and specifically considers the sparse-reward setting, which seems contrived.\nSecond, the paper states that the assumption \"when resetting, the agent can be reset to any state\" can be satisfied in problems such as real-world robotic manipulation. This is not correct. If the robot could autonomously reset to any state, then we would have largely solved robotic manipulation. Further, it is not always realistic to assume access to low-dimensional state information during training on a real robotic system (e.g. knowing the poses of all of the objects in the world).\nThird, the experiments section lacks crucial information needed to understand the experiments. What is the state, observation, and action space for each problem setting? What is the reward function for each problem setting? What reinforcement learning algorithm is used in combination with the curriculum and tendency rewards? Are the states and actions continuous or discrete? Without this information, it is difficult to judge the merit of the experimental setting.\nFourth, the proposed method seems to lack motivation, making the proposed scheme seem a bit ad hoc. Could each of the components be motivated further through more discussion and/or ablative studies?\nFinally, the main text of the paper is substantially longer than the recommended page limit. It should be shortened by making the writing more concise.\n\nBeyond my feedback on clarity and significance, here are further pieces of feedback with regard to the technical content, experiments, and related work:\nI'm wondering -- can the reward shaping in Equation 2 be made to satisfy the property of not affecting the final policy? (see Ng et al. '09) If so, such a reward shaping would make the method even more appealing.\nHow do the experiments in section 5.4 compare to prior methods and ablations? Without such a comparison, it is impossible to judge the performance of the proposed method and the level of difficulty of these tasks. At the very least, the paper should compare the performance of the proposed method to the performance a random policy.\n\nThe paper is missing some highly relevant references. First, how does the proposed method compare to hindsight experience replay? [1] Second, learning from keyframes (rather than demonstrations) has been explored in the past [1]. It would be preferable to use the standard terminology of \"keyframe\".\n\n[1] Andrychowicz et al. Hindsight Experience Replay. 2017\n[2] Akgun et al. Keyframe-based Learning from Demonstration. 2012\n\nIn summary, I think this paper has a number of promising ideas and experimental results, but given the significant issues in clarity and significance to real world problems, I don't think that the current version of this paper is suitable for publication in ICLR.\n\nMore minor feedback on clarity and correctness:\n- Abstract: \"Deep RL algorithms have proven successful in a vast variety of domains\" -- This is an overstatement.\n- The introduction should be more clear with regard to the assumptions. In particular, it would be helpful to see discussion of requiring human-provided keyframes. As is, it is unclear what is meant by \"checkpoint scheme\", which is not commonly used terminology.\n- \"This kind of spare reward, goal-oriented tasks are considered the most difficult challenges\" -- This is also an overstatement. Long-horizon tasks and high-dimensional observations are also very difficult. Also, the sentence is not grammatically correct.\n- \"That is, environment\" -> \"That is, the environment\"\n- In the last paragraph of the intro, it would be helpful to more clearly state what the experiments can accomplish. Can they handle raw pixel inputs?\n- \"diverse domains\" -> \"diverse simulated domains\"\n- \"a robotic grasping task\" -> \"a simulated robotic grasping task\"\n- There are a number of issues and errors in citations, e.g. missing the year, including the first name, incorrect reference\n- Assumption 1: \\mathcal{P} has not yet been defined.\n- The last two paragraphs of section 3.2 are very difficult to understand without reading the method yet\n- \"conventional RL solver tend\" -> \"conventional RL tend\", also should mention sparse reward in this sentence.\n- Algorithm 1 and Figure 1 are not referenced in the text anywhere, and should be\n- The text in Figure 1 and Figure 3 is extremely small\n- The text in Figure 3 is extremely small\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"TRL: Discriminative Hints for Scalable Reverse Curriculum Learning","abstract":"Deep reinforcement learning algorithms have proven successful in a vast variety of domains. However, tasks with sparse rewards remain challenging when the state space is large. Goal-oriented tasks are among the most typical problems in this domain, where a reward can only be received when the final goal is accomplished. In this work, we propose a potential solution to such problems with the introduction of an experience-based tendency reward mechanism, which provides the agent with additional hints based on a discriminative learning on past experiences during an automated reverse curriculum. This mechanism not only provides dense additional learning signals on what states lead to success, but also allows the agent to retain only this tendency reward instead of the whole histories of experience during multi-phase curriculum learning. We extensively study the advantages of our method on the standard sparse reward domains like Maze and Super Mario Bros and show that our method performs more efficiently and robustly than prior approaches in tasks with long time horizons and large state space. In addition, we demonstrate that using an optional checkpoint scheme with very small quantity of key states, our approach can solve difficult robot manipulation challenges directly from perception and sparse rewards.","pdf":"/pdf/69e784dc2c3766f5e8a1fcfea501eb845724b46c.pdf","TL;DR":"We propose Tendency RL to efficiently solve goal-oriented tasks with large state space using automated curriculum learning and discriminative shaping reward, which has the potential to tackle robot manipulation tasks with perception.","paperhash":"anonymous|trl_discriminative_hints_for_scalable_reverse_curriculum_learning","_bibtex":"@article{\n  anonymous2018trl:,\n  title={TRL: Discriminative Hints for Scalable Reverse Curriculum Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJssAZ-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper744/Authors"],"keywords":["deep learning","deep reinforcement learning","robotics","perception"]}},{"tddate":null,"ddate":null,"tmdate":1509739127911,"tcdate":1509133986621,"number":744,"cdate":1509739125250,"id":"rJssAZ-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rJssAZ-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"TRL: Discriminative Hints for Scalable Reverse Curriculum Learning","abstract":"Deep reinforcement learning algorithms have proven successful in a vast variety of domains. However, tasks with sparse rewards remain challenging when the state space is large. Goal-oriented tasks are among the most typical problems in this domain, where a reward can only be received when the final goal is accomplished. In this work, we propose a potential solution to such problems with the introduction of an experience-based tendency reward mechanism, which provides the agent with additional hints based on a discriminative learning on past experiences during an automated reverse curriculum. This mechanism not only provides dense additional learning signals on what states lead to success, but also allows the agent to retain only this tendency reward instead of the whole histories of experience during multi-phase curriculum learning. We extensively study the advantages of our method on the standard sparse reward domains like Maze and Super Mario Bros and show that our method performs more efficiently and robustly than prior approaches in tasks with long time horizons and large state space. In addition, we demonstrate that using an optional checkpoint scheme with very small quantity of key states, our approach can solve difficult robot manipulation challenges directly from perception and sparse rewards.","pdf":"/pdf/69e784dc2c3766f5e8a1fcfea501eb845724b46c.pdf","TL;DR":"We propose Tendency RL to efficiently solve goal-oriented tasks with large state space using automated curriculum learning and discriminative shaping reward, which has the potential to tackle robot manipulation tasks with perception.","paperhash":"anonymous|trl_discriminative_hints_for_scalable_reverse_curriculum_learning","_bibtex":"@article{\n  anonymous2018trl:,\n  title={TRL: Discriminative Hints for Scalable Reverse Curriculum Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rJssAZ-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper744/Authors"],"keywords":["deep learning","deep reinforcement learning","robotics","perception"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}