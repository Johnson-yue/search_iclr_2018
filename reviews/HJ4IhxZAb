{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222703550,"tcdate":1511922099058,"number":3,"cdate":1511922099058,"id":"rki3FqilM","invitation":"ICLR.cc/2018/Conference/-/Paper627/Official_Review","forum":"HJ4IhxZAb","replyto":"HJ4IhxZAb","signatures":["ICLR.cc/2018/Conference/Paper627/AnonReviewer1"],"readers":["everyone"],"content":{"title":"nice idea, needing more experiments","rating":"6: Marginally above acceptance threshold","review":"Overview\n\nThe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets. The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next. They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation. The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically. The experiments show the proposed approach is effective on 14 UCI datasets.\n\nstrength\n\n* The paper is mostly clear and easy to follow.\n* The overall idea is interesting and has many potentials.\n* The experimental results are promising on multiple datasets.\n* There are thorough discussion with related works.\n\nweakness\n\n* The graph in p.3 don't show the architecture of the network clearly.\n* The motivation of using feature histograms as embedding is not clear.\n* The description of the 2-D histogram on p.4 is not clear. The term \"posterior value\" sounds ambiguous.\n* The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners. Also, the experiments takes a fixed 20K iterations for training, and the convergence status (e.g. whether the accumulated gradient has stabilized the policy) is not clear.\n* Are there particular reasons in using policy learning instead of other reinforcement learning approaches?\n* The term A(Z) in the objective function can be more clearly described.\n* While many loosely-related works were surveyed, it is not clear why literally none of them were compared. There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy. There is also no evidence on whether adaptive learning on the fly is needed or not.\n* In Equation 2, should there be a balancing parameter for the reconstruction loss?\n* Some typos\n    - page 4: some duplicate words in discriminative embedding session\n    - page 4: auxliary -> auxiliary\n    - page 7: tescting -> testing\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning","abstract":"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalize across diverse problems.","pdf":"/pdf/829af1cb96e8f3cd72a8ca4755731d93f2b6c19d.pdf","paperhash":"anonymous|metalearning_transferable_active_learning_policies_by_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4IhxZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper627/Authors"],"keywords":["Active Learning","Deep Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222703588,"tcdate":1511817655673,"number":2,"cdate":1511817655673,"id":"H1g6bb9gG","invitation":"ICLR.cc/2018/Conference/-/Paper627/Official_Review","forum":"HJ4IhxZAb","replyto":"HJ4IhxZAb","signatures":["ICLR.cc/2018/Conference/Paper627/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A novel meta-learning way to do active learning, slightly complicated embedding strategy, needs more evidence to show if it'll generalise to more challenging problems. ","rating":"5: Marginally below acceptance threshold","review":"The approach solves an important problem as getting labelled data is hard. The focus is on the key aspect, which is generalisation across heteregeneous data. The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.\n\nPros: \n1. The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on. \n2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets, which isnt very common in most meta-learning papers today, so it’s encouraging that the method works in that regime.\n\nCons: \n1. The embedding strategy, especially the representative and discriminative histograms, is complicated. It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images. More evidence in the paper for why it would work on harder problems would be great. \n2. The policy network would have to output a probability for each datapoint in the dataset U, which could be fairly large, thus the method is computationally much more expensive than random sampling. A section devoted to showing what practical problems could be potentially solved by this method would be useful.\n3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM, or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner.\n\nSignificance/Conclusion: The idea of meta-learning or learning to learn is fairly common now. While they do show good performance, it’s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks. \n\nComments: There’s lots of typos, please proof read to improve the paper.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning","abstract":"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalize across diverse problems.","pdf":"/pdf/829af1cb96e8f3cd72a8ca4755731d93f2b6c19d.pdf","paperhash":"anonymous|metalearning_transferable_active_learning_policies_by_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4IhxZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper627/Authors"],"keywords":["Active Learning","Deep Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222703627,"tcdate":1511634780106,"number":1,"cdate":1511634780106,"id":"SJEDvEvez","invitation":"ICLR.cc/2018/Conference/-/Paper627/Official_Review","forum":"HJ4IhxZAb","replyto":"HJ4IhxZAb","signatures":["ICLR.cc/2018/Conference/Paper627/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This is an exciting paper on an important topic (active learning), but it suffers from a weak empirical evaluation.","rating":"6: Marginally above acceptance threshold","review":"This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements:\n1) you should include in your comparison Query-by- Bagging & Boosting, which are two of the best out-of-the-box active learning strategies\n2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones, but many questions are still unanswered:\n -  would any 7-7 split work just as well (ie, cross-validate over the 14 domains)\n - do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains? are the results significantly different? \n\nOTHER COMMENTS:\n- p3: both images in Figure 1 are labeled Figure 1.a\n- p3: typo \"theis\" --> \"this\" \n\nAbe & Mamitsuksa (ICML-1998). Query Learning Strategies Using Boosting and Bagging.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning","abstract":"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalize across diverse problems.","pdf":"/pdf/829af1cb96e8f3cd72a8ca4755731d93f2b6c19d.pdf","paperhash":"anonymous|metalearning_transferable_active_learning_policies_by_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4IhxZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper627/Authors"],"keywords":["Active Learning","Deep Reinforcement Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739193620,"tcdate":1509129291961,"number":627,"cdate":1509739190936,"id":"HJ4IhxZAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJ4IhxZAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning","abstract":"Active learning (AL) aims to enable training high performance classifiers with low annotation cost by predicting which subset of unlabelled instances would be most beneficial to label. The importance of AL has motivated extensive research, proposing a wide variety of manually designed AL algorithms with diverse theoretical and intuitive motivations. In contrast to this body of research, we propose to treat active learning algorithm design as a meta-learning problem and learn the best criterion from data. We model an active learning algorithm as a deep neural network that inputs the base learner state and the unlabelled point set and predicts the best point to annotate next. Training this active query policy network with reinforcement learning, produces the best non-myopic policy for a given dataset. The key challenge in achieving a general solution to AL then becomes that of learner generalisation, particularly across heterogeneous datasets. We propose a multi-task dataset-embedding approach that allows dataset-agnostic active learners to be trained. Our evaluation shows that AL algorithms trained in this way can directly generalize across diverse problems.","pdf":"/pdf/829af1cb96e8f3cd72a8ca4755731d93f2b6c19d.pdf","paperhash":"anonymous|metalearning_transferable_active_learning_policies_by_deep_reinforcement_learning","_bibtex":"@article{\n  anonymous2018meta-learning,\n  title={Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ4IhxZAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper627/Authors"],"keywords":["Active Learning","Deep Reinforcement Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}