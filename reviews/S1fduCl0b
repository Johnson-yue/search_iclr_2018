{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222657807,"tcdate":1511891426624,"number":3,"cdate":1511891426624,"id":"rJokGmjgG","invitation":"ICLR.cc/2018/Conference/-/Paper457/Official_Review","forum":"S1fduCl0b","replyto":"S1fduCl0b","signatures":["ICLR.cc/2018/Conference/Paper457/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Adapted VAE training to streaming data setting. Experiment shows the learned model can generate reasonable-looking samples. Unsure about quantitative results.","rating":"4: Ok but not good enough - rejection","review":"The paper proposed a teacher-student framework and a modified objective function to adapt VAE training to streaming data setting. The qualitative experimental result shows that the learned model can generate reasonable-looking samples. I'm not sure about what conclusion to make from the numerical result, as the test negative ELBO actually increased after decreasing initially. Why did it increase?\n\nThe modified objective function is a little ad-hoc, and it's unclear how to relate the overall objective function to Bayesian posterior inference (what exactly is the posterior that the encoder tries to approximate?). There is a term in the objective function that is synthetic data specific. Does that imply that the objective function is different depending on if the data is synthetic or real? What is the motivation/justification of choosing KL(Q_student||Q_teacher) as regularisation instead of the other way around? Would that make a difference in the goodness of the learned model? If not, wouldn't KL(Q_teacher||Q_student) result reduction in the variance of gradients and therefore a better choice?\n\nDetails on the minimum number of real samples per interval for the model to be able to learn is also missing. Also, how many synthetic samples per real samples are needed? How is the update with respect to synthetic sample scheduled? Given infinite amount of streaming data with a fixed number of classes/underlying distributions and interval length, and sample the class of each interval (uniformly) randomly, will the model/algorithm converge? Is there a minimum number of real examples that the student learner needs to see before it can be turned into a teacher?\n\nOther question: How is the number of latent category J of the latent discrete distribution chosen?\n\nQuality: The numerical experiment doesn't really compare to any other streaming benchmark and is a little unsatisfying. Without a streaming benchmark or a realistic motivating example in which the proposed scheme makes a significant difference, it's difficult to judge the contribution of this work.\nClarity: The manuscript is reasonably well-written. (minor: Paragraph 2, section 5, 'in principle' instead of 'in principal')\nOriginality: Average. The student-teacher framework by itself isn't novel. The modifications to the objective function appears to be novel as far as I am aware, but it doesn't require much special insights.\nSignificance: Below average. I think it will be very helpful if the authors can include a realistic motivating example where lifelong unsupervised learning is critical, and demonstrate that the proposed scheme makes a difference in the example.\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Generative Modeling","abstract":"Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. We do so through a student-teacher architecture which allows us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary of everything seen till now. The regularizer has the additional benefit of reducing the effect of catastrophic interference that appears when we learn over streaming data. We demonstrate its efficacy on streaming distributions as well as its ability to learn a common latent representation across a complex transfer learning scenario.\n","pdf":"/pdf/c5f7953245584781dfac5b74e42db5a57c0494ed.pdf","TL;DR":"Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.","paperhash":"anonymous|lifelong_generative_modeling","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fduCl0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper457/Authors"],"keywords":["Lifelong","Generative Modeling","Variational Autoencoder","VAE","Catastrophic Interference"]}},{"tddate":null,"ddate":null,"tmdate":1512222657850,"tcdate":1511832192072,"number":2,"cdate":1511832192072,"id":"SJuF9Eqez","invitation":"ICLR.cc/2018/Conference/-/Paper457/Official_Review","forum":"S1fduCl0b","replyto":"S1fduCl0b","signatures":["ICLR.cc/2018/Conference/Paper457/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good initiative for using VAEs in a new framework. The work needs to be a bit more principled though.","rating":"4: Ok but not good enough - rejection","review":"- Second paragraph in Section 1: Nice motivation. I am not sure though whether the performed experiments are the most expressive for such motivation. For instance, is the experiment in Section 5.1 a common task in that sequential lifelong learning setting?\n\n- Section 4, which is the main technical section of the paper, is quite full of lengthy descriptions that are a bit equivocal. I reckon each claim really needs to be supported by a corresponding unequivocal mathermatical formulation.\n\n- An example of the last point can be found in Section 4.2: \"The synthetic samples need to be representative of all the previously observed distributions ...\": It will be much clearer how such samples are representative if a formulation follows, and that did not happen in Section 4.2.\n\n- \"1) Sampling the prior can select a point in the latent space that is in between two separate distributions ...\": I am not sure I got this drawback of using the standard form of VAEs. Could you please further elaborate on this?\n\n- \"we restrict the posterior representation of the student model to **be close to that of the teacher** for the previous distributions** accumulated by the teacher. This allows the model parameters to **vary as necessary** in order to best fit the data\": What if the previous distributions are not that close to the new one?\n\n- Distribution intervals: Will it be the case in reality that these intervals will be given? Otherwise, what are the solutions to that? Can they be estimated somehow (as a future work)?\n\n\nMinor:\n- \"we observe a sample X of K\": sample X of size K, I guess?\n- \"... form nor an efficient estimator Kingma (2017)\": citation style.\n- \"we illustrates ...\"","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Generative Modeling","abstract":"Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. We do so through a student-teacher architecture which allows us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary of everything seen till now. The regularizer has the additional benefit of reducing the effect of catastrophic interference that appears when we learn over streaming data. We demonstrate its efficacy on streaming distributions as well as its ability to learn a common latent representation across a complex transfer learning scenario.\n","pdf":"/pdf/c5f7953245584781dfac5b74e42db5a57c0494ed.pdf","TL;DR":"Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.","paperhash":"anonymous|lifelong_generative_modeling","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fduCl0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper457/Authors"],"keywords":["Lifelong","Generative Modeling","Variational Autoencoder","VAE","Catastrophic Interference"]}},{"tddate":null,"ddate":null,"tmdate":1512222657888,"tcdate":1511017519582,"number":1,"cdate":1511017519582,"id":"rJuN3apyf","invitation":"ICLR.cc/2018/Conference/-/Paper457/Official_Review","forum":"S1fduCl0b","replyto":"S1fduCl0b","signatures":["ICLR.cc/2018/Conference/Paper457/AnonReviewer1"],"readers":["everyone"],"content":{"title":"At last, a deep generative model addressing a fresh problem","rating":"9: Top 15% of accepted papers, strong accept","review":"We have seen numerous variants of variational autoencoders, most of them introducing delta changes to the original architecture to address the same sort of modeling problems. This paper attacks a different kind of problem, namely lifelong learning. This key aspect of the paper, besides the fact that it constitutes a very important problem, does also addes a strong element of freshness to the paper.\n\nThe construction of the generative model is correct, and commensurate with standard practice in the field of deep generative models. The derivations are correct, while the experimental evaluation is diverse and convincing. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Lifelong Generative Modeling","abstract":"Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. We do so through a student-teacher architecture which allows us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary of everything seen till now. The regularizer has the additional benefit of reducing the effect of catastrophic interference that appears when we learn over streaming data. We demonstrate its efficacy on streaming distributions as well as its ability to learn a common latent representation across a complex transfer learning scenario.\n","pdf":"/pdf/c5f7953245584781dfac5b74e42db5a57c0494ed.pdf","TL;DR":"Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.","paperhash":"anonymous|lifelong_generative_modeling","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fduCl0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper457/Authors"],"keywords":["Lifelong","Generative Modeling","Variational Autoencoder","VAE","Catastrophic Interference"]}},{"tddate":null,"ddate":null,"tmdate":1509739294178,"tcdate":1509120105676,"number":457,"cdate":1509739291518,"id":"S1fduCl0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1fduCl0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Lifelong Generative Modeling","abstract":"Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner where knowledge gained from previous tasks is retained and used for future learning. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to generative modeling where we continuously incorporate newly observed streaming distributions into our learnt model. We do so through a student-teacher architecture which allows us to learn and preserve all the distributions seen so far without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, the student model leverages the information learnt by the teacher, which acts as a summary of everything seen till now. The regularizer has the additional benefit of reducing the effect of catastrophic interference that appears when we learn over streaming data. We demonstrate its efficacy on streaming distributions as well as its ability to learn a common latent representation across a complex transfer learning scenario.\n","pdf":"/pdf/c5f7953245584781dfac5b74e42db5a57c0494ed.pdf","TL;DR":"Lifelong distributional learning through a student-teacher architecture coupled with a cross model posterior regularizer.","paperhash":"anonymous|lifelong_generative_modeling","_bibtex":"@article{\n  anonymous2018lifelong,\n  title={Lifelong Generative Modeling},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1fduCl0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper457/Authors"],"keywords":["Lifelong","Generative Modeling","Variational Autoencoder","VAE","Catastrophic Interference"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}