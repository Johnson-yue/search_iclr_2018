{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222774648,"tcdate":1511887414981,"number":3,"cdate":1511887414981,"id":"BkJBMfslf","invitation":"ICLR.cc/2018/Conference/-/Paper811/Official_Review","forum":"H1a37GWCZ","replyto":"H1a37GWCZ","signatures":["ICLR.cc/2018/Conference/Paper811/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A good practical extension of SkipThought, but fairly straightforward","rating":"5: Marginally below acceptance threshold","review":"This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence. Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related \"sentences\" could be encoded, including document title, section title, footnotes, hyperlinked sentences. This is a valid good idea and indeed improves results. The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful. The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution.\n\nOn the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text. There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable. The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference. And the coreference experiment isn't that clearly described nor necessarily that meaningful. Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting.\n\nOther comments:\n - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a \"topic sentence\" at the beginning.\n - p.5. Notation seemed a bit non-standard. I thought most people use \\sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid....\n - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common.\n - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them. A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work. The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015). More generally, a now quite common way to handle this problem is to use \"pointing\" or \"copying\", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too. \n - p.7. Why such an old Wikipedia dump? Most people use a more recent one!\n - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model.\n - p.8. For various reasons, the coreference results seem less useful than they could have been, but they do show some value for the technique in the area of domain-specific coreference.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT","abstract":"We present a new unsupervised method for learning general-purpose sentence embeddings.\nUnlike existing methods which rely on local contexts, such as words\ninside the sentence or immediately neighboring sentences, our method selects, for\neach target sentence, influential sentences in the entire document based on a document\nstructure. We identify a dependency structure of sentences using metadata\nor text styles. Furthermore, we propose a novel out-of-vocabulary word handling\ntechnique to model many domain-specific terms, which were mostly discarded by\nexisting sentence embedding methods. We validate our model on several tasks\nshowing 30% precision improvement in coreference resolution in a technical domain,\nand 7.5% accuracy increase in paraphrase detection compared to baselines.","pdf":"/pdf/2b39308996c208a5210758e46f15aae17b20aa6b.pdf","TL;DR":"To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words.","paperhash":"anonymous|unsupervised_sentence_embedding_using_document_structurebased_context","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1a37GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper811/Authors"],"keywords":["distributed representation","sentence embedding","structure","technical documents","sentence embedding","out-of-vocabulary"]}},{"tddate":null,"ddate":null,"tmdate":1512222774695,"tcdate":1511823481553,"number":2,"cdate":1511823481553,"id":"HkMKdz9gz","invitation":"ICLR.cc/2018/Conference/-/Paper811/Official_Review","forum":"H1a37GWCZ","replyto":"H1a37GWCZ","signatures":["ICLR.cc/2018/Conference/Paper811/AnonReviewer1"],"readers":["everyone"],"content":{"title":"borderline","rating":"5: Marginally below acceptance threshold","review":"1) This paper proposes a method for learning the sentence representations with sentences dependencies information. It is more like a dependency-based version skip-thought on the sentence level. The idea is interesting to me, but I think this paper still needs some improvements. The introduction and related work part are clear with strong motivations to me. But section 4 and 6 need a lot of details. \n\n2) My comments are as follows:\ni) this paper claims that this is a general sentence embedding method, however, from what has been described in section 3, I think this dependency is only defined in HTML format document. What if I only have pure text document without these HTML structure information? So I suggest the authors do not claim that this method is a \"general-purpose\" sentence embedding model.\n\nii) The authors do not any descriptions of figure 3.  The equation 1 is also very confusing.\n\niii) the experiments are insufficient without details. How is the loss calculated against with? How is the detection accuracy calculated?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT","abstract":"We present a new unsupervised method for learning general-purpose sentence embeddings.\nUnlike existing methods which rely on local contexts, such as words\ninside the sentence or immediately neighboring sentences, our method selects, for\neach target sentence, influential sentences in the entire document based on a document\nstructure. We identify a dependency structure of sentences using metadata\nor text styles. Furthermore, we propose a novel out-of-vocabulary word handling\ntechnique to model many domain-specific terms, which were mostly discarded by\nexisting sentence embedding methods. We validate our model on several tasks\nshowing 30% precision improvement in coreference resolution in a technical domain,\nand 7.5% accuracy increase in paraphrase detection compared to baselines.","pdf":"/pdf/2b39308996c208a5210758e46f15aae17b20aa6b.pdf","TL;DR":"To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words.","paperhash":"anonymous|unsupervised_sentence_embedding_using_document_structurebased_context","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1a37GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper811/Authors"],"keywords":["distributed representation","sentence embedding","structure","technical documents","sentence embedding","out-of-vocabulary"]}},{"tddate":null,"ddate":null,"tmdate":1512222774738,"tcdate":1511727005584,"number":1,"cdate":1511727005584,"id":"HJUoksOxG","invitation":"ICLR.cc/2018/Conference/-/Paper811/Official_Review","forum":"H1a37GWCZ","replyto":"H1a37GWCZ","signatures":["ICLR.cc/2018/Conference/Paper811/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper on extracting more signal from document structure","rating":"7: Good paper, accept","review":"This paper presents simple but useful ideas for improving sentence embedding by drawing from more context. The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other \"governing\" sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc. The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context.\n\nHere are the pros of this paper:\n1) Useful contribution in terms of using broader context for embedding a sentence.\n2) Novel and simple \"trick\" for generating OOV words by mapping them to \"local\" variables and generating those variables.\n3) Outperforms SkipThought in evals.\n\nCons:\n1) Coreference eval: No details are provided for how the data was annotated for the coreference task. This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference. Also, the authors should make this dataset available for replicability. Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes. Please clarify.\n2) It is not clear to me how the model learns to generate specific OOV variables. Can the authors clarify how does the decoder learns to generate these words.\n\nClarifications:\n1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper?\n2) What is the exact heuristic in \"Text Styles\" in section 3.1? Should be stated for replicability.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT","abstract":"We present a new unsupervised method for learning general-purpose sentence embeddings.\nUnlike existing methods which rely on local contexts, such as words\ninside the sentence or immediately neighboring sentences, our method selects, for\neach target sentence, influential sentences in the entire document based on a document\nstructure. We identify a dependency structure of sentences using metadata\nor text styles. Furthermore, we propose a novel out-of-vocabulary word handling\ntechnique to model many domain-specific terms, which were mostly discarded by\nexisting sentence embedding methods. We validate our model on several tasks\nshowing 30% precision improvement in coreference resolution in a technical domain,\nand 7.5% accuracy increase in paraphrase detection compared to baselines.","pdf":"/pdf/2b39308996c208a5210758e46f15aae17b20aa6b.pdf","TL;DR":"To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words.","paperhash":"anonymous|unsupervised_sentence_embedding_using_document_structurebased_context","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1a37GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper811/Authors"],"keywords":["distributed representation","sentence embedding","structure","technical documents","sentence embedding","out-of-vocabulary"]}},{"tddate":null,"ddate":null,"tmdate":1509739088462,"tcdate":1509135284629,"number":811,"cdate":1509739085782,"id":"H1a37GWCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1a37GWCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT","abstract":"We present a new unsupervised method for learning general-purpose sentence embeddings.\nUnlike existing methods which rely on local contexts, such as words\ninside the sentence or immediately neighboring sentences, our method selects, for\neach target sentence, influential sentences in the entire document based on a document\nstructure. We identify a dependency structure of sentences using metadata\nor text styles. Furthermore, we propose a novel out-of-vocabulary word handling\ntechnique to model many domain-specific terms, which were mostly discarded by\nexisting sentence embedding methods. We validate our model on several tasks\nshowing 30% precision improvement in coreference resolution in a technical domain,\nand 7.5% accuracy increase in paraphrase detection compared to baselines.","pdf":"/pdf/2b39308996c208a5210758e46f15aae17b20aa6b.pdf","TL;DR":"To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words.","paperhash":"anonymous|unsupervised_sentence_embedding_using_document_structurebased_context","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1a37GWCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper811/Authors"],"keywords":["distributed representation","sentence embedding","structure","technical documents","sentence embedding","out-of-vocabulary"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}