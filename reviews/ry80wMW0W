{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222798054,"tcdate":1512016964581,"number":1,"cdate":1512016964581,"id":"H16Hn-6lf","invitation":"ICLR.cc/2018/Conference/-/Paper865/Official_Review","forum":"ry80wMW0W","replyto":"ry80wMW0W","signatures":["ICLR.cc/2018/Conference/Paper865/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Novel model to discover subtasks in probabilistic planning (LMDP).","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a formulation for discovering subtasks in Linearly-solvable MDPs. The idea is to decompose the optimal value function into a fixed set of sub value functions (each corresponding to a subtask) in a way that they best approximate (e.g. in a KL-divergence sense) the original value.\n\nAutomatically discovering hierarchies in planning/RL problems is an important problem that may provide important benefits especially in multi-task environments. In that sense, this paper makes a reasonable contribution to that goal for multitask LMDPs. The simulations also show that the discovered hierarchy can be interpreted. Although the contribution is a methodological one, from an empirical standpoint, it may be interesting to provide further evidence of the benefits of the proposed approach. Overall, it would also be useful to provide a short paragraph about similarities to the literature on discovering hierarchies in MDPs.  \n\nA few other comments and questions: \n\n- This may be a fairly naive question but given your text I'm under the impression that the goal in LMDPs is to find z(s) for all states (and Z in the multitask formulation). Then, your formulation for discovery subtasks seems to assume that Z is given. Does that mean that the LMDPs must first be solved and only then can subtasks be discovered? (The first sentence in the introduction seems to imply that there's hope of faster learning by doing hierarchical decomposition).\n\n- You motivate your approach (Section 3) using a max-variance criterion (as in PCA), yet your formulation actually uses the KL-divergence. Are these equivalent objectives in this case?\n\n\nOther (minor) comments: \n\n- In Section it would be good to define V(s) as well as 'i' in q_i (it's easy to mistake it for an index). ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hierarchical Subtask Discovery with Non-Negative Matrix Factorization","abstract":"Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions.","pdf":"/pdf/ae92b2fb4226151d0689cd7fcc959a61202bf663.pdf","TL;DR":"We present a novel algorithm for hierarchical subtask discovery which leverages the multitask linear Markov decision process framework.","paperhash":"anonymous|hierarchical_subtask_discovery_with_nonnegative_matrix_factorization","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Subtask Discovery with Non-Negative Matrix Factorization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry80wMW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper865/Authors"],"keywords":["Reinforcement Learning","Hierarchy","Subtask Discovery","Linear Markov Decision Process"]}},{"tddate":null,"ddate":null,"tmdate":1509739061135,"tcdate":1509136334181,"number":865,"cdate":1509739058467,"id":"ry80wMW0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry80wMW0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hierarchical Subtask Discovery with Non-Negative Matrix Factorization","abstract":"Hierarchical reinforcement learning methods offer a powerful means of planning flexible behavior in complicated domains. However, learning an appropriate hierarchical decomposition of a domain into subtasks remains a substantial challenge. We present a novel algorithm for subtask discovery, based on the recently introduced multitask linearly-solvable Markov decision process (MLMDP) framework. The MLMDP can perform never-before-seen tasks by representing them as a linear combination of a previously learned basis set of tasks. In this setting, the subtask discovery problem can naturally be posed as finding an optimal low-rank approximation of the set of tasks the agent will face in a domain. We use non-negative matrix factorization to discover this minimal basis set of tasks, and show that the technique learns intuitive decompositions in a variety of domains. Our method has several qualitatively desirable features: it is not limited to learning subtasks with single goal states, instead learning distributed patterns of preferred states; it learns qualitatively different hierarchical decompositions in the same domain depending on the ensemble of tasks the agent will face; and it may be straightforwardly iterated to obtain deeper hierarchical decompositions.","pdf":"/pdf/ae92b2fb4226151d0689cd7fcc959a61202bf663.pdf","TL;DR":"We present a novel algorithm for hierarchical subtask discovery which leverages the multitask linear Markov decision process framework.","paperhash":"anonymous|hierarchical_subtask_discovery_with_nonnegative_matrix_factorization","_bibtex":"@article{\n  anonymous2018hierarchical,\n  title={Hierarchical Subtask Discovery with Non-Negative Matrix Factorization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry80wMW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper865/Authors"],"keywords":["Reinforcement Learning","Hierarchy","Subtask Discovery","Linear Markov Decision Process"]},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}