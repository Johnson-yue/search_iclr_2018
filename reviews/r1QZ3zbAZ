{"notes":[{"tddate":null,"ddate":null,"tmdate":1512160482619,"tcdate":1512160439068,"number":2,"cdate":1512160439068,"id":"SkkTnVJbz","invitation":"ICLR.cc/2018/Conference/-/Paper954/Public_Comment","forum":"r1QZ3zbAZ","replyto":"r1QZ3zbAZ","signatures":["~Ryan_Knight1"],"readers":["everyone"],"writers":["~Ryan_Knight1"],"content":{"title":"Reproducibility Challenge ","comment":"As part of the reproducibility challenge, our team of students would like to attempt to reproduce the results of your paper.\nIf possible, it would be incredibly helpful if you could provide parts of the code used in your creation of the adversarial examples.\n\nAlso could you confirm if these are the datasets used for the paper:\nTrec07p: https://plg.uwaterloo.ca/~gvcormac/treccorpus07/\nYelp: https://www.yelp.com/dataset\nNews: https://github.com/GeorgeMcIntire/fake_real_news_dataset/\n\nAnd if this is the pre-trained word vector model that was used for word replacement in the LSTM method:\nwor2vec: https://code.google.com/archive/p/word2vec/\nwhile this pre-trained model was used for the other 3 methods:\nGloVe: https://nlp.stanford.edu/projects/glove/\n\nThank you \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222827534,"tcdate":1512153366151,"number":3,"cdate":1512153366151,"id":"B1CfWm1WG","invitation":"ICLR.cc/2018/Conference/-/Paper954/Official_Review","forum":"r1QZ3zbAZ","replyto":"r1QZ3zbAZ","signatures":["ICLR.cc/2018/Conference/Paper954/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Ok paper, but needs some revision","rating":"4: Ok but not good enough - rejection","review":"The paper shows that neural networks are sensitive to adversarial perturbation for a set of NLP text classifications. They propose constructing (model-dependent) adversarial examples by optimizing a function J (that doesn't seem defined in the paper) subject to a constraint c(x, x') < \\gamma (i.e. that the original input and adversarial input should be similar)\n\nc is composed of two constraints:\n1. || v - v' ||_2 < \\gamma_1, where v and v' are bag of embeddings for each input \n2. |log P(x') - log P(x)| < \\gamma_2 where P is a language model\n\nThe authors then show that for 3 classification problems \"Trec07p\", \"Yelp\", and \"News\" and 4 models (Naive Bayes, LSTM, word CNNs, deep-char-CNNs) that the models that perform considerably worse on adversarial examples than on the test set.  Furthermore to test the validity of their adversarial examples, the authors show the following:\n1. Humans achieve somewhat similar accuracy on the original adverarial examples (8 points higher on one dataset and 8 points lower on the other two)\n2. Humans rate the writing quality of both the original and adversarial examples to be similar\n3. The adversarial examples only somewhat transfer across models\n\nMy main questions/complaints/suggestions for the paper are:\n\n-Novelty/Methodology. The paper has mediocre novelty given other similar papers recently. \n\nOn question I have is about whether the generated examples are actually close to the original examples. The authors do show some examples that do look good, but do not provide any systematic study (e.g. via human annotation)\n\n This is a key challenge in NLP (as opposed to vision where the inputs are continuous so it is easy to perturb them and be reasonably sure that the image hasn't changed much). In NLP however, the words are discrete, and the authors measure the difference between an original example and the adversary only in continuous space which may not actually be a good measure of how different they are.\n\nThey do have some constraint that the fraction of changed words cannot differ by more than delta, but delta = 0.5 in the experiments, which is really large! (i.e. 50% of the words could be different according to Algorithm 1)\n\n-Writing: the function J is never mathematically defined, neither is the function c (except that it is known to be composed of the semantic/syntactic similarity constraints).\n\nThe authors talk about \"syntactic\" similarity but then propose a language model constraint. I think is a better word is \"fluency\" constraint. \n\nThe results in Table 3 and Table 6 seem different, shouldn't the diagonal of Table 6 line up with the results in Table 3?\n\n-Experimental methodology (more of a question since authors are unclear): The authors write that \"all adversarial examples are generated and evaluated on the test set\".\n\nThere are many hyperparameters in the proposed authors' approach, are these also tuned on the test set? That is unfair to the base classifier. The adversarial model should be tuned on the validation set, and then the same model should be used to generate test set examples. (The authors can even show the validation adversarial accuracy to show how/if it deviates from the test accuracy)\n\n-Lack of related work in NLP (see the anonymous comment for some examples). Even the related work in NLP that is cited e.g. Jia and Liang 2017 is obfuscated in the last page. The authors' introduction only refers to related works in vision/speech and ignores related NLP work.\n\nFurthermore, adversarial perturbation is related to domain transfer  (since both involve shifts between the training and test distribution) and it is well known for instance that models that are trained on Wall Street Journal perform poorly on other domains.  See SJ Pan and Q Yang, A Survey on transfer learning, 2010, for some example references.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222827580,"tcdate":1511834791496,"number":2,"cdate":1511834791496,"id":"Byk2NS9xf","invitation":"ICLR.cc/2018/Conference/-/Paper954/Official_Review","forum":"r1QZ3zbAZ","replyto":"r1QZ3zbAZ","signatures":["ICLR.cc/2018/Conference/Paper954/AnonReviewer1"],"readers":["everyone"],"content":{"title":"No comparison to existing work","rating":"3: Clear rejection","review":"This paper proposes a method to generate adversarial examples for text classification problems. They do this by iteratively replacing words in a sentence with words that are close in its embedding space and which cause a change in the predicted class of the text. To preserve correct grammar, they only change words that don't significantly change the probability of the sentence under a language model.\n\nThe approach seems incremental and very similar to existing work such as Papernot et. al. The paper also states in the discussion in section 5.1 that they generate adversarial examples in state-of-the-art models, however, they ignore some state of the art models entirely such as Miyato et. al.\n\nThe experiments are solely missing comparisons to existing text adversarial generation approaches such as Papernot et. al and a comparison to adversarial training for text classification in Miyato et. al which might already mitigate this attack. The experimental section also fails to describe what kind of language model is used, (what kind of trigram LM is used? A traditional (non-neural) LM? Does it use backoff?).\n\nFinally, algorithm 1 does not seem to enforce the semantic constraints in Eq. 4 despite it being mentioned in the text. This can be seen in section 4.5 where the algorithm is described as choosing words that were far in word vector space. The last sentence in section 6 is also unfounded.\n\n\nNicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z.Berkay Celik, and Ananthram Swami\nPractical Black-Box Attacks against Machine Learning.\nProceedings of the 2017 ACM Asia Conference on Computer and Communications Security\n\nTakeru Miyato, Andrew M. Dai and Ian Goodfellow\nAdversarial Training Methods for Semi-Supervised Text Classification.\nInternational Conference on Learning Representation (ICLR), 2017\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222827623,"tcdate":1510725697312,"number":1,"cdate":1510725697312,"id":"HJKBdUKJf","invitation":"ICLR.cc/2018/Conference/-/Paper954/Official_Review","forum":"r1QZ3zbAZ","replyto":"r1QZ3zbAZ","signatures":["ICLR.cc/2018/Conference/Paper954/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Nice overview of adversarial techniques in natural language classification","rating":"6: Marginally above acceptance threshold","review":"Nice overview of adversarial techniques in natural language classification. The paper introduces the problem of adversarial perturbations, how they are constructed and demonstrate what effect they can have on a machine learning models. \n\nThe authors study several real-world adversarial examples, such as spam filtering, sentiment analysis and fake news and use these examples to test several popular classification models in context of adversarial perturbations. \n\nTheir results demonstrate the existence of adversarial perturbations in NLP and show that several different types of errors occur (syntactic, semantic, and factual). Studying each of these errors type can help defend and improve the classification algorithms via adversarial training.\n\nPros: Good analysis on real-world examples\nCons: I was expecting more actual solutions in addition to analysis","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509919322825,"tcdate":1509919322825,"number":1,"cdate":1509919322825,"id":"rJ7D9bpC-","invitation":"ICLR.cc/2018/Conference/-/Paper954/Public_Comment","forum":"r1QZ3zbAZ","replyto":"r1QZ3zbAZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Lack of Related Works on NLP","comment":"Interesting paper; however, it fails to cite related NLP papers. There is a vast amount of research on related topics, such as evading spam filters. Aside from that, adversarial examples for language has been also studied before. The followings are some related papers:\n\nhttp://www.aclweb.org/anthology/W16-5603\nhttps://arxiv.org/pdf/1702.08138.pdf\nhttps://arxiv.org/pdf/1707.02812.pdf"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092383614,"tcdate":1509137406018,"number":954,"cdate":1510092361949,"id":"r1QZ3zbAZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1QZ3zbAZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Adversarial Examples for Natural Language Classification Problems","abstract":"Modern machine learning algorithms are often susceptible to adversarial examples — maliciously crafted inputs that are undetectable by humans but that fool the algorithm into producing undesirable behavior. In this work, we show that adversarial examples exist in natural language classification: we formalize the notion of an adversarial example in this setting and describe algorithms that construct such examples. Adversarial perturbations can be crafted for a wide range of tasks — including spam filtering, fake news detection, and sentiment analysis — and affect different models — convolutional and recurrent neural networks as well as linear classifiers to a lesser degree. Constructing an adversarial example involves replacing 10-30% of words in a sentence with synonyms that don’t change its meaning. Up to 90% of input examples admit adversarial perturbations; furthermore, these perturbations retain a degree of transferability across models. Our findings demonstrate the existence of vulnerabilities in machine learning systems and hint at limitations in our understanding of classification algorithms.","pdf":"/pdf/af7bad9ab688e80ec5aa6670c811c585d26d24dd.pdf","paperhash":"anonymous|adversarial_examples_for_natural_language_classification_problems","_bibtex":"@article{\n  anonymous2018adversarial,\n  title={Adversarial Examples for Natural Language Classification Problems},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1QZ3zbAZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper954/Authors"],"keywords":[]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}