{"notes":[{"tddate":null,"ddate":null,"tmdate":1511935289107,"tcdate":1511935289107,"number":8,"cdate":1511935289107,"id":"r1-ST6olG","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SJp9ze61G","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: Re: The \"huge drop\"","comment":"This work considers ImageNet which is a far more challenging dataset. After reading the other submission (which is quite interesting too), I humbly think that this work is the most sensible ICLR submission on defending against adversarial attacks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222706912,"tcdate":1511832698264,"number":3,"cdate":1511832698264,"id":"SJzYnEqef","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":" The paper investigates using input transformation techniques as a defence against adversarial examples. The authors evaluate a number of simple defences that are based on input transformations such TV minimization and image quilting and compare it against previously proposed ideas of JPEG compression and decompression and random crops.  The authors have evaluated their defences against four main kinds of adversarial attacks.\n\nThe main takeaways of the paper are to incorporate transformations that are non-differentiable and randomised. Both TV minimisation and image quilting have that property and show good performance in withstanding adversarial attacks in various settings. \n\nOne argument that I am not sure would be applicable perhaps and could be used by adversarial attacks is as follows: If the defence uses image quilting for instance and obtains an image $P$ that approximates the original observation $X$, it could be possible to use a model based approach that obtains an observation $Q$ that is close to $P$ which can be attacked using adversarial attacks. Would this observation then be vulnerable to such attacks? This could perhaps be explored in future.\n\nThe paper provides useful contributions in forming model agnostic defences that could be further investigated. The authors show that the simple input transformations advocated work against the major kind of attacks. The input transformations of TV minimization and image quilting share varying characteristics in terms of being sensitive to various kinds of attacks and therefore can be combined. The evaluation is carried out on ImageNet dataset with large number of examples.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222706953,"tcdate":1511774491624,"number":2,"cdate":1511774491624,"id":"Sk47YIYlM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Well argumented, solid work","rating":"8: Top 50% of accepted papers, clear accept","review":"Summary: This works proposes strategies to make neural networks less sensitive to adversarial attacks. They consist into applying different transformations to the images, such as quantization, JPEG compression, total variation minimization and image quilting. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images.\nExperiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).\n60% of attacks are countered in this last most difficult setting.\nThe previous best approach for this task consists in ensemble training and is attack specific. It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25%.  \n\nComments: The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results.\n \nThe discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice.\nMinor: the bibliography should be uniformed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222706993,"tcdate":1511441950604,"number":1,"cdate":1511441950604,"id":"S1wXIrVgM","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Review","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference/Paper636/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Valuable idea but immature contribution.","rating":"4: Ok but not good enough - rejection","review":"To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. The purpose of the transformation is to erase the high-frequency signals potentially embedded by an adversarial attack.\n\nStrong points:\n\n* To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). \n\n* The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work.\n\n* The proposed approach really helps in a black-box scenario (Figure 4). As explained below, the presented investigation is however insufficient to assess whether the proposed defense helps in a true white-box scenario. \n\n\nWeak points:\n\n* The black-box versus white-box terminology is not appropriate, and confusing. In general, black-box means that the adversary ignores everything from the decision process. Hence, in this case, the adversary does not know about the classification model, nor the defensive method, when used. This corresponds to Figure 3. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. Assimilating the parameters of the transform to a secret key is not correct because those parameters could be inferred by presenting many image samples to the transform and looking at the outcome of the transformation (which is supposed to be available in a 'white-box' paradigm) for those samples. \n\n* Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5.\n\n* The paper does not discuss the impact of the denfense strategy on the classification performance in absence of adversity.\n\n* The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without\nSecurity through Obscurity' at https://arxiv.org/abs/1612.01401. \n\n* In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge. Designing an attack in case of a non differentiable transformation is obviously not trivial since back-propagation can not be used. However, since the proposed transformation primarily aim at removing the high frequency pattern induced by the attack, one could for example design an attack that account for a (linear and differentiable) low-pass filter transformation. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation.\n\n* If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all?\n\n\nOverall, the works investigates an interesting idea, but lacks maturity to be accepted. Therefore, I would only recommend acceptation if room.\n\nMinor issues:\n\nTypo on p7: to change*s*\nClarify poor formulations:\n* p1: 'enforce model-specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization schemes'. \n* p1: 'too simple to remove adversarial perturbations from input images sufficiently'","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510961812999,"tcdate":1510961812999,"number":7,"cdate":1510961812999,"id":"SJp9ze61G","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"H1mJgiqJG","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: The \"huge drop\"","comment":"You might want to check out https://openreview.net/forum?id=S18Su--CW where the authors show that simple quantization (depth reduction) leads to a loss of accuracy on clean examples, but you can get around it by discretizing your input. Also simple quantization maintains a roughly linear response of the classifier to it's input (see Figure 1), which is hypothesized as the cause of non-robustness to adversarial attacks (Goodfellow, 2014). The attacks proposed are also not \"grey-box\" like in this paper, i.e. the attacker has knowledge of the transformations and can attack using \"discretized\" versions of various iterated algorithms. The defense of this paper is essentially defense by \"obfuscation\", under the assumption that the attacker does not know what \"obfuscation\" has happened."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510809563479,"tcdate":1510809563479,"number":6,"cdate":1510809563479,"id":"H1mJgiqJG","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"HyOUaNzkz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The \"huge drop\"","comment":"The authors' result is not surprising at all. It is actually one of the few works on the defense side which makes sense to me.\nPersonally, I have not seen any strong evidence suggesting that we can generally maintain a high accuracy on clean and adversarial samples simultaneously. We have to take any such idealistic results on ImageNet with a grain of salt. In most of these works, either the defense is super weak (i.e., easily attackable) or something is hidden under the carpet."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510414603838,"tcdate":1510414603838,"number":5,"cdate":1510414603838,"id":"ry4zt5Vyf","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"HyOUaNzkz","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Performance on clean examples","comment":"The transformations we studied, indeed, all have some hyper-parameter that controls how lossy the transformation is, and can be used to trade off clean accuracy and adversarial accuracy. These hyper-parameters are: crop ratio for the crop-rescale transform, pixel drop rate and regularization parameter for total variation minimization, and patch size for image quilting. For instance, using a larger patch size in image quilting will remove more of the adversarial perturbation (which likely leads to higher adversarial accuracy), but also affects clean images which deteriorates clean accuracy. \n\nWe selected hyper-parameter that achieve high adversarial accuracy, but one may choose to set them differently depending on the user's needs. We surmise it may be possible to achieve high clear and adversarial accuracy by ensembling predictions over multiple hyper-parameter settings, but further experimentation is needed to confirm this hypothesis."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510260215579,"tcdate":1510260048162,"number":5,"cdate":1510260048162,"id":"HyOUaNzkz","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Performance on clean examples","comment":"The proposed several transformation methods are quite effective in recognizing the adversarial examples. For most existing works, people are trying to increase the performance on adversarial examples while maintaining the accuracy of clean examples. However, in table 2, I found there is a huge drop in accuracy for all your methods on clean examples. \nI wonder can you solve this problem by tuning some hyper-parameters to balance the performance of no attack and with attack?"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430875,"tcdate":1509891866807,"number":4,"cdate":1509891866807,"id":"ByXm1shC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"ryuYj65C-","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: Three comments","comment":"1) We have performed experiments with our defenses against PGD; see our previous comment for the results of those experiments, which we will add to the paper. Our results do not suggest substantial differences in the effectiveness of our defenses between PGD and I-FGSM. \n\n2) Our proposed defenses are not intended to compete with adversarial-training-based defenses: the two defenses can be used together and are likely to be complementary. We chose ImageNet to conduct our experiment for the following two reasons:\n\n- The interest in adversarial examples mainly stems from the concern of use of computer vision models in real-world applications such as self-driving cars and image-classification services. In these settings, the input to the model has high resolution and diverse content; ImageNet more closely resembles this scenario than MNIST or CIFAR.\n\n- Defending a model that performs classification on ImageNet is inherently more difficult than defending a MNIST or CIFAR classification model, since the model must output very diverse class labels and the model's prediction is often uncertain. Moreover, the input dimensionality for ImageNet is much higher (~150000 compared to 768 for MNIST and 3072 for CIFAR-10), which gives the attacker much more maneuverability.\n\n3) We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples. In our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430922,"tcdate":1509891831321,"number":3,"cdate":1509891831321,"id":"HJkZJjhAb","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"Hk1TeF9RZ","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples.\n\nIn our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509772159993,"tcdate":1509772159993,"number":4,"cdate":1509772159993,"id":"ryuYj65C-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"HktFIuqC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Three comments","comment":"1) Projecting after every step would make a lot of difference if the number of iterations and step length is high, since typically gradient signal from outside the epsilon ball is not that useful in finding adversarial examples.\n\n2) I am wondering what kind of accuracy your defense would get on MNIST/CIFAR-10, so one could compare to vanilla adversarial training as in Madry et al. It is not clear at the moment whether it would improve on adversarial training with PGD.\n\n3) Since some of the transformations are non-differentiable you could also have settings where the attacker knows your transformations and attack your transformed inputs (rather than the original image) - as the other commenter says, what you study is the \"grey-box\" case and not truly the \"white-box\" case. I would expect that in the truly \"white-box case\", it is no more robust against such attacks than just adversarial training with PGD."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509753015381,"tcdate":1509753015381,"number":3,"cdate":1509753015381,"id":"Hk1TeF9RZ","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"H1dO8OqC-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"Thanks for these clarifications! It might make sense to introduce a new term to characterize the threat model here, as \"white-box\" typically refers to full information about the defense. I think \"grey-box\" has been used before although it isn't very evocative either."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092430965,"tcdate":1509750400591,"number":2,"cdate":1509750400591,"id":"HktFIuqC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"SJ67F79R-","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: PGD evaluation missing","comment":"For our experiments, we selected four attacks that we believe are representative of the large number of attacks that people have proposed. Since PGD is related to I-FGSM (the main difference between the two is the projection step after every iteration), we expect that our defenses will have similar performance against PGD. \n\nWe performed a small experiment with PGD to confirm this. We created attacks with an average L2-dissimilarity of 0.06, and find that the accuracy of our defenses against white-box I-FGSM/PGD attacks are: no defense 0%/0%, crop ensemble 44%/48%, TVM 29%/36%, and image quilting 35%/37%. These results suggest that the effectiveness of our defenses against PGD is similar to their effectiveness against I-FGSM. We will add these results to the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1510092431009,"tcdate":1509750384434,"number":1,"cdate":1509750384434,"id":"H1dO8OqC-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Official_Comment","forum":"SyJ7ClWCb","replyto":"BkDPS_tAb","signatures":["ICLR.cc/2018/Conference/Paper636/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper636/Authors"],"content":{"title":"Re: White-box attacks with knowledge of defense transformation","comment":"Thank you for your comment! We were unaware of [1] at the time of submission. We will include it as a citation. Based on our understanding, [2] applies Carlini-Wagner's attack against a target loss that averages the prediction over multiple fixed models with random network weight dropout rather than random pixel dropout, but the two techniques are certainly related.\n\nIn regards to attacking the transformation defenses, independent of [1], we have observed that it is possible to produce adversarial examples that are invariant to crop location and scale. By randomly selecting a crop of size 135x135 each iteration to compute the loss function for CW-L2, the resulting adversarial examples can reduce the accuracy of crop defense to 30% at an average L2-dissimilarity of 0.045. \n\nEnhancing the attack with random pixel dropping can, indeed, reduce the effectiveness of TV minimization significantly. Using a pixel dropout mask with drop probability 0.1 each iteration, CW-L2 can reduce the accuracy of TV minimization defense to 9% at an average L2-dissimilarity of 0.06. However, we do not have a good idea of how to backpropagate through the quilting transformation, as the construction is stochastic and non-differentiable in nature. Nevertheless, section 5.5 of our paper does show that it is possible to successfully attack the quilting defense in some cases even without knowledge of this transformation; we presume because the convolutional filters reveal some information about the patch database used.\n\nThe attacks we use for the white-box setting do not have knowledge of the defense mechanism used. \n\nWe will clarify these points in the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509730597194,"tcdate":1509730597194,"number":2,"cdate":1509730597194,"id":"SJ67F79R-","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"PGD evaluation missing","comment":"I am curious why the authors have not evaluated their defense methods against the PGD attack of Madry et al https://arxiv.org/abs/1706.06083, which is the strongest first order adversary possible."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509684574939,"tcdate":1509684574939,"number":1,"cdate":1509684574939,"id":"BkDPS_tAb","invitation":"ICLR.cc/2018/Conference/-/Paper636/Public_Comment","forum":"SyJ7ClWCb","replyto":"SyJ7ClWCb","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"White-box attacks with knowledge of defense transformation","comment":"Interesting paper! It wasn't clear to me whether you evaluated attacks (white-box or black-box) with knowledge of the defensive technique being used? For JPEG or TV-minimization for instance, this would mean back-propagating through the transformation step. Analogously, one could incorporate randomized procedures such as cropping, pixel dropout or quilting into the adversarial example generation procedure, either in a white-box setting or in a black-box setting over a locally trained model with a similar defense.\n\nSome prior works [1,2] seem to indicate that various types of transformations do actually remain vulnerable to adversaries with such tailored attacks. ([1] considered random crops and found that an attack tailored to a particular cropping mechanism was still effective. [2] looked at random pixel dropout and found that computing white-box or black-box attacks for multiple randomly chosen dropout masks generalized well to unseen random masks).\n\n[1] Foveation-based Mechanisms Alleviate Adversarial Examples, https://arxiv.org/abs/1511.06292\n[2] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods, https://arxiv.org/abs/1705.07263"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739188613,"tcdate":1509129751066,"number":636,"cdate":1509739185961,"id":"SyJ7ClWCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyJ7ClWCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Countering Adversarial Images using Input Transformations","abstract":"This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong white-box and 90% of strong black-box attacks by a variety of major attack methods.","pdf":"/pdf/5c42e75edbc924cd14a606265e707bfe832c5d3f.pdf","TL;DR":"We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.","paperhash":"anonymous|countering_adversarial_images_using_input_transformations","_bibtex":"@article{\n  anonymous2018countering,\n  title={Countering Adversarial Images using Input Transformations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyJ7ClWCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper636/Authors"],"keywords":["adversarial example","machine learning security","computer vision","image classification"]},"nonreaders":[],"replyCount":16,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}