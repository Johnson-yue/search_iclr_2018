{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222653742,"tcdate":1512162308278,"number":4,"cdate":1512162308278,"id":"Sy3Z4By-z","invitation":"ICLR.cc/2018/Conference/-/Paper441/Official_Review","forum":"Sy3XxCx0Z","replyto":"Sy3XxCx0Z","signatures":["ICLR.cc/2018/Conference/Paper441/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good paper, needs some more experiments.","rating":"7: Good paper, accept","review":"This work is interesting and fairly thorough. The ablation studies at the end of the paper are the most compelling part of the argument, more so than achieving SoTa. Having said that, since their studies on performance with a low dataset size are the most interesting part of the paper, I would have liked to see results on smaller datasets like RTE. Additionally, it would be useful to see results on MultiNLI which is more challenging and spans more domains; using world knowledge with MultiNLI would be a good test of their claims and methods. \nI'm also glad that the authors establish statistical significance! I would have liked to see some additional analysis on the kinds of sentences the KIM models succeeds at where their baseline ESIM fails. I think this would be a compelling addition.\n\nPros:-\n- Thorough experimentation with ablation studies; show success of method when using limited training data.\n- Establish statistical significance.\n- Acheive SoTa on SNLI.\n\nCons:-\n- Authors make the broad claim of world knowledge being helpful for textual entailment, and show usefulness in a limited datasize setting, but don't test their method on other datasets RTE (which has a lot less data). If this helps performance on RTE then this could be a technique for low resource settings.\n- No results for MultiNLI shown. MultiNLI has many more domains of text and may benefit more from world knowledge.\n- Authors don't provide a list of examples where KIM succeeds and ESIM fails.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1512222653785,"tcdate":1512147175299,"number":3,"cdate":1512147175299,"id":"SkkxKWJbM","invitation":"ICLR.cc/2018/Conference/-/Paper441/Official_Review","forum":"Sy3XxCx0Z","replyto":"Sy3XxCx0Z","signatures":["ICLR.cc/2018/Conference/Paper441/AnonReviewer4"],"readers":["everyone"],"content":{"title":"This paper marginally improves performance on SNLI using a limited set of features indicating WordNet relations. The result is nice but predictable and the methods are not obviously applicable to other external forms of information. This contribution is not sufficient for ICLR.","rating":"3: Clear rejection","review":"This paper adds WordNet word pair relations to an existing natural language inference model. Synonyms, antonyms, and non-synonymous sister terms in the ontology are represented using indicator features. Hyponymy and hypernymy are represented using path length features. These features are used to modify inter sentence attention, the final post-attention word representations, and the pooling operation used to aggregate the final sentence representations prior to inference. All of these three additions help, especially in the low data learning scenario. When all of the SNLI training data is used this approach adds 0.6% accuracy on the SNLI 3-way classification task. \n\nI think that the integration of structured knowledge representations into neural models is a great avenue of investigation. And I'm glad to see that WordNet helps. But very little was done to investigate different ways in which these data can be integrated. The authors mention work on knowledge base embeddings and there has been plenty of work on learning WordNet embeddings. An obvious avenue of exploration would compare the use of these to the use of the indicator features in this paper. Another avenue of exploration is the integration of more resources such as VerbNet, propbank, WikiData etc. An approach that works with all of these would be much more impressive as it would need to handle a much more diverse feature space than the 4 inter-dependent features introduced here.\n\nQuestions for authors:\n\nIs the WordNet hierarchy bounded at a depth of 8? If so please state this and if not, what is the motivation of your hypernymy and hyponymy features?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1512222653821,"tcdate":1511739424505,"number":2,"cdate":1511739424505,"id":"rkumgRdxM","invitation":"ICLR.cc/2018/Conference/-/Paper441/Official_Review","forum":"Sy3XxCx0Z","replyto":"Sy3XxCx0Z","signatures":["ICLR.cc/2018/Conference/Paper441/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Alignment is back in NN","rating":"5: Marginally below acceptance threshold","review":"This is a very interesting paper!\nWe are finally back to what has been already proven valid for NLI also know as RTE. External knowledge is important to reduce the amount of training material for NLI. When dataset were extremely smaller yet more complex, this fact has been already noticed and reported in many systems. Now, it is extremely important that it is has been started to be true also in NN-based models for NLI/RTE.\n\nHovever, the paper fails in describing the model with respect to the vast body of research in RTE. In fact, alignment is one of the basis for building RTE systems. Attention models are in fact extremely related to alignment and \"KNOWLEDGE-ENRICHED CO-ATTENTION\" is somehow a model that incorporates what has been already extensively used to align word pairs. \nHence, models such as those described in the book \"Recognizing textual entailment\" can be extremely useful in modeling the same features in NN-based models, for example, \"A Phrase-Based Alignment Model for Natural Language Inference\", EMNLP 2008, or \"Measuring the semantic similarity of texts\", 2005 or \"Learning Shallow Semantic Rules for Textual Entailment\", RANLP, 2007.\n\nThe final issue is the validity of the initial claim. Is it really the case that external knowledge is useful? It appears that external knowledge is useful only in the case of restricted data (see Figure 3). Hence, it is unclear whether this is useful for the overall set. One of the important question here is then if the knowledge of all the data is in fact replicating the knowledge of wordnet. If this is the case, this may be a major result. \n\nMinor issues\n=====\nThe paper would be easier to read if Fig. 1 were completed with all the mathematical symbols.\nFor example, where are a^c_i and b^c_i ? Are they in the attention box?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1512222653861,"tcdate":1511129066131,"number":1,"cdate":1511129066131,"id":"SkGeeKygf","invitation":"ICLR.cc/2018/Conference/-/Paper441/Official_Review","forum":"Sy3XxCx0Z","replyto":"Sy3XxCx0Z","signatures":["ICLR.cc/2018/Conference/Paper441/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction; modest positive results","rating":"6: Marginally above acceptance threshold","review":"This paper presents a method to use external lexical knowledge (word–word relations from WordNet) as an auxiliary input when solving the problem of textual entailment (aka NLI). The idea of accessing outside commonsense knowledge within an end-to-end trained model is one that I expect to be increasingly important in work on language understanding. This paper does not make that much progress on the problem in general—the methods here are quite specific to words and to NLI—and the proposed methods yields only yield large empirical gains in a reduced-data setting, but the paper serves as a well-executed proof of concept. In short, the paper is likely to be of low technical impact, but it's interesting and thorough enough that I lean slightly toward acceptance.\n\nMy only concern is on fair comparison: Numbers from this model are compared with numbers from the published ESIM model in several places (Table 3, Figure 2, etc.) as a way to provide evidence for the paper's core claim (that the added knowledge in the proposed model helps). This only constitutes clear evidence if the proposed model is identical to ESIM in all of its unimportant details—word representations, hyperparameter tuning methods, etc. Can the authors comment on this?\n\nFor what it's worth, the existence of another paper submission on roughly the same topic with roughly the same results (https://openreview.net/pdf?id=B1twdMCab) makes more confident that the main results in this paper are sound, since they've already been replicated, at least to a coarse approximation.\n\nMinor points:\n\nFor TransE, what does this mean:\"However, these kind of approaches usually need to train a knowledge-graph embedding beforehand.\"\n\nYou should say more about why you chose the constant 8 in Table 1 (both why you chose to hard code a value, and why that value).\n\nThere's a mysterious box above the text 'Figure 1'. Possibly a figure rendering error?\n\nThe LSTM equations are quite widely known. I'd encourage you to cite a relevant source and remove them.\n\nSay more about why you choose equation (9). This notably treats all five relation types equally, which seems like a somewhat extreme simplifying assumption.\n\nEquation (15) is confusing. Is a^m a matrix, since it doesn't have an index on it?\n\nWhat is \"early stopping with patience of 7\"? Is that meant to mean 7 epochs?\n\nThe opening paragraph of 5.1 seems entirely irrelevant, as do the associated results in the results table. I suspect that this might be an opportunity for a gratuitous self-citation.\n\nThere are plenty of typos: \"... to make it replicatibility purposes.\"; \"Specifically, we use WordNet to measure the semantic relatedness of the *word* in a pair\"; etc.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1510857573759,"tcdate":1510857573759,"number":2,"cdate":1510857573759,"id":"HJCviIs1G","invitation":"ICLR.cc/2018/Conference/-/Paper441/Public_Comment","forum":"Sy3XxCx0Z","replyto":"rJWNCT9kz","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Thanks!","comment":"It looks great. Thanks!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1510822100078,"tcdate":1510821417292,"number":1,"cdate":1510821417292,"id":"rJWNCT9kz","invitation":"ICLR.cc/2018/Conference/-/Paper441/Official_Comment","forum":"Sy3XxCx0Z","replyto":"SJ-i-Fy1z","signatures":["ICLR.cc/2018/Conference/Paper441/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper441/Authors"],"content":{"title":"MultiNLI results","comment":"We really appreciate the comments. We ran the proposed models on MultiNLI. With the same setting, on the \"in-domain\" test set, the proposed model, KIM (knowledge-based inference model), achieves an accuracy of 77.4% (vs. 77.0% of the ESIM model (Chen et al. ACL '17)). In addition, on the \"cross-domain\" test set, KIM achieves a 75.8% accuracy (vs. 75.5% of ESIM). We will add these results in our revision. Thanks for the constructive comments, which make the results more comprehensive.\n[References]\nQ. Chen, X. Zhu, Z. Ling, S. Wei, H. Jiang, and D. Inkpen. (2017). Enhanced LSTM for Natural Language Inference. In: Proc. of ACL, Vancouver, Canada."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1510080921106,"tcdate":1510080921106,"number":1,"cdate":1510080921106,"id":"SJ-i-Fy1z","invitation":"ICLR.cc/2018/Conference/-/Paper441/Public_Comment","forum":"Sy3XxCx0Z","replyto":"Sy3XxCx0Z","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Have you tried your method on MultiNLI dataset?","comment":"Good work. I'm wondering whether you have tried your model on MultiNLI corpus? The pipeline should be same. MultiNLI corpus requires higher level understanding of the text. With external knowledge, the performance on MultiNLI should be better than the systems without external knowledge."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]}},{"tddate":null,"ddate":null,"tmdate":1509739302309,"tcdate":1509117987944,"number":441,"cdate":1509739299643,"id":"Sy3XxCx0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy3XxCx0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Natural Language Inference with External Knowledge","abstract":"Modeling informal inference in natural language is very challenging. With the recent availability of large annotated data, it has become feasible to train complex models such as neural networks to perform natural language inference (NLI), which have achieved state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform NLI from the data? If not, how can NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we aim to answer these questions by enriching the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models with external knowledge further improve the state of the art on the Stanford Natural Language Inference (SNLI) dataset. ","pdf":"/pdf/87ccc2c9ede781d0333680f36c73c674be067f4b.pdf","TL;DR":"the proposed models with external knowledge further improve the state of the art on the SNLI dataset.","paperhash":"anonymous|natural_language_inference_with_external_knowledge","_bibtex":"@article{\n  anonymous2018natural,\n  title={Natural Language Inference with External Knowledge},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy3XxCx0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper441/Authors"],"keywords":["natural language inference","external knowledge","state of the art"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}