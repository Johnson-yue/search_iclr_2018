{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222633587,"tcdate":1511811166276,"number":2,"cdate":1511811166276,"id":"BkLPOycez","invitation":"ICLR.cc/2018/Conference/-/Paper388/Official_Review","forum":"HJPSN3gRW","replyto":"HJPSN3gRW","signatures":["ICLR.cc/2018/Conference/Paper388/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting Problem, but Limited Novelty and Flawed Evaluation","rating":"4: Ok but not good enough - rejection","review":"Interesting Problem, but Limited Novelty and Flawed Evaluation\n\n\nThe paper considers the problem of following natural language instructions given an first-person view of an a priori unknown environment. The paper proposes a neural architecture that employs an RNN to encode the language input and a CNN to encode the visual input. The two modalities are fused and fed to an RNN policy network. The method is evaluated on a new dataset consisting of short, simple instructions conveyed in simple environments.\n\nThe problem of following free-form navigation instructions is interesting and has achieved a fair bit of attention, previously with \"traditional\" structured approaches (rule-based and learned) and more recently with neural models. Unlike most existing work, this paper reasons over the raw visual input (vs., a pre-processed representation such as a bag-of-words model). HoA notable exception is the work of Chaplot et al. 2017, which addresses the same problem with a nearly identical architecture (see below). Overall, this paper constitutes a reasonable first-pass at this problem, but there is significant room for improvement to address issues related to the stated contributions and flawed evaluations.\n\nThe paper makes several claims regarding the novelty and expressiveness of the model and the contributions of the paper that are either invalid or not justified by the experimental results. As noted, a neural approach to instruction following is not new (see Mei et al. 2016) nor is a multimodal fusion architecture that incorporates raw images (see Chaplot et al.). The paper needs to make the contributions and novelty relative to existing methods clear (e.g., those stated in the intro are nearly identical to those of Mei et al. and Chaplot et al.). This includes discussion of the attention mechanism, for which the contributions and novelty are justified only by simple visualizations that are not very insightful. Related, the paper omits a large body of work in language understanding from the NLP and robotics domains, e.g., the work of Yoav Artzi, Thomas Howard, and Stefanie Tellex, among others (see below). While the approaches are different, it is important to describe this work in the context of these methods.\n\n\nThere are important shortcomings with the evaluation. First, one of the two scenarios involves testing on instructions from the training set. The test set should only include held-out environments and instructions, which the paper incorrectly refers to as the \"zero-shot\" scenario. This test set is very small, with only 19 instructions. Related, there is no mention of a validation set, and the discussion seems to suggest that hyperparameters were tuned on the test set. Further, the method is compared to incomplete implementations of existing baselines that admittedly don't attempt to replicate the baseline architectures. Consequently, it isn't clear what if anything can be concluded from the evaluation. There is a\n\n\n\nComments/Questions\n\n* The action space does not include an explicit stop action. Instead, a run is considered to be finished either when the agent reaches the destination or a timeout is exceeded. This is clearly not valid in practice. The model should determine when to stop, as with existing approaches.\n\n* The paper makes strong claims regarding the sophistication of the dataset that are unfounded. Despite the claims, the environment is rather small and the instructions almost trivially simple. For example, compare to the SAIL corpus that includes multi-sentence instructions with an average of 5 sentences/instruction (vs. 2); 37 words/instruction (vs. a manual cap of 9); and a total of 660 words (vs. 40); and three \"large\" virtual worlds (vs. 10x10 grids with 3-6 objects).\n\n* While the paper makes several claims regarding novelty, the contributions over existing approaches are unclear. For example, Chaplot et al. 2017 propose a similar architecture that also fuses a CNN-based representation of raw visual input with an RNN encoding of language, the result of which is fed to a RNN policy network. What is novel with the proposed approach and what are the advantages? The paper makes an incomplete attempt to evaluate the proposed model against Chaplot et al., but without implementing their complete architecture, little can be inferred from the comparison.\n\n* The paper claims that the fusion method realizes a *minimalistic* representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn't clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?).\n\n* It isn't clear that much can be concluded from the attention visualizations in Figs. 6 and 7, particularly regarding its contribution. Regarding Fig 6. the network attends to the target object (large apple), but not the smaller apple, which would be necessary to reason over their relative size. Further, the attention figure in Fig. 7(b) seems to foveate on both bags. In both cases, the distractor objects are very close to the true target, and one would expect the behavior to be similar irrespective of which one was being attended to.\n\n* The conclusion states that the method is \"highly flexible\" and able to handle a \"rich set of natural language instructions\". Neither of these claims are justified by the discussion (please elaborate on what makes the method \"highly flexible\", presumably the end-to-end nature of the architecture) or the experimental results.\n\n* The significance of randomly moving non-target objects that the agent encounters is unclear. What happens when the objects are not moved, as in real scenarios?\n\n* A stated contribution is that the \"textual representations are semantically meaningful\" but the importance is not justified.\n\n* Figure captions should appear below the figure, not at top.\n\n* Figures and tables should appear as close to their first reference as possible (e.g., Table 1 is 6 pages away from its reference at the beginning of Section 7).\n\n\n* Many citations should be enclosed in parentheses.\n\n\n\nReferences:\n\n* Artzi and Zettlemoyer, Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions, TACL 2013\n\n* Howard, Tellex, and Roy, A Natural Language Planner Interface for Mobile Manipulators, ICRA 2014\n\n* Chung, Propp, Walter, and Howard, On the performance of hierarchical distributed correspondence graphs for efficient symbol grounding of robot instructions, IROS 2015\n\n* Paul, Arkin, Roy, and Howard, Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators, RSS 2016\n\n* Tellex, Kollar, Dickerson, Walter, Banerjee, Teller and Roy, Understanding natural language commands for robotic navigation and mobile manipulation, AAAI 2011","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to navigate by distilling visual information and natural language instructions","abstract":"In this work, we focus on the problem in which an agent learns to navigate to the target object in a 2D grid environment. The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved. We propose a simple, novel architecture for grounding natural language instructions in our environment. Our model does not have any prior information of both the visual and textual modalities and is end-to-end trainable. We develop a novel attention mechanism for multimodal fusion of visual and textual modalities. Our experimental results show that our attention mechanism outperforms the existing multimodal fusion mechanisms proposed in order to solve the above mentioned task. We demonstrate through the visualization of attention weights that our model learns to correlate attributes of the object referred in the instruction with visual representations and also show that the learnt textual representations are semantically meaningful as they follow vector arithmetic. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities. In order to simulate the above described challenges, we introduce a new 2D environment for an agent to jointly learn visual and textual modalities.","pdf":"/pdf/3dc541a16594ed79d05221029f1a6ddaf0426bd0.pdf","TL;DR":"Novel architecture for language grounding via reinforcement learning in a new customizable 2D grid environment  ","paperhash":"anonymous|learning_to_navigate_by_distilling_visual_information_and_natural_language_instructions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to navigate by distilling visual information and natural language instructions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJPSN3gRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper388/Authors"],"keywords":["Deep reinforcement learning","Computer Vision","Multi-modal fusion","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1512222633632,"tcdate":1511381616783,"number":1,"cdate":1511381616783,"id":"BJtd58mlM","invitation":"ICLR.cc/2018/Conference/-/Paper388/Official_Review","forum":"HJPSN3gRW","replyto":"HJPSN3gRW","signatures":["ICLR.cc/2018/Conference/Paper388/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Toy experiments; Not much novelty; Missing references","rating":"4: Ok but not good enough - rejection","review":"Paper summary: The paper tackles the problem of navigation given an instruction. The paper proposes an approach to combine textual and visual information via an attention mechanism. The experiments have been performed on a 2D grid, where the agent has partial observation.\n\nPaper Strengths:\n- The proposed approach outperforms the baselines.\n- Generalization to unseen combination of objects and attributes is interesting.\n\nPaper Weaknesses:\nThis paper has the following issues so I vote for rejection: (1) The experiments have been performed on a toy environment, which is similar to the environments used in the 80's. There is no guarantee that the conclusions are valid for slightly more complex environments or real world. I highly recommend using environments such as AI2-THOR or SUNCG. (2) There is no quantitative result for the zero-shot experiments, which is one of the main claims of the paper. (3) The ideas of using instructions for navigation or using attention for combining visual and textual information have been around for a while. So there is not much novelty in the proposed method either. (4) References to attention papers that combine visual and textual modalities are missing.\n\nMore detailed comments:\n\n- Ego-centric is not a correct word for describing the input. Typically, the perspective changes in ego-centric views, which does not happen in this environment.\n\n- I do not agree that the attention maps focus on the right objects. Figures 6 and 7 show that the attention maps focus on all objects. The weights should be shown using a heatmap to see if the model is attending more to the right object.\n\n- I cannot find any table for the zero-shot experiments. In the rebuttal, please point me to the results in case I am missing them.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning to navigate by distilling visual information and natural language instructions","abstract":"In this work, we focus on the problem in which an agent learns to navigate to the target object in a 2D grid environment. The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved. We propose a simple, novel architecture for grounding natural language instructions in our environment. Our model does not have any prior information of both the visual and textual modalities and is end-to-end trainable. We develop a novel attention mechanism for multimodal fusion of visual and textual modalities. Our experimental results show that our attention mechanism outperforms the existing multimodal fusion mechanisms proposed in order to solve the above mentioned task. We demonstrate through the visualization of attention weights that our model learns to correlate attributes of the object referred in the instruction with visual representations and also show that the learnt textual representations are semantically meaningful as they follow vector arithmetic. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities. In order to simulate the above described challenges, we introduce a new 2D environment for an agent to jointly learn visual and textual modalities.","pdf":"/pdf/3dc541a16594ed79d05221029f1a6ddaf0426bd0.pdf","TL;DR":"Novel architecture for language grounding via reinforcement learning in a new customizable 2D grid environment  ","paperhash":"anonymous|learning_to_navigate_by_distilling_visual_information_and_natural_language_instructions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to navigate by distilling visual information and natural language instructions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJPSN3gRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper388/Authors"],"keywords":["Deep reinforcement learning","Computer Vision","Multi-modal fusion","Language Grounding"]}},{"tddate":null,"ddate":null,"tmdate":1509739330176,"tcdate":1509110847162,"number":388,"cdate":1509739327504,"id":"HJPSN3gRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJPSN3gRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning to navigate by distilling visual information and natural language instructions","abstract":"In this work, we focus on the problem in which an agent learns to navigate to the target object in a 2D grid environment. The agent receives visual information through raw pixels and a natural language instruction telling what task needs to be achieved. We propose a simple, novel architecture for grounding natural language instructions in our environment. Our model does not have any prior information of both the visual and textual modalities and is end-to-end trainable. We develop a novel attention mechanism for multimodal fusion of visual and textual modalities. Our experimental results show that our attention mechanism outperforms the existing multimodal fusion mechanisms proposed in order to solve the above mentioned task. We demonstrate through the visualization of attention weights that our model learns to correlate attributes of the object referred in the instruction with visual representations and also show that the learnt textual representations are semantically meaningful as they follow vector arithmetic. We also show that our model generalizes effectively to unseen scenarios and exhibit zero-shot generalization capabilities. In order to simulate the above described challenges, we introduce a new 2D environment for an agent to jointly learn visual and textual modalities.","pdf":"/pdf/3dc541a16594ed79d05221029f1a6ddaf0426bd0.pdf","TL;DR":"Novel architecture for language grounding via reinforcement learning in a new customizable 2D grid environment  ","paperhash":"anonymous|learning_to_navigate_by_distilling_visual_information_and_natural_language_instructions","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning to navigate by distilling visual information and natural language instructions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJPSN3gRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper388/Authors"],"keywords":["Deep reinforcement learning","Computer Vision","Multi-modal fusion","Language Grounding"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}