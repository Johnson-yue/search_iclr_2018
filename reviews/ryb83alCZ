{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222651930,"tcdate":1511915700689,"number":2,"cdate":1511915700689,"id":"SyangtilG","invitation":"ICLR.cc/2018/Conference/-/Paper429/Official_Review","forum":"ryb83alCZ","replyto":"ryb83alCZ","signatures":["ICLR.cc/2018/Conference/Paper429/AnonReviewer3"],"readers":["everyone"],"content":{"title":"TOWARDS UNSUPERVISED CLASSIFICATION WITH DEEP GENERATIVE MODELS","rating":"4: Ok but not good enough - rejection","review":"The authors propose a deep hierarchical model for unsupervised classification by using a combination of latent continuous and discrete distributions.\n\nAlthough, the detailed description of flow cytometry and chronic lymphocytic leukemia are appreciated, they are probably out of the scope of the paper or not relevant for the presented approach.\n\nThe authors claim that existing approaches for clustering cell populations in flow cytometry data are sensitive to noise and rely on cumbersome hyperparameter specifications, which in some sense is true, however, that does not mean that the proposed approach is less sensitive to noise or that that the proposed model has less free-parameters to tune (layers, hidden units, regularization, step size, link function, etc.). In fact, it is not clear how the authors would be able to define a model architecture without label information, what would be the model selection metric to optimize, ELBO?. At least this very issue is not addressed in the manuscript.\n\nIn Figure 1, please use different colors for different cell types. It is not described, but it would be good to stress out that each of the 4 components in Figure 1 right, corresponds to a mixture component.\n\nThe results in Tables 1 and 2 are not very convincing without clarity on the selection of the thresholds for each of the models. It would be better to report threshold-free metrics such as area under the ROC or PR curve. From Figures 3 and 4 for example, it is difficult to grasp the performance gap between the proposed approach and \\beta-VAE.\n\n- FC and CLL are not spelled out in the introduction.\n- Equation (5) is confusing, what is h, y = h or is h a mixture of Gaussians with \\alpha mixing proportions?\n- Equation (6) should be q(z_L|z)\n- Equation (8) is again confusing.\n- Equation (10) is not correct, x can't be conditioned on h, as it is clearly conditioned on z_1.\n- Equation (11) it should be q_\\phi().\n- It is not clear why the probabilities are thresholded at 0.5\n- Figures 3 and 4 could use larger markers and font sizes.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Unsupervised Classification with Deep Generative Models","abstract":"Deep generative models have advanced the state-of-the-art in semi-supervised classification, however their capacity for deriving useful discriminative features in a completely unsupervised fashion for classification in difficult real-world data sets, where adequate manifold separation is required has not been adequately explored. Most methods rely on defining a pipeline of deriving features via generative modeling and then applying clustering algorithms, separating the modeling and discriminative processes. We propose a deep hierarchical generative model which uses a mixture of discrete and continuous distributions to learn to effectively separate the different data manifolds and is trainable end-to-end. We show that by specifying the form of the discrete variable distribution we are imposing a specific structure on the model's latent representations. We test our model's discriminative performance on the task of CLL diagnosis against baselines from the field of computational FC, as well as the Variational Autoencoder literature.","pdf":"/pdf/a9fefa632004ea09fc72cbb6c168538bee8ea464.pdf","TL;DR":"Unsupervised classification via deep generative modeling with controllable feature learning evaluated in a difficult real world task","paperhash":"anonymous|towards_unsupervised_classification_with_deep_generative_models","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Unsupervised Classification with Deep Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryb83alCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper429/Authors"],"keywords":["variational inference","vae","variational autoencoders","generative modeling","representation learning","classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222651969,"tcdate":1511863574587,"number":1,"cdate":1511863574587,"id":"SJk7H29xM","invitation":"ICLR.cc/2018/Conference/-/Paper429/Official_Review","forum":"ryb83alCZ","replyto":"ryb83alCZ","signatures":["ICLR.cc/2018/Conference/Paper429/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Apparently impressive result, but very little novelty","rating":"4: Ok but not good enough - rejection","review":"This paper addresses the question of unsupervised clustering with high classification performance. They propose a deep variational autoencoder architecture with categorical latent variables at the deepest layer and propose to train it with modifications of the standard variational approach with reparameterization gradients. The model is tested on a medical imagining dataset where the task is to distinguish healthy from pathological lymphocytes from blood samples. \n\nI am not an expert on this particular dataset, but to my eye the results look impressive. They show high sensitivity and high specificity. This paper may be an important contribution to the medical imaging community.\n\nMy primary concern with the paper is the lack of novelty and relatively little in the way of contributions to the ICLR community. The proposed model is a simple variant on the standard VAE models (see for example the Ladder VAE https://arxiv.org/abs/1602.02282 for deep models with multiple stochastic layers). This would be OK if a thorough evaluation on at least two other datasets showed similar improvements as the lymphocytes dataset. As it stands, it is difficulty for me to assess the value of this model.\n\nMinor questions / concerns:\n\n- The authors claim in the first paragraph of 3.2 that deterministic mappings lack expressiveness. Would be great to see the paper take this claim seriously and investigate it.\n- In equation (13) it isn't clear whether you use q_phi to be the discrete mass or the concrete density. The distinction is discussed in https://arxiv.org/abs/1611.00712\n- Would be nice to report the MCC in experimental results.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Unsupervised Classification with Deep Generative Models","abstract":"Deep generative models have advanced the state-of-the-art in semi-supervised classification, however their capacity for deriving useful discriminative features in a completely unsupervised fashion for classification in difficult real-world data sets, where adequate manifold separation is required has not been adequately explored. Most methods rely on defining a pipeline of deriving features via generative modeling and then applying clustering algorithms, separating the modeling and discriminative processes. We propose a deep hierarchical generative model which uses a mixture of discrete and continuous distributions to learn to effectively separate the different data manifolds and is trainable end-to-end. We show that by specifying the form of the discrete variable distribution we are imposing a specific structure on the model's latent representations. We test our model's discriminative performance on the task of CLL diagnosis against baselines from the field of computational FC, as well as the Variational Autoencoder literature.","pdf":"/pdf/a9fefa632004ea09fc72cbb6c168538bee8ea464.pdf","TL;DR":"Unsupervised classification via deep generative modeling with controllable feature learning evaluated in a difficult real world task","paperhash":"anonymous|towards_unsupervised_classification_with_deep_generative_models","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Unsupervised Classification with Deep Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryb83alCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper429/Authors"],"keywords":["variational inference","vae","variational autoencoders","generative modeling","representation learning","classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739308354,"tcdate":1509117000621,"number":429,"cdate":1509739305698,"id":"ryb83alCZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryb83alCZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Unsupervised Classification with Deep Generative Models","abstract":"Deep generative models have advanced the state-of-the-art in semi-supervised classification, however their capacity for deriving useful discriminative features in a completely unsupervised fashion for classification in difficult real-world data sets, where adequate manifold separation is required has not been adequately explored. Most methods rely on defining a pipeline of deriving features via generative modeling and then applying clustering algorithms, separating the modeling and discriminative processes. We propose a deep hierarchical generative model which uses a mixture of discrete and continuous distributions to learn to effectively separate the different data manifolds and is trainable end-to-end. We show that by specifying the form of the discrete variable distribution we are imposing a specific structure on the model's latent representations. We test our model's discriminative performance on the task of CLL diagnosis against baselines from the field of computational FC, as well as the Variational Autoencoder literature.","pdf":"/pdf/a9fefa632004ea09fc72cbb6c168538bee8ea464.pdf","TL;DR":"Unsupervised classification via deep generative modeling with controllable feature learning evaluated in a difficult real world task","paperhash":"anonymous|towards_unsupervised_classification_with_deep_generative_models","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Unsupervised Classification with Deep Generative Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryb83alCZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper429/Authors"],"keywords":["variational inference","vae","variational autoencoders","generative modeling","representation learning","classification"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}