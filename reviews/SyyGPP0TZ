{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222823643,"tcdate":1511824021877,"number":2,"cdate":1511824021877,"id":"ByC55Gcxz","invitation":"ICLR.cc/2018/Conference/-/Paper94/Official_Review","forum":"SyyGPP0TZ","replyto":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference/Paper94/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Evaluation of a number of heuristics to improve LSTM language models.","rating":"7: Good paper, accept","review":"Clearly presented paper, including a number of reasonable techniques to improve LSTM-LMs. The proposed techniques are heuristic, but are reasonable and appear to yield improvements in perplexity. Some specific comments follow.\n\nre. \"ASGD\" for Averaged SGD: ASGD usually stands for Asynchronous SGD, have the authors considered an alternative acronym? AvSGD?\n\nre. Optimization criterion on page 2, note that SGD is usually taken to minimizing expected loss, not just empirical loss (Bottou thesis 1991).\n\nIs there any theoretical analysis of convergence for Averaged SGD?\n\nre. paragraph starting with \"To prevent such inefficient data usage, we randomly select the sequence length for the forward and backward pass in two steps\": the explanation is a bit unclear. What is the \"base sequence length\" exactly? Also, re. the motivation above this paragraph, I'm not sure what \"elements\" really refers to, though I can guess.\n\nWhat is the number of training tokens of the datasets used, PTB and WT2?\n\nCan the authors provide more explanation for what \"neural cache models\" are, and how they relate to \"pointer models\"?\n\nWhy do the sections \"Pointer models\", \"Ablation analysis\", and \"AWD-QRNN\" come after the Experiments section?","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]}},{"tddate":null,"ddate":null,"tmdate":1512222823685,"tcdate":1511536939006,"number":1,"cdate":1511536939006,"id":"HyQVY2Bxz","invitation":"ICLR.cc/2018/Conference/-/Paper94/Official_Review","forum":"SyyGPP0TZ","replyto":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference/Paper94/AnonReviewer3"],"readers":["everyone"],"content":{"title":"State of the art on small word level language modelling datasets","rating":"7: Good paper, accept","review":"The paper sets a new state of the art on word level language modelling on the Penn Treebank and Wikitext-2 datasets using various optimization and regularization techniques. These already very good results are further improved, by a large margin, using a Neural Cache.\n\nThe paper is well written, easy to follow and the results speak for themselves. One possible criticism is that the experimental methodology does not allow for reliable conclusions to be drawn about contributions of all different techniques, because they seem to have been evaluated at a single hyperparameter setting (that was hand tuned for the full model?).\n\nA variant on the Averaged SGD method is proposed. This so called NT-ASGD optimizer switches to averaging mode based on recent validation losses. I would have liked to see a more thorough assessment of NT-ASGD, especially against well tuned SGD.\n\nI particularly liked Figure 3 which shows how the Neural Cache makes the model much better at handling rare words and UNK (!) at the expense of very common words. Speaking of the Neural Cache, a natural baseline would have been dynamic evaluation.\n\nAll in all, the paper is a solid contribution which deserves to be accepted. It could become even better, were the experiments to tease the various factors apart.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]}},{"tddate":null,"ddate":null,"tmdate":1509739489421,"tcdate":1508960007207,"number":94,"cdate":1509739486763,"id":"SyyGPP0TZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyyGPP0TZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Regularizing and Optimizing LSTM Language Models","abstract":"In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM, which uses DropConnect on hidden-to-hidden weights, as a form of recurrent regularization. Further, we introduce NT-ASGD, a non-monotonically triggered  (NT) variant of the averaged stochastic gradient method (ASGD), wherein the averaging trigger is determined using a NT condition as opposed to being tuned by the user. Using these and other regularization strategies, our ASGD Weight-Dropped LSTM (AWD-LSTM) achieves state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2. We also explore the viability of the proposed regularization and optimization strategies in the context of the quasi-recurrent neural network (QRNN) and demonstrate comparable performance to the AWD-LSTM counterpart. The code for reproducing the results is open sourced and is available at \\url{MASKED}.","pdf":"/pdf/8ea5b4c43b73f8c57bf115123bfd9bb7c0026c98.pdf","TL;DR":"Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2. ","paperhash":"anonymous|regularizing_and_optimizing_lstm_language_models","_bibtex":"@article{\n  anonymous2018regularizing,\n  title={Regularizing and Optimizing LSTM Language Models},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyyGPP0TZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper94/Authors"],"keywords":["language model","LSTM","regularization","optimization","ASGD","dropconnect"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}