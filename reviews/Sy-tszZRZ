{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222823754,"tcdate":1511990951011,"number":3,"cdate":1511990951011,"id":"r1knUinef","invitation":"ICLR.cc/2018/Conference/-/Paper940/Official_Review","forum":"Sy-tszZRZ","replyto":"Sy-tszZRZ","signatures":["ICLR.cc/2018/Conference/Paper940/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"8: Top 50% of accepted papers, clear accept","review":"This is quite an interesting paper. Thank you. Here are a few comments:\n\nI think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix. \nHowever I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved). I can appreciate though that this a fine line to walk. Also in the appendix, please restate the lemma that is being proven. Otherwise one will have to scroll up and down all the time to understand the proof. \n\nI think the paper could also discuss a bit more in detail the results provided. For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice. Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bounding and Counting Linear Regions of Deep Neural Networks","abstract":"In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Mont´ufar et al. (2014) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.","pdf":"/pdf/d32bf700a1a75b548333244a3f440231dc35435b.pdf","TL;DR":"We empirically count the number of linear regions of rectifier networks and refine upper and lower bounds.","paperhash":"anonymous|bounding_and_counting_linear_regions_of_deep_neural_networks","_bibtex":"@article{\n  anonymous2018bounding,\n  title={Bounding and Counting Linear Regions of Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-tszZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper940/Authors"],"keywords":["rectifier networks","maxout networks","piecewise linear functions","linear regions","mixed-integer programming"]}},{"tddate":null,"ddate":null,"tmdate":1512222823795,"tcdate":1511818749559,"number":2,"cdate":1511818749559,"id":"SkSZLZ5gf","invitation":"ICLR.cc/2018/Conference/-/Paper940/Official_Review","forum":"Sy-tszZRZ","replyto":"Sy-tszZRZ","signatures":["ICLR.cc/2018/Conference/Paper940/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Official Review for Bounding and Counting Linear Regions of DNNs","rating":"4: Ok but not good enough - rejection","review":"Paper Summary:\n\nThis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network. It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions. These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have. This paper improves on the upper bound given by [2] and the lower bound given by [1]. They also provide a tight bound for the one dimensional input case. Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST\n\nMain Comments:\nThe paper is very well written and clearly states and explains the contributions. However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds, with no other novel interpretations or insights into deep architectures. (The improvement on Zaslavsky's theorem is interesting.) The idea of counting the number of regions exactly by solving a linear program is interesting, but is not going to scale well, and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST. It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful.\n\nOverall, while the paper is well written and makes some interesting points, it presently isn't a significant enough contribution to warrant acceptance.\n\n[1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio\n[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bounding and Counting Linear Regions of Deep Neural Networks","abstract":"In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Mont´ufar et al. (2014) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.","pdf":"/pdf/d32bf700a1a75b548333244a3f440231dc35435b.pdf","TL;DR":"We empirically count the number of linear regions of rectifier networks and refine upper and lower bounds.","paperhash":"anonymous|bounding_and_counting_linear_regions_of_deep_neural_networks","_bibtex":"@article{\n  anonymous2018bounding,\n  title={Bounding and Counting Linear Regions of Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-tszZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper940/Authors"],"keywords":["rectifier networks","maxout networks","piecewise linear functions","linear regions","mixed-integer programming"]}},{"tddate":null,"ddate":null,"tmdate":1512222823836,"tcdate":1511810826080,"number":1,"cdate":1511810826080,"id":"SkfMvJqez","invitation":"ICLR.cc/2018/Conference/-/Paper940/Official_Review","forum":"Sy-tszZRZ","replyto":"Sy-tszZRZ","signatures":["ICLR.cc/2018/Conference/Paper940/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper builds on previous work to bound and count the number of linear regions in ReLU networks, and evaluates this in small experiments. ","rating":"6: Marginally above acceptance threshold","review":"This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions. It also evaluates the number of regions of small networks during training. \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar. \n\nThe improved lower bound given in Theorem 6 is very modest but neat. Theorem 5 follows easily from this. \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel. \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks. It presents experiments during training and with varying network sizes. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Bounding and Counting Linear Regions of Deep Neural Networks","abstract":"In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Mont´ufar et al. (2014) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.","pdf":"/pdf/d32bf700a1a75b548333244a3f440231dc35435b.pdf","TL;DR":"We empirically count the number of linear regions of rectifier networks and refine upper and lower bounds.","paperhash":"anonymous|bounding_and_counting_linear_regions_of_deep_neural_networks","_bibtex":"@article{\n  anonymous2018bounding,\n  title={Bounding and Counting Linear Regions of Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-tszZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper940/Authors"],"keywords":["rectifier networks","maxout networks","piecewise linear functions","linear regions","mixed-integer programming"]}},{"tddate":null,"ddate":null,"tmdate":1509739022942,"tcdate":1509137275257,"number":940,"cdate":1509739020281,"id":"Sy-tszZRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Sy-tszZRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Bounding and Counting Linear Regions of Deep Neural Networks","abstract":"In this paper, we study the representational power of deep neural networks (DNN) that belong to the family of piecewise-linear (PWL) functions, based on PWL activation units such as rectifier or maxout. We investigate the complexity of such networks by studying the number of linear regions of the PWL function. Typically, a PWL function from a DNN can be seen as a large family of linear functions acting on millions of such regions. We directly build upon the work of Mont´ufar et al. (2014) and Raghu et al. (2017) by refining the upper and lower bounds on the number of linear regions for rectified and maxout networks. In addition to achieving tighter bounds, we also develop a novel method to perform exact enumeration or counting of the number of linear regions with a mixed-integer linear formulation that maps the input space to output. We use this new capability to visualize how the number of linear regions change while training DNNs.","pdf":"/pdf/d32bf700a1a75b548333244a3f440231dc35435b.pdf","TL;DR":"We empirically count the number of linear regions of rectifier networks and refine upper and lower bounds.","paperhash":"anonymous|bounding_and_counting_linear_regions_of_deep_neural_networks","_bibtex":"@article{\n  anonymous2018bounding,\n  title={Bounding and Counting Linear Regions of Deep Neural Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy-tszZRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper940/Authors"],"keywords":["rectifier networks","maxout networks","piecewise linear functions","linear regions","mixed-integer programming"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}