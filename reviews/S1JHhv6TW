{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222744785,"tcdate":1511845243660,"number":2,"cdate":1511845243660,"id":"SyEtTPclG","invitation":"ICLR.cc/2018/Conference/-/Paper76/Official_Review","forum":"S1JHhv6TW","replyto":"S1JHhv6TW","signatures":["ICLR.cc/2018/Conference/Paper76/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very interesting and thorough paper.","rating":"9: Top 15% of accepted papers, strong accept","review":"To date the theoretical advantage of deep learning has focused on the concept of \"expressive efficiency\" where one network must grow much larger to replicate functions that another \"more efficient\" network can produce. This has focused so far on depth (i.e. shallow networks have to grow much larger than deeper networks to express the same set of networks)\n\nThe authors explore another dimension here, namely that of \"connectivity\". They study dilated convolutional networks and show that intertwining two dilated convolutional networks A and B at various stages (formalized via mixed tensor decompositions) it is more expressively efficient than not intertwining. \n\nThe authors' experiments support their theory showing that their mixed strategy leads to gains over a vanilla dilated convolutional net.\n\nI found the paper very well written despite its level of mathematical depth (the authors provide many helpful pictures) and strongly recommend accepting this paper.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions","abstract":"The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to replicate functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency brought forth by connectivity, motivated by the observation that modern networks interconnect their layers in elaborate ways. We focus on dilated convolutional networks, a family of deep models delivering state of the art performance in sequence processing tasks. By introducing and analyzing the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that even a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not. Empirical evaluation demonstrates how the expressive efficiency of connectivity, similarly to that of depth, translates into gains in accuracy. This leads us to believe that expressive efficiency may serve a key role in developing new tools for deep network design.","pdf":"/pdf/1f44d08536e8a390a774c33eef37b347b3581132.pdf","TL;DR":"We introduce the notion of mixed tensor decompositions, and use it to prove that interconnecting dilated convolutional networks boosts their expressive power.","paperhash":"anonymous|boosting_dilated_convolutional_networks_with_mixed_tensor_decompositions","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1JHhv6TW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper76/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Dilated Convolutions","Tensor Decompositions"]}},{"tddate":null,"ddate":null,"tmdate":1512222744821,"tcdate":1511577981859,"number":1,"cdate":1511577981859,"id":"ryLYFULlM","invitation":"ICLR.cc/2018/Conference/-/Paper76/Official_Review","forum":"S1JHhv6TW","replyto":"S1JHhv6TW","signatures":["ICLR.cc/2018/Conference/Paper76/AnonReviewer3"],"readers":["everyone"],"content":{"title":"see detailed review below.","rating":"7: Good paper, accept","review":"This paper theoretically validates that interconnecting networks with different dilations can lead to expressive efficiency, which indicates an interesting phenomenon that connectivity is able to enhance the expressiveness of deep networks. A key technical tool is a mixed tensor decomposition, which is shown to have representational advantage over the individual hierarchical decompositions it comprises.\n\nPros:\n\nExisting work have focused on understanding the depth of the networks and established that deep networks are expressively efficient with respect to shallow ones. On the other hand, this paper focused on the architectural feature of connectivity. The problem is fundamentally important and its theoretical development is solid. The conclusion is useful for developing new tools for deep network design. \n\nCons:\n\nIn order to show that the mixed dilated convolutional network is expressively efficient w.r.t. the corresponding individual dilated convolutional network, the authors prove it in two steps: Proposition 1 and Proposition 2. However, in the proof of Proposition 2 (see Theorem 1), the authors only focus on a particular case of convolutional arithmetic circuits, i.e., $g(a,b)= a*b$. In the experiments, see line 4 of page 9, the authors instead used ReLU activation $g(a, b)= max{a+b, 0}$. Can authors provide some justifications of such different choices of activation functions? It would be great if authors can discuss how to generate the activation function in Theorem 1 to more general cases.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions","abstract":"The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to replicate functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency brought forth by connectivity, motivated by the observation that modern networks interconnect their layers in elaborate ways. We focus on dilated convolutional networks, a family of deep models delivering state of the art performance in sequence processing tasks. By introducing and analyzing the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that even a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not. Empirical evaluation demonstrates how the expressive efficiency of connectivity, similarly to that of depth, translates into gains in accuracy. This leads us to believe that expressive efficiency may serve a key role in developing new tools for deep network design.","pdf":"/pdf/1f44d08536e8a390a774c33eef37b347b3581132.pdf","TL;DR":"We introduce the notion of mixed tensor decompositions, and use it to prove that interconnecting dilated convolutional networks boosts their expressive power.","paperhash":"anonymous|boosting_dilated_convolutional_networks_with_mixed_tensor_decompositions","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1JHhv6TW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper76/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Dilated Convolutions","Tensor Decompositions"]}},{"tddate":null,"ddate":null,"tmdate":1509739500005,"tcdate":1508895798740,"number":76,"cdate":1509739497350,"id":"S1JHhv6TW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1JHhv6TW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions","abstract":"The driving force behind deep networks is their ability to compactly represent rich classes of functions. The primary notion for formally reasoning about this phenomenon is expressive efficiency, which refers to a situation where one network must grow unfeasibly large in order to replicate functions of another. To date, expressive efficiency analyses focused on the architectural feature of depth, showing that deep networks are representationally superior to shallow ones. In this paper we study the expressive efficiency brought forth by connectivity, motivated by the observation that modern networks interconnect their layers in elaborate ways. We focus on dilated convolutional networks, a family of deep models delivering state of the art performance in sequence processing tasks. By introducing and analyzing the concept of mixed tensor decompositions, we prove that interconnecting dilated convolutional networks can lead to expressive efficiency. In particular, we show that even a single connection between intermediate layers can already lead to an almost quadratic gap, which in large-scale settings typically makes the difference between a model that is practical and one that is not. Empirical evaluation demonstrates how the expressive efficiency of connectivity, similarly to that of depth, translates into gains in accuracy. This leads us to believe that expressive efficiency may serve a key role in developing new tools for deep network design.","pdf":"/pdf/1f44d08536e8a390a774c33eef37b347b3581132.pdf","TL;DR":"We introduce the notion of mixed tensor decompositions, and use it to prove that interconnecting dilated convolutional networks boosts their expressive power.","paperhash":"anonymous|boosting_dilated_convolutional_networks_with_mixed_tensor_decompositions","_bibtex":"@article{\n  anonymous2018boosting,\n  title={Boosting Dilated Convolutional Networks with Mixed Tensor Decompositions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1JHhv6TW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper76/Authors"],"keywords":["Deep Learning","Expressive Efficiency","Dilated Convolutions","Tensor Decompositions"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}