{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222669075,"tcdate":1511838917940,"number":3,"cdate":1511838917940,"id":"SyR6NUcxz","invitation":"ICLR.cc/2018/Conference/-/Paper486/Official_Review","forum":"rkGZuJb0b","replyto":"rkGZuJb0b","signatures":["ICLR.cc/2018/Conference/Paper486/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Not very rigorously written, results are mediocre","rating":"4: Ok but not good enough - rejection","review":"The authors study compressing feed forward layers using low rank tensor decompositions. For instance a feed forward layer of 4096 x 4096 would first be reshaped into a rank-12 tensor with each index having dimension 2, and then a tensor decomposition would be applied to reduce the number of parameters. \n\nPrevious work used tensor trains which decompose the tensor as a chain. Here the authors explore a tree like decomposition. The authors only describe their model using pictures and do not provide any rigorous description of how their decomposition works.\n\nThe results are mediocre. While the author's approach does seem to reduce the feed forward net parameters by 30% compared to the tensor train decomposition for similar accuracy, the total number of parameters for both MERA (authors' approach) and Tensor Train is similar since in this regime the CNN parameters dominate (and the authors' approach does not work to compress those).\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz","abstract":"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n","pdf":"/pdf/7855851dde4730b880e9645193facac6662fd0c3.pdf","TL;DR":"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. ","paperhash":"anonymous|compact_neural_networks_based_on_the_multiscale_entanglement_renormalization_ansatz","_bibtex":"@article{\n  anonymous2018compact,\n  title={Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkGZuJb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper486/Authors"],"keywords":["Neural Networks","Tensor Networks","Tensor Trains"]}},{"tddate":null,"ddate":null,"tmdate":1512222669121,"tcdate":1511725012728,"number":2,"cdate":1511725012728,"id":"B1TCDcOxG","invitation":"ICLR.cc/2018/Conference/-/Paper486/Official_Review","forum":"rkGZuJb0b","replyto":"rkGZuJb0b","signatures":["ICLR.cc/2018/Conference/Paper486/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting idea but more experimental validation required","rating":"5: Marginally below acceptance threshold","review":"The paper presents a new parameterization of linear maps for use in neural networks, based on the Multiscale Entanglement Renormalization Ansatz (MERA). The basic idea is to use a hierarchical factorization of the linear map, that greatly reduces the number of parameters while still allowing for relatively complex interactions between variables to be modelled. A limited number of experiments on CIFAR10 suggests that the method may work a bit better than related factorizations.\n\nThe paper contains interesting new ideas and is generally well written. However, a few things are not fully explained, and the experiments are too limited to be convincing.\n\n\nExposition\nOn a first reading, it is initially unclear why we are talking about higher order tensors at all. Usually, fully connected layers are written as matrix-vector multiplications. It is only on the bottom of page 3 that it is explained that we will reshape the input to a rank-k (k=12) tensor before applying the MERA factored map. It would be helpful to state this sooner. It would also be nice to state that (in the absense of any factorization of the weight tensor) a linear contraction of such a high-rank tensor is no less general than a matrix-vector multiplication.\n\nMost ML researchers will not know Haar measure. It would be more reader friendly to say something like \"uniform distribution over orthogonal matrices (i.e. Haar measure)\" or something like that. Explaining how to sample orthogonal matrices / tensors (e.g. by SVD) would be helpful as well.\n\nThe article does not explain what \"disentanglers\" are. It is very important to explain this, because it will not be generally known by the machine learning audience, and is the main thing that distinguishes this work form earlier tree-based factorizations.\n\nOn page 5 it is explained that the computational complexity of the proposed method is N^{log_2 D}. For D=2, this is better than a fully connected layer. Although this theoretical speedup may not currently have been realized, it perhaps could be achieved by a custom GPU kernel. It would be nice to highlight this potential benefit in the introduction.\n\n\nTheoretical motivation\nAlthough I find the theoretical motivation for the method somewhat compelling, some questions remain that the authors may want to address. For one thing, the paper talks about exploiting \"hierarchical / multiscale structure\", but this does not refer to the spatial multi-scale structure that is naturally present in images. Instead, the dimensions of a hidden activation vector are arbitrarily ordered, partitioned into pairs, and reshaped into a (2, 2, ..., 2) shape tensor. The pairing of dimensions determines the kinds of interactions the MERA layer can express. Although the earlier layers could learn to produce a representation that can be effectively analyzed by the MERA layer, one is left to wonder if the method could be made to exploit the spatial multi-scale structure that we know is actually present in image data.\n\nAnother point is that although from a classical statistics perspective it would seem that reducing the number of parameters should be generally beneficial, it has been observed many times that in deep learning, highly overparameterized models are easier to optimize and do not necessarily overfit. Thus at this point it is not clear whether starting with a highly constrained parameterization would allow us to obtain state of the art accuracy levels, or whether it is better to start with an overparameterized model and gradually constrain it or perform a post-training compression step.\n\n\nExperiments\nIn the introduction it is claimed that the method of Liu et al. cannot capture correlations on different length scales because it lacks disentanglers. Although this may be theoretically correct, the paper does not experimentally verify that the proposed factorization with disentanglers outperforms a similar approach without disentanglers. In my opinion this is a critical omission, because the addition of disentanglers seems to be the main or perhaps only difference to previous work.\n\nThe experiments show that MERA can drastically reduce the number of parameters of fully connected layers with only a modest drop in accuracy, for a particular ConvNet trained on CIFAR10. Unfortunately this ConvNet is far from state of the art, so it is not clear if the method would also work for better architectures. Furthermore, training deep nets can be tricky, and so the poor performance makes it impossible to tell if the baseline is (unintentionally) crippled.\n\nComparing MERA-2 to TT-3 or MERA-3 to TT-5 (which have an approximately equal number of parameters), the difference in accuracy appears to be less than 1 percentage point. Since only a handful of specific MERA / TT architectures were compared on a single dataset, it is not at all clear that we can expect MERA to outperform TT in many situations. In fact, it is not even clear that the small difference observed is stable under random retraining.\n\n\nSummary\nAn interesting paper with novel theoretical ideas, but insufficient experimental validation. Some expository issues need to be fixed.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz","abstract":"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n","pdf":"/pdf/7855851dde4730b880e9645193facac6662fd0c3.pdf","TL;DR":"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. ","paperhash":"anonymous|compact_neural_networks_based_on_the_multiscale_entanglement_renormalization_ansatz","_bibtex":"@article{\n  anonymous2018compact,\n  title={Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkGZuJb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper486/Authors"],"keywords":["Neural Networks","Tensor Networks","Tensor Trains"]}},{"tddate":null,"ddate":null,"tmdate":1512222669168,"tcdate":1511702682170,"number":1,"cdate":1511702682170,"id":"HJfixHulz","invitation":"ICLR.cc/2018/Conference/-/Paper486/Official_Review","forum":"rkGZuJb0b","replyto":"rkGZuJb0b","signatures":["ICLR.cc/2018/Conference/Paper486/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting method but the paper needs to be rewritten","rating":"4: Ok but not good enough - rejection","review":"In the paper the authors suggest to use MERA tensorization technique for compressing neural networks. MERA itseld in a known framework in QM but not in ML. Although the idea seems to be fruitful and interesting I find the paper quite unclear. The most important part is section 2 which presents the methodology used. However there no equations or formal descriptions of what is MERA and how it works. Only figures which are difficult to understand. It is almost impossible to reproduce the results based on such iformal description of tensorization method. The authors should be more careful and provide more details when describing the algorithm. There was enough room for making the algorithm more clear. This is my main point for critisism.\n\nAnother issue is related with practical usefulness. MERA allows to get better compression than TT keeping the same accuracy. But the authors do compress only one layer. In this case the total compression of DNN is almost tha same so why do we need yet another tensorization technique? I think the authors should try tenzorizing several layers and explore whether they can do any better than TT compression. Currently I would say the results are comparable but not definitely better.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz","abstract":"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n","pdf":"/pdf/7855851dde4730b880e9645193facac6662fd0c3.pdf","TL;DR":"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. ","paperhash":"anonymous|compact_neural_networks_based_on_the_multiscale_entanglement_renormalization_ansatz","_bibtex":"@article{\n  anonymous2018compact,\n  title={Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkGZuJb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper486/Authors"],"keywords":["Neural Networks","Tensor Networks","Tensor Trains"]}},{"tddate":null,"ddate":null,"tmdate":1509739276145,"tcdate":1509124090251,"number":486,"cdate":1509739273467,"id":"rkGZuJb0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkGZuJb0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz","abstract":"The goal of this paper is to demonstrate a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for linear layers in a neural network and test this implementation on the CIFAR-10 dataset. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA-layers with 3900 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers.\n","pdf":"/pdf/7855851dde4730b880e9645193facac6662fd0c3.pdf","TL;DR":"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. ","paperhash":"anonymous|compact_neural_networks_based_on_the_multiscale_entanglement_renormalization_ansatz","_bibtex":"@article{\n  anonymous2018compact,\n  title={Compact Neural Networks based on the Multiscale Entanglement Renormalization Ansatz},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkGZuJb0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper486/Authors"],"keywords":["Neural Networks","Tensor Networks","Tensor Trains"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}