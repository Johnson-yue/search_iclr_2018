{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222709846,"tcdate":1511803870100,"number":3,"cdate":1511803870100,"id":"BkIy2pKxf","invitation":"ICLR.cc/2018/Conference/-/Paper656/Official_Review","forum":"ry-TW-WAb","replyto":"ry-TW-WAb","signatures":["ICLR.cc/2018/Conference/Paper656/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A modern sparse Bayesian learning approach to weight quantization","rating":"7: Good paper, accept","review":"\nThe goal of this work is to infer weights of a neural network, constrained to a discrete set, where each weight can be represented by a few bits. This is a quite important and hot topic in deep learning. As a direct optimization would lead to a highly nontrivial combinatorial optimization problem, the authors propose a so-called 'quantizing prior' (actually a relaxed spike and slab prior to induce a sparsity enforcing heavy tail prior) over weights and derive a differentiable variational KL approximation. One important advantage of the current method is that this approach does not require fine-tuning after quantization. The paper presents ternary quantization for LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).\n\nThe paper is mostly well written and cites carefully the recent relevant literature. While there are a few glitches here and there in the writing, overall the paper is easy to follow. One exception is that in section 2, many ideas are presented in a sequence without providing any guidance where all this will lead.\nThe idea is closely related to sparse Bayesian learning but the variational approximation is achieved via the local reparametrization trick of Kingma 2015, with the key idea presented in section 3.3.\n\n\n\nMinor\n\nIn the introduction, the authors write \"... weights with a large variance can be pruned as they do not contribute much to the overall computation\". What does this mean? Is this the marginal posterior variance as in ARD? \n\nThe authors write: \"Additionally, variational Bayesian inference  is known to automatically reduce parameter redundancy by penalizing overly complex models.\" I would argue that \nit is Bayesian inference; variational inference sometimes retains this property, but not always.\n\nIn Eq (10), z needs also subscripts, as otherwise the notation may suggest parameter tying. Alternatively, drop the indices entirely, as later in the paper.\n\nSec. 3.2. is not very well written. This seems to be the MAP of the product of the marginals,\nor the mode of the variational distribution, not the true MAP configuration of the weight posterior. Please be more precise. \n\nThe abbreviation P&Q (probably Post-training Quantization) seems to be not defined in the paper.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Network Quantization","abstract":"We formulate the preparation of a neural network for pruning and few-bit quantization as a variational inference problem. We introduce a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights and further derive a differentiable KL approximation for this prior. After training with Variational Network Quantization (VNQ), weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). Our method does not require fine-tuning after quantization. We show results for ternary quantization on LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).","pdf":"/pdf/4f437a3dfc46a2dae29951d608efe9d42caa15c7.pdf","TL;DR":"We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.","paperhash":"anonymous|variational_network_quantization","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Network Quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry-TW-WAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper656/Authors"],"keywords":["Network compression","variational inferene","ternary network","Bayesian neural network","weight quantization","weight sharing"]}},{"tddate":null,"ddate":null,"tmdate":1512222709883,"tcdate":1511776821811,"number":2,"cdate":1511776821811,"id":"S10EfvFxM","invitation":"ICLR.cc/2018/Conference/-/Paper656/Official_Review","forum":"ry-TW-WAb","replyto":"ry-TW-WAb","signatures":["ICLR.cc/2018/Conference/Paper656/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper extending on previous work on variational compression for neural networks.","rating":"7: Good paper, accept","review":"This paper presents Variational Network Quantization; a variational Bayesian approach for quantising neural network weights to ternary values post-training in a principled way. This is achieved by a straightforward extension of the scale mixture of Gaussians perspective of the log-uniform prior proposed at [1]. The authors posit a mixture of delta peaks hyperprior over the locations of the Gaussian distribution, where each peak can be seen as the specific target value for quantisation (including zero to induce sparsity). They then further propose an approximation for the KL-divergence, necessary for the variational objective, from this multimodal prior to a factorized Gaussian posterior by appropriately combining the approximation given at [2] for each of the modes. At test-time, the variational posterior for each weight is replaced by the target quantisation value that is closest, w.r.t. the squared distance, to the mean of the Gaussian variational posterior. Encouraging experimental results are shown with performance comparable to the state-of-the-art for ternary weight neural networks.\n\nThis paper presented a straightforward extension of the work done at [1, 2] for ternary networks through a multimodal quantising prior. It is generally well-written, with extensive preliminaries and clear equations. The visualizations also serve as a nice way to convey the behaviour of the proposed approach. The idea is interesting and well executed so I propose for acceptance. I only have a couple of minor questions: \n- For the KL-divergence approximation you report a maximum difference of 1 nat per weight that seems a bit high; did you experiment with the `naive` Monte Carlo approximation of the bound (e.g. as done at Bayes By Backprop) during optimization? If yes, was there a big difference in performance?\n- Was pre-training necessary to obtain the current results for MNIST? As far as I know, [1] and [2] did not need pre-training for the MNIST results (but did employ pre-training for CIFAR 10).\n- How necessary was each one of the constraints during optimization (and what did they prevent)? \n- Did you ever observe posterior means that do not settle at one of the prior modes but rather stay in between? Or did you ever had issues of the variance growing large enough, so that q(w) captures multiple modes of the prior (maybe the constraints prevent this)? How sensitive is the quantisation scheme?\n\nOther minor comments / typos:\n(1) 7th line of section 2.1 page 2, ‘a unstructured data’ -> ‘unstructured data’\n(2) 5th line on page 3, remove ‘compare Eq. (1)’ (or rephrase it appropriately).\n(3) Section 2.2, ’Kullback-Leibler divergence between the true and the approximate posterior’; between implies symmetry (and the KL isn’t symmetric) so I suggest to change it to e.g. ‘from the true to the approximate posterior’ to avoid confusion. Same for the first line of Section 3.3.\n(4) Footnote 2, the distribution of the noise depends on the random variable so I would suggest to change it to a general \\epsilon \\sim p(\\epsilon).\n(5) Equation 4 is confusing.\n\n[1] Louizos, Ullrich & Welling, Bayesian Compression for Deep Learning.\n[2] Molchanov, Ashukha & Vetrov, Variational Dropout Sparsifies Deep Neural Networks.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Network Quantization","abstract":"We formulate the preparation of a neural network for pruning and few-bit quantization as a variational inference problem. We introduce a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights and further derive a differentiable KL approximation for this prior. After training with Variational Network Quantization (VNQ), weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). Our method does not require fine-tuning after quantization. We show results for ternary quantization on LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).","pdf":"/pdf/4f437a3dfc46a2dae29951d608efe9d42caa15c7.pdf","TL;DR":"We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.","paperhash":"anonymous|variational_network_quantization","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Network Quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry-TW-WAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper656/Authors"],"keywords":["Network compression","variational inferene","ternary network","Bayesian neural network","weight quantization","weight sharing"]}},{"tddate":null,"ddate":null,"tmdate":1512222709920,"tcdate":1511431010549,"number":1,"cdate":1511431010549,"id":"By5wsMNxM","invitation":"ICLR.cc/2018/Conference/-/Paper656/Official_Review","forum":"ry-TW-WAb","replyto":"ry-TW-WAb","signatures":["ICLR.cc/2018/Conference/Paper656/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Sparsity prior in variational Bayesian deep learning","rating":"7: Good paper, accept","review":"This paper proposes to use a mixture of continuous spikes propto 1/abs(w_ij-c_k) as prior for a Bayesian neural network and demonstrates good performance with relatively sparsified convnets for minist and cifar-10. The paper is building quite a lot upon Kingma et al 2015 and  Molchanov et al 2017. \n\nThe paper is of good quality, clearly written with an ok level of originality and significance.\n\nPros:\n1. Demonstrates a sparse Bayesian approach that scales.\n2. Really a relevant research area for being able to make more efficient and compact deployment.\nCons:\n1. Somewhat incremental relative to the papers mentioned above.\n2. Could have taken the experimental part further. For example can we learn something about what part of the network has  the biggest potential for being pruned and use that to come up with modifications of the architecture?    ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Network Quantization","abstract":"We formulate the preparation of a neural network for pruning and few-bit quantization as a variational inference problem. We introduce a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights and further derive a differentiable KL approximation for this prior. After training with Variational Network Quantization (VNQ), weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). Our method does not require fine-tuning after quantization. We show results for ternary quantization on LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).","pdf":"/pdf/4f437a3dfc46a2dae29951d608efe9d42caa15c7.pdf","TL;DR":"We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.","paperhash":"anonymous|variational_network_quantization","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Network Quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry-TW-WAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper656/Authors"],"keywords":["Network compression","variational inferene","ternary network","Bayesian neural network","weight quantization","weight sharing"]}},{"tddate":null,"ddate":null,"tmdate":1510241495730,"tcdate":1510241495730,"number":1,"cdate":1510241495730,"id":"Hyg1BgzJM","invitation":"ICLR.cc/2018/Conference/-/Paper656/Official_Comment","forum":"ry-TW-WAb","replyto":"ry-TW-WAb","signatures":["ICLR.cc/2018/Conference/Paper656/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper656/Authors"],"content":{"title":"Typo corrections","comment":"The authors would like to correct four typos in the current version of the manuscript:\n-) Table 1: Percentage of non-zero weights for Soft Weight-Sharing (P&Q) is 0.5 (not 3 as reported in the table) and bits for Deep Compression is 5 - 8 (not 10 - 13 as reported in the table)\n-) Last paragraph before 4.1: We ensure alpha >= 0.05 by clipping.\n-) Page 8, last sentence: we use a batch size of 64 samples\n-) Appendix, Figure 3: The validation accuracy of the network shown is 91.55% (corresponds to VNQ (no P&Q) in Table 1).\n\nWe additionally want to clarify that in Eq. (11) p_m denotes the prior over locations whereas p_k is a scalar (the mixture weight for component k).\n\nNote that the point of these corrections is to avoid potential confusion, our main results are not affected by these typos."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variational Network Quantization","abstract":"We formulate the preparation of a neural network for pruning and few-bit quantization as a variational inference problem. We introduce a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights and further derive a differentiable KL approximation for this prior. After training with Variational Network Quantization (VNQ), weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). Our method does not require fine-tuning after quantization. We show results for ternary quantization on LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).","pdf":"/pdf/4f437a3dfc46a2dae29951d608efe9d42caa15c7.pdf","TL;DR":"We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.","paperhash":"anonymous|variational_network_quantization","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Network Quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry-TW-WAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper656/Authors"],"keywords":["Network compression","variational inferene","ternary network","Bayesian neural network","weight quantization","weight sharing"]}},{"tddate":null,"ddate":null,"tmdate":1509739177560,"tcdate":1509130680674,"number":656,"cdate":1509739174896,"id":"ry-TW-WAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ry-TW-WAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variational Network Quantization","abstract":"We formulate the preparation of a neural network for pruning and few-bit quantization as a variational inference problem. We introduce a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights and further derive a differentiable KL approximation for this prior. After training with Variational Network Quantization (VNQ), weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). Our method does not require fine-tuning after quantization. We show results for ternary quantization on LeNet-5 (MNIST) and DenseNet-121 (CIFAR-10).","pdf":"/pdf/4f437a3dfc46a2dae29951d608efe9d42caa15c7.pdf","TL;DR":"We quantize and prune neural network weights using variational Bayesian inference with a multi-modal, sparsity inducing prior.","paperhash":"anonymous|variational_network_quantization","_bibtex":"@article{\n  anonymous2018variational,\n  title={Variational Network Quantization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ry-TW-WAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper656/Authors"],"keywords":["Network compression","variational inferene","ternary network","Bayesian neural network","weight quantization","weight sharing"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}