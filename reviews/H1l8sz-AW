{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222819085,"tcdate":1511893133929,"number":3,"cdate":1511893133929,"id":"HkI5OXsxz","invitation":"ICLR.cc/2018/Conference/-/Paper921/Official_Review","forum":"H1l8sz-AW","replyto":"H1l8sz-AW","signatures":["ICLR.cc/2018/Conference/Paper921/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper has a weak theoretical justification and poor choice of baseline method for empirical evaluation.","rating":"4: Ok but not good enough - rejection","review":"The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space). This goal is to achieve a similar effect to that of natural gradient, but with lighter computation. The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below). Experiments on MNIST with show improved generalization (but the baseline is chosen poorly, read below).\n\nThe paper is easy to read and organized very well, and has adequate literature review. However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides.\n\nOn the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2.2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate]. From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form. However, in the latter, the optimization objective is the f itself (sup E[f_1]-E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric).\n\nOn the practical side, the chosen baseline is very poor. The authors only experiment with MNIST dataset. The baseline model lacks both \"batch normalization\" and \"dropout\", which I guess is because otherwise the proposed method would under-perform against the baseline. It is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baseline.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving generalization with Wasserstein regularization","abstract":"Natural gradients are expensive to calculate but have been shown to both improve convergence speed and generalization. We point out that the natural gradient is the optimal update when one regularizes the Kullbeck-Leibler divergence between the output distributions of successive updates. The natural gradient can thus be seen as a regularization term upon the change in the parameterized function. With this intuition, we propose it is possible to achieve the same effect more efficiently by choosing and regularizing a simpler metric of two distributions' similarity. The resulting algorithm, which we term Wasserstein regularization, explicitly penalizes changes in predictions on a held-out set. It can be interpreted as a way of regularizing that encourages simple functions. Experiments show that the Wasserstein regularization is efficient and leads to considerably better generalization.","pdf":"/pdf/8136b4f93b54ad266acbedc42f1ef4505bc0d114.pdf","TL;DR":"We show that the natural gradient, which underlies many successful practices, is a regularization term upon changes in the function outputs. Wasserstein regularization is cheaper and easier to compute but performs the same task.","paperhash":"anonymous|improving_generalization_with_wasserstein_regularization","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving generalization with Wasserstein regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1l8sz-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper921/Authors"],"keywords":["natural gradient","generalization","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222819128,"tcdate":1510842377228,"number":2,"cdate":1510842377228,"id":"S1-zlmikf","invitation":"ICLR.cc/2018/Conference/-/Paper921/Official_Review","forum":"H1l8sz-AW","replyto":"H1l8sz-AW","signatures":["ICLR.cc/2018/Conference/Paper921/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Ideas based on WGAN but for NN training; some experiments show some improvement but they are not complete/convincing. Rejection","rating":"4: Ok but not good enough - rejection","review":"Summary: The paper considers the use of natural gradients for learning. The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training. The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.\n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions.\n2. Regularizing the objective with KL div. seems promising, but expensive.\n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix.\n4. However, computing and inverting the Fisher information matrix is computationally expensive.\n5. One solution is to approximate the solution F^{-1} J using gradient descent. However, still we need to calculate F. There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').\n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \"good\" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective. \n7. No large matrices need to be formed or inverted, however more passes needed per outer step.\n\nImportance:\nSomewhat lack of originality and poor experiments lead to low importance.\n\nClarity:\nThe paper needs major revision w.r.t. presenting and highlighting the new main points. E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS).\n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks. Beyond this, the paper does not provide any futher original idea. So, slight to no novelty.\n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help? This would require the combination of two Hessian matrices.\n\n2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results. \n\nOverall:\nRejection.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving generalization with Wasserstein regularization","abstract":"Natural gradients are expensive to calculate but have been shown to both improve convergence speed and generalization. We point out that the natural gradient is the optimal update when one regularizes the Kullbeck-Leibler divergence between the output distributions of successive updates. The natural gradient can thus be seen as a regularization term upon the change in the parameterized function. With this intuition, we propose it is possible to achieve the same effect more efficiently by choosing and regularizing a simpler metric of two distributions' similarity. The resulting algorithm, which we term Wasserstein regularization, explicitly penalizes changes in predictions on a held-out set. It can be interpreted as a way of regularizing that encourages simple functions. Experiments show that the Wasserstein regularization is efficient and leads to considerably better generalization.","pdf":"/pdf/8136b4f93b54ad266acbedc42f1ef4505bc0d114.pdf","TL;DR":"We show that the natural gradient, which underlies many successful practices, is a regularization term upon changes in the function outputs. Wasserstein regularization is cheaper and easier to compute but performs the same task.","paperhash":"anonymous|improving_generalization_with_wasserstein_regularization","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving generalization with Wasserstein regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1l8sz-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper921/Authors"],"keywords":["natural gradient","generalization","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222819172,"tcdate":1510555022242,"number":1,"cdate":1510555022242,"id":"H1L5a2I1z","invitation":"ICLR.cc/2018/Conference/-/Paper921/Official_Review","forum":"H1l8sz-AW","replyto":"H1l8sz-AW","signatures":["ICLR.cc/2018/Conference/Paper921/AnonReviewer3"],"readers":["everyone"],"content":{"title":"nice but not solid","rating":"4: Ok but not good enough - rejection","review":"SUMMARY:\n\nThe authors propose a new variant of (stochastic) gradient descent, the steps of which are regularized with the earth mover's distance instead of Kullback-Leibler divergence to obtain a computationally tractable natural gradient approximation.\n\n\nGENERAL IMPRESSION:\n\nI like the idea and the approach of the paper. However, the authors fail to convince me of its value. The main argument is practical performance, but the empirical evaluation is by far insufficient to be convincing.\n\n\nCRITICISM:\n\nI dislike that there is no clear distinction between the roles of optimization and regularization in this paper. Often in learning, a \"static\" regularization term is added, modifying the original optimization problem, with the goal to obtain a smoother solution. However, here the optimization problem to solve remains unaltered, and instead the way steps are computed is changed by introducing a per-step penalty. To me the central question is: does this method still converge to the same solution, say, given a convex problem? I sure hope so. But it should approach the optimum on a different path: instead of following the vanilla gradient, directions are rescaled so that the distribution maintains more entropy for a longer time. And in a highly multi-modal problem like neural network training this probably means that it converges to a different local optimum. These two different issues of 1. modifying the problem and its optimum and 2. modifying the optimization strategy and hence the path really need to be disentangled very carefully. The paper fails to make this distinction at all.\n\nThe method is completely general, and still it is evaluated only on a single problem! There is no excuse for this. It could easily be applied to a dozen of classic data sets, not necessarily from the computer vision domain, just what's found in the UCI repository. Also, different network architectures could be tested, CNNs in particular. Why use an architecture that is known to be sub-optimal on purpose? Why not show the value of the method on state-of-the-art problems, and more importantly, why not use a whole battery of benchmark data sets?\n\nThe method has parameters: \\lambda and n. Do they need problem specific tuning? This question cannot be answered, simply because there is only a single experiment. Again, this makes me question the general value of the method.\n\n\nMINOR POINTS:\n\npage 1: \"can be seen also be seen\" ?\n\npage 3: \"a array\" -> \"an array\"\n\npage 4: \"If the norm is an l_2 norm we can be guaranteed convergence.\" - please explain your reasoning or cite a source.\n\npage 4: \"a difference batch\" -> \"a different batch\"\n\npage 7: \"in that it likely a function that\" -- \"is\" missing?\n\nThe citations around the middle of page 7 should use parenthesis (\\citep).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Improving generalization with Wasserstein regularization","abstract":"Natural gradients are expensive to calculate but have been shown to both improve convergence speed and generalization. We point out that the natural gradient is the optimal update when one regularizes the Kullbeck-Leibler divergence between the output distributions of successive updates. The natural gradient can thus be seen as a regularization term upon the change in the parameterized function. With this intuition, we propose it is possible to achieve the same effect more efficiently by choosing and regularizing a simpler metric of two distributions' similarity. The resulting algorithm, which we term Wasserstein regularization, explicitly penalizes changes in predictions on a held-out set. It can be interpreted as a way of regularizing that encourages simple functions. Experiments show that the Wasserstein regularization is efficient and leads to considerably better generalization.","pdf":"/pdf/8136b4f93b54ad266acbedc42f1ef4505bc0d114.pdf","TL;DR":"We show that the natural gradient, which underlies many successful practices, is a regularization term upon changes in the function outputs. Wasserstein regularization is cheaper and easier to compute but performs the same task.","paperhash":"anonymous|improving_generalization_with_wasserstein_regularization","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving generalization with Wasserstein regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1l8sz-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper921/Authors"],"keywords":["natural gradient","generalization","optimization"]}},{"tddate":null,"ddate":null,"tmdate":1510092385761,"tcdate":1509137224768,"number":921,"cdate":1510092362357,"id":"H1l8sz-AW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1l8sz-AW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Improving generalization with Wasserstein regularization","abstract":"Natural gradients are expensive to calculate but have been shown to both improve convergence speed and generalization. We point out that the natural gradient is the optimal update when one regularizes the Kullbeck-Leibler divergence between the output distributions of successive updates. The natural gradient can thus be seen as a regularization term upon the change in the parameterized function. With this intuition, we propose it is possible to achieve the same effect more efficiently by choosing and regularizing a simpler metric of two distributions' similarity. The resulting algorithm, which we term Wasserstein regularization, explicitly penalizes changes in predictions on a held-out set. It can be interpreted as a way of regularizing that encourages simple functions. Experiments show that the Wasserstein regularization is efficient and leads to considerably better generalization.","pdf":"/pdf/8136b4f93b54ad266acbedc42f1ef4505bc0d114.pdf","TL;DR":"We show that the natural gradient, which underlies many successful practices, is a regularization term upon changes in the function outputs. Wasserstein regularization is cheaper and easier to compute but performs the same task.","paperhash":"anonymous|improving_generalization_with_wasserstein_regularization","_bibtex":"@article{\n  anonymous2018improving,\n  title={Improving generalization with Wasserstein regularization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1l8sz-AW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper921/Authors"],"keywords":["natural gradient","generalization","optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}