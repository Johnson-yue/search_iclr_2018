{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222746551,"tcdate":1511967539076,"number":2,"cdate":1511967539076,"id":"S1iEoBnlf","invitation":"ICLR.cc/2018/Conference/-/Paper768/Official_Review","forum":"r1ISxGZRb","replyto":"r1ISxGZRb","signatures":["ICLR.cc/2018/Conference/Paper768/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper presents important and timely problem of lifelong learning under resource constraints; the manuscript lacks clarity and structure; limited novelty.","rating":"5: Marginally below acceptance threshold","review":"This paper addresses lifelong learning setting under resource constraints, i.e. how to efficiently manage the storage and how to generalise well with a relatively small diversity of prior experiences. The authors investigate how to avoid storing a lot of original training data points while avoiding catastrophic forgetting at the same time.\nThe authors propose a complex neural network architecture that has several components. One of the components is a variational autoencoder with discrete latent variables, where the recently proposed Gumbel-softmax distribution is used to efficiently draw samples from a categorical distribution (Jang et al ICLR 2017). Discrete variables are categorical latent variables using 1-hot encoding of the class variables. In fact, in the manuscript, the authors describe one-hot encoding of c classes as l-dimensional representation. Why is it not c-dimentional? Also the class probabilities p_i are not defined in (7). \nThis design choice is reasonable, as autoencoder with categorical latent variables can achieve more storage compression of input observations in comparison with autoencoders with continuos variables. \nAnother component of the proposed model is a recollection buffer/generator, a generative module (alongside the main model) which produces pseudo-experiences. These self generated pseudo experiences are sampled from the buffer and are combined with available real samples during training to avoid catastrophic forgetting of prior experiences. This module is inspired by episodic training proposed by Lopez-Paz and Ranzato in ICLR2017 for continual learning. In fact, a recollection buffer for MNIST benchmark has 50K codes to store. How fast would it grow with more tasks/training data? Is it suitable for lifelong learning? \n\nMy main concern with this paper is that it is not easy to grasp the gist of it. The paper is 11 pages long and often has sections with weakly related motivations described in details (essentially it would be good to cut the first 6 pages into half and concentrate on the relevant aspects only). It is easy to get lost in unimportant details, where as important details on model components are not very clear and not structured. Second concern is limited novelty (from what I understood). \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning","abstract":"Deep lifelong learning systems need to efficiently manage resources to scale to large numbers of experiences and non-stationary goals. In this paper, we explore the relationship between lossy compression and the resource constrained lifelong learning problem of function transferability. We demonstrate that lossy episodic experience storage can enable efficient function transferability between different architectures and algorithms at a fraction of the storage cost of lossless storage. This is achieved by introducing a generative knowledge distillation strategy that does not store any full training examples. As an important extension of this idea, we show that lossy recollections stabilize deep networks much better than lossless sampling in resource constrained settings of lifelong learning while avoiding catastrophic forgetting. For this setting, we propose a novel dual purpose recollection buffer used to both stabilize the recollection generator itself and an accompanying reasoning model. ","pdf":"/pdf/08cb0a2adc2537ef36720cb17a3e082c7d950497.pdf","paperhash":"anonymous|generation_and_consolidation_of_recollections_for_efficient_deep_lifelong_learning","_bibtex":"@article{\n  anonymous2018generation,\n  title={Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ISxGZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper768/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222746595,"tcdate":1511770826142,"number":1,"cdate":1511770826142,"id":"ryfA9SYez","invitation":"ICLR.cc/2018/Conference/-/Paper768/Official_Review","forum":"r1ISxGZRb","replyto":"r1ISxGZRb","signatures":["ICLR.cc/2018/Conference/Paper768/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Recollections for efficient deep lifelong learning","rating":"5: Marginally below acceptance threshold","review":"The paper proposes an architecture for efficient deep lifelong learning. The key idea is to use recollection generator (autoencoder) to remember the previously processed data in a compact representation. Then when training a reasoning model, recollections generated from the recollection generator are used with real-world examples as input data. Using the recollection, it can avoid forgetting previous data. In the experiments, it has been shown that the proposed approach is efficient for transfer knowledge with small data compared to random sampling approach.\n\nIt is an interesting idea to remember previous examples using the compact representation from autoencoder and use it for transfer learning. However, I think the paper would be improved if the following points are clarified.\n\n1. It seems that reconstructed data from autoencoder does not contain target values. It is not clear to me how the reasoning model can use the reconstructed data (recollections) for supervised learning tasks. \n\n2. It seems that the proposed framework can be better presented as a method for data compression for deep learning. Ideally, for lifelong learning, the reasoning model should not forget previously learned kwnoledge embeded in their weights. \nHowever, under the current architecture, it seems that the reasoning model does not have such mechanisms.\n\n3. For lifelong learning, it would be interesting to test if the same reasoning model can deal with increasing number of tasks from different datasets using the recollection mechanisms.\n\n \n\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning","abstract":"Deep lifelong learning systems need to efficiently manage resources to scale to large numbers of experiences and non-stationary goals. In this paper, we explore the relationship between lossy compression and the resource constrained lifelong learning problem of function transferability. We demonstrate that lossy episodic experience storage can enable efficient function transferability between different architectures and algorithms at a fraction of the storage cost of lossless storage. This is achieved by introducing a generative knowledge distillation strategy that does not store any full training examples. As an important extension of this idea, we show that lossy recollections stabilize deep networks much better than lossless sampling in resource constrained settings of lifelong learning while avoiding catastrophic forgetting. For this setting, we propose a novel dual purpose recollection buffer used to both stabilize the recollection generator itself and an accompanying reasoning model. ","pdf":"/pdf/08cb0a2adc2537ef36720cb17a3e082c7d950497.pdf","paperhash":"anonymous|generation_and_consolidation_of_recollections_for_efficient_deep_lifelong_learning","_bibtex":"@article{\n  anonymous2018generation,\n  title={Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ISxGZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper768/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739113607,"tcdate":1509134398204,"number":768,"cdate":1509739110932,"id":"r1ISxGZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r1ISxGZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning","abstract":"Deep lifelong learning systems need to efficiently manage resources to scale to large numbers of experiences and non-stationary goals. In this paper, we explore the relationship between lossy compression and the resource constrained lifelong learning problem of function transferability. We demonstrate that lossy episodic experience storage can enable efficient function transferability between different architectures and algorithms at a fraction of the storage cost of lossless storage. This is achieved by introducing a generative knowledge distillation strategy that does not store any full training examples. As an important extension of this idea, we show that lossy recollections stabilize deep networks much better than lossless sampling in resource constrained settings of lifelong learning while avoiding catastrophic forgetting. For this setting, we propose a novel dual purpose recollection buffer used to both stabilize the recollection generator itself and an accompanying reasoning model. ","pdf":"/pdf/08cb0a2adc2537ef36720cb17a3e082c7d950497.pdf","paperhash":"anonymous|generation_and_consolidation_of_recollections_for_efficient_deep_lifelong_learning","_bibtex":"@article{\n  anonymous2018generation,\n  title={Generation and Consolidation of Recollections for Efficient Deep Lifelong Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ISxGZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper768/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}