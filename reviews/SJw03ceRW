{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222625139,"tcdate":1511814433024,"number":3,"cdate":1511814433024,"id":"BJK7re9ez","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The paper proposes a network/classifier expansion method to learn to classify with additional novel  classes in the future, without re-training with all the original data. It fine tunes the new parameters added with the new data (from novel classes), and with sampled examples from  simple generative models of the old classes. Overall the paper has a simple idea which is validated on limited settings (~10 classes only). ","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a method for adapting a pre-trained network, trained on a fixed number of\nclasses, to incorporate novel classes for doing classification, especially when the novel classes\nonly have a few training examples available. They propose to do a `hard' distillation, i.e. they\nintroduce new nodes and parameters to the network to add the new classes, but only fine-tune the new\nnetworks without modifying the original parameters. This ensures that, in the new expanded and\nfine-tuned network, the class confusions will only be between the old and new classes and not\nbetween the old classes, thus avoiding catastrophic forgetting. In addition they use GMMs trained on\nthe old classes during the fine-tuning process, thus avoiding saving all the original training data.\nThey show experiments on public benchmarks with three different scenarios, i.e.  base and novel\nclasses from different domains, base and novel classes from the same domain and novel classes have\nsimilarities among themselves, and base and novel classes from the same domain and each novel class\nhas similarities with at least one of the base class.                        \n                                                                             \n- The paper is generally well written and it is clear what is being done     \n- The idea is simple and novel; to the best of my knowledge it has not been tested before\n- The method is compared with Nearest Class Means (NCM) and Prototype-kNN with soft distillation\n  (iCARL; where all weights are fine-tuned). The proposed method performs better in low-shot\n  settings and comparably when large number of training examples of the novel classes are available\n- My main criticism will be the limited dataset size on which the method is validated. The ILSVRC12\n  subset contains 5 base and 5 novel classes and the UT-Zappos50K subset also has 10 classes. The\n  idea is simple and novel, which is good, but the validation is limited and far from any realistic\n  use. Having only O(10) classes is not convincing, especially when the datasets used do have large\n  number of classes. I agree that this will not allow or will takes some involved manual effort to\n  curate subsets for the settings proposed, but it is necessary for being convincing.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1512222625175,"tcdate":1511775225175,"number":2,"cdate":1511775225175,"id":"r1W-h8Flz","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting, yet (by far) not sufficiently explored","rating":"4: Ok but not good enough - rejection","review":"The goal of this paper is to study generalisation to novel classes. This paper stipulates some interesting ideas, using an idea of expansion layers (using a form of hard distillation, where the weights of known classes are fixed), a GMM to model the already learned classes (to reduce storage), and a form of gradient dropout (updating just a subset of the weights using a dropout mask). All of these assume a fixed representation, trained on the base classifier, then only the final classification layer is adjusted for the novel examples. \n\nThe major drawback is that none of these ideas are fully explored. Given fixed representation, for example the influence of forgetting on base classes, the number of components used in the GMM, the influence of the low-shot, the dropout rate, etc etc.  The second major drawback is that the experimental setting seems very unrealistic: 5 base classes and 2 novel classes. \n\nTo conclude: the ideas in this paper are very interesting, but difficult to gather insights given the focus of the experiments.\n\nMinor remarks\n- Sect 4.1 \"The randomly ... 5 novel classes\" is not a correct sentence.\n- The extended version of NCM (4.2.1), here uses as prototype-kNN (Hastie 2001) has also been explored in the paper of NCM, using k-means per class to extract prototypes.\n- Given fixed representations, plenty of work has focused on few-shot (linear) learning, this work should be compared to these. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1512222625217,"tcdate":1511659222261,"number":1,"cdate":1511659222261,"id":"r1R0L9Def","invitation":"ICLR.cc/2018/Conference/-/Paper359/Official_Review","forum":"SJw03ceRW","replyto":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference/Paper359/AnonReviewer3"],"readers":["everyone"],"content":{"title":"a paper proposing hard-distillation for few-shot learning ","rating":"6: Marginally above acceptance threshold","review":"On few-shot learning problem, this paper presents a simple yet powerful distillation method where the base network is augmented with additional weights to classify the novel classes, while keeping the weights of the base network unchanged. Thus the so-called hard distillation is proposed. This paper is well-written and well organized. The good points are as follows,\n\n1. The paper proposes a well-performance method for the important low-shot learning problem based on the transform learning.\n2. The Gen-LSNE maintains a small memory footprint using a generative model for base examples and requires a few more parameters to avoid overfitting and take less time to train.\n3. This paper builds up a benchmark for low-shot network expansion.\n\nThere are some problems,\n1. There still is drop in accuracy on the base classes after adding new classes, and the accuracy may still drop as adding more classes due to the fixed parameters corresponding to the base classes. This is slightly undesired.\n2. Grammatical mistake: page 3, line 5(“a additional layers”)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]}},{"tddate":null,"ddate":null,"tmdate":1509739344982,"tcdate":1509104846800,"number":359,"cdate":1509739342325,"id":"SJw03ceRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJw03ceRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GENERATIVE LOW-SHOT NETWORK EXPANSION","abstract":"Conventional deep learning classifiers are static in the sense that they are trained on\na predefined set of classes and learning to classify a novel class typically requires\nre-training. In this work, we address the problem of Low-shot network-expansion\nlearning. We introduce a learning framework which enables expanding a pre-trained\n(base) deep network to classify novel classes when the number of examples for the\nnovel classes is particularly small. We present a simple yet powerful distillation\nmethod where the base network is augmented with additional weights to classify\nthe novel classes, while keeping the weights of the base network unchanged. We\nterm this learning hard distillation, since we preserve the response of the network\non the old classes to be equal in both the base and the expanded network. We\nshow that since only a small number of weights needs to be trained, the hard\ndistillation excels for low-shot training scenarios. Furthermore, hard distillation\navoids detriment to classification performance on the base classes. Finally, we\nshow that low-shot network expansion can be done with a very small memory\nfootprint by using a compact generative model of the base classes training data\nwith only a negligible degradation relative to learning with the full training set.","pdf":"/pdf/8011e34779de8251fb5437de8271b45ae05d4ea3.pdf","TL;DR":" In this paper, we address the problem of Low-shot network-expansion learning","paperhash":"anonymous|generative_lowshot_network_expansion","_bibtex":"@article{\n  anonymous2018generative,\n  title={GENERATIVE LOW-SHOT NETWORK EXPANSION},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJw03ceRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper359/Authors"],"keywords":["Low-Shot Learning","class incremental learning","Network expansion","Generative model","Distillation"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}