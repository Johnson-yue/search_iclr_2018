{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222798114,"tcdate":1511655649839,"number":2,"cdate":1511655649839,"id":"rk9JKKwxM","invitation":"ICLR.cc/2018/Conference/-/Paper866/Official_Review","forum":"HklZOfW0W","replyto":"HklZOfW0W","signatures":["ICLR.cc/2018/Conference/Paper866/AnonReviewer1"],"readers":["everyone"],"content":{"title":"rejection","rating":"3: Clear rejection","review":"There are many language issues rendering the text hard to understand, e.g.,\n-- in the abstract: \"several convolution on graphs architectures\"\n-- in the definitions: \"Let data with N observation\" (no verb, no plural, etc).\n-- in the computational section: \"Training size is 9924 and testing is 6695. \"\nso part of my negative impression may be pure mis-understanding of what\nthe authors had to say. \n\nStill, the authors clearly utilise basic concepts (c.f. \"utilize eigenvector \nbasis of the graph Laplacian to do filtering in the Fourier domain\") in ways\nthat do not seem to have any sensible interpretation whatsoever, even allowing\nfor the mis-understanding due to grammar. There are no clear insight, \nno theorems, and an empirical evaluation on an ill-defined problem in \ntime-series forecasting. (How does it relate to graphs? What is the graph \nin the time series or among the multiple time series? How do the authors\nimplement the other graph-related approaches in this problem featuring\ntime series?) My impression is hence that the only possible outcome is\n\nrejection.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UPS: optimizing Undirected Positive Sparse graph for neural graph filtering","abstract":"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation. Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain. We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity. Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","pdf":"/pdf/5da9203be1405a0943822a21379af2566d2ef8fa.pdf","TL;DR":"Graph Optimization with signal filtering in the vertex domain.","paperhash":"anonymous|ups_optimizing_undirected_positive_sparse_graph_for_neural_graph_filtering","_bibtex":"@article{\n  anonymous2018ups:,\n  title={UPS: optimizing Undirected Positive Sparse graph for neural graph filtering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HklZOfW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper866/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222798159,"tcdate":1511288745406,"number":1,"cdate":1511288745406,"id":"HJb3ygfxf","invitation":"ICLR.cc/2018/Conference/-/Paper866/Official_Review","forum":"HklZOfW0W","replyto":"HklZOfW0W","signatures":["ICLR.cc/2018/Conference/Paper866/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Authors of this paper built a neural network architecture compatible with the novel approach for learning graph representation of the data, motivated by graph filtering in the vertex domain. The learned graph is demonstrated to be richer structures than nearest neighbor graphs.","rating":"6: Marginally above acceptance threshold","review":"Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper. A projected sub-gradient descent algorithm is used. The UPS optimizer by itself is not new.\n\nGraph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter. First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. There are some concerns about this network that need to be clarified:\n1. sigma is never clarified in the main context or experiments\n2. the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random.\n3. another explanation about the weights as the rescaling to matrix A needs to further clarified. As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction. What is the disagreement between L1 penalty and prediction quality? Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?\n4. Authors stated that the last step is to build a mapping from the GPS features into the response Y. They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features. However, no detailed information is given in the paper. In the experiments, authors only stated that “we fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph”, and then the graph is used to train existing models as the input of the graph. Which architecture is used for building the mapping ?\n\nIn the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified. For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on. More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"UPS: optimizing Undirected Positive Sparse graph for neural graph filtering","abstract":"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation. Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain. We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity. Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","pdf":"/pdf/5da9203be1405a0943822a21379af2566d2ef8fa.pdf","TL;DR":"Graph Optimization with signal filtering in the vertex domain.","paperhash":"anonymous|ups_optimizing_undirected_positive_sparse_graph_for_neural_graph_filtering","_bibtex":"@article{\n  anonymous2018ups:,\n  title={UPS: optimizing Undirected Positive Sparse graph for neural graph filtering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HklZOfW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper866/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739060585,"tcdate":1509136376063,"number":866,"cdate":1509739057927,"id":"HklZOfW0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HklZOfW0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"UPS: optimizing Undirected Positive Sparse graph for neural graph filtering","abstract":"In this work we propose a novel approach for learning graph representation of the data using gradients obtained via backpropagation. Next we build a neural network architecture compatible with our optimization approach and motivated by graph filtering in the vertex domain. We demonstrate that the learned graph has richer structure than often used nearest neighbors graphs constructed based on features similarity. Our experiments demonstrate that we can improve prediction quality for several convolution on graphs architectures, while others appeared to be insensitive to the input graph.","pdf":"/pdf/5da9203be1405a0943822a21379af2566d2ef8fa.pdf","TL;DR":"Graph Optimization with signal filtering in the vertex domain.","paperhash":"anonymous|ups_optimizing_undirected_positive_sparse_graph_for_neural_graph_filtering","_bibtex":"@article{\n  anonymous2018ups:,\n  title={UPS: optimizing Undirected Positive Sparse graph for neural graph filtering},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HklZOfW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper866/Authors"],"keywords":[]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}