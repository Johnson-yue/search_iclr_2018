{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222626727,"tcdate":1511820369290,"number":3,"cdate":1511820369290,"id":"rkY82b5lM","invitation":"ICLR.cc/2018/Conference/-/Paper368/Official_Review","forum":"B1al7jg0b","replyto":"B1al7jg0b","signatures":["ICLR.cc/2018/Conference/Paper368/AnonReviewer3"],"readers":["everyone"],"content":{"title":"contribution unclear","rating":"5: Marginally below acceptance threshold","review":"The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report. Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference. Figure 2 is almost identical, again a reference to the original would be better.\nConceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression. What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach. The fact that additional conceptors can be trained does not appear new for the approach described here. If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version.\n\nThe evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting. The evaluation shows that the approach performs better classifying MNIST digits than another approach. This is nice but doesn't really tell me much about overcoming catastrophic forgetting. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation","abstract":"Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","pdf":"/pdf/0e18b4a1505f05c171a73454246eafb613749a6b.pdf","TL;DR":"We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.","paperhash":"anonymous|overcoming_catastrophic_interference_using_conceptoraided_backpropagation","_bibtex":"@article{\n  anonymous2018overcoming,\n  title={Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1al7jg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper368/Authors"],"keywords":["Catastrophic Interference","Conceptor","Backpropagation","Continual Learning","Lifelong Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222626761,"tcdate":1511786942314,"number":2,"cdate":1511786942314,"id":"HkUpFYKeM","invitation":"ICLR.cc/2018/Conference/-/Paper368/Official_Review","forum":"B1al7jg0b","replyto":"B1al7jg0b","signatures":["ICLR.cc/2018/Conference/Paper368/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Rejected based on violation of blind review process","rating":"4: Ok but not good enough - rejection","review":"Before any review, I need to attract attention to the disregard demonstrated by the authors for the ICLR double-blind review process. A technical report by the same name and mostly the same content was posted on Arxiv by Xe and Jaeger earlier this year. We are either dealing with a case of incredible academic rip-off (which, were I to play dumb, I could use as an excuse to reject this article), or a mockery of the blind review process. Since the authors did not take the care to erase the name and number of their research grant to Pr Jaeger, and that most of the citations in the text are to work by Pr Jaeger, we clearly are facing the second case. This could be explained if it came from newcomers in the field, but I want to believe that such a respected and experienced contributor to the field as Pr Jaeger knows what he is doing. If he disapproves of blind reviews, this is entirely his prerogative (and there is indeed a lot to be said on this topic, see for example Luc Devroye’s stance against blind reviewing [http://luc.devroye.org/blindreferee.html ]), but consistency would then recommend to not submit to ICLR. I should emphasize this is the only submission that so blatantly ignored the reviewing process out of the four this reviewer was assigned for ICLR18.\n\nBecause this violation of double blind-review corrupts the intended process and makes it unfair to other colleagues who are more respectful of the rules, I would recommend that the Area Chairs reject this paper, leaving it to the authors to submit it somewhere non-blind reviews are the norm.\n\nBelow is the review I would have otherwise written.\n\nThis article applies the notion of “conceptors” -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks. It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation.  After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation. Follows a section of experiments on variants of MNIST commonly used for continual learning.\n\nContinual learning in neural networks is a hot topic, and this article contributes a very interesting idea. The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.  The numeric examples, although quite toy, provide a clear illustration.\n\nA few things are still missing to back the strong claims of this paper:\n* Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer.  This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures.\n* Some examples beyond the contrived MNIST toy examples would be welcome. For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation. \n\n\nIt could also be welcome to use a more grounded vocabulary, e.g. on p.2 “Figure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources” could be much more simply said as “Figure 1 shows the ellipses corresponding to three sets of R^3 points”. Being less grandiose would make the value of this article nicely on its own.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation","abstract":"Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","pdf":"/pdf/0e18b4a1505f05c171a73454246eafb613749a6b.pdf","TL;DR":"We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.","paperhash":"anonymous|overcoming_catastrophic_interference_using_conceptoraided_backpropagation","_bibtex":"@article{\n  anonymous2018overcoming,\n  title={Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1al7jg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper368/Authors"],"keywords":["Catastrophic Interference","Conceptor","Backpropagation","Continual Learning","Lifelong Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222626800,"tcdate":1511697572523,"number":1,"cdate":1511697572523,"id":"rkhi37_gz","invitation":"ICLR.cc/2018/Conference/-/Paper368/Official_Review","forum":"B1al7jg0b","replyto":"B1al7jg0b","signatures":["ICLR.cc/2018/Conference/Paper368/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This is an interesting method for continual learning. It relies mostly on conceptors, Linear Algebra method, for minimizing the interference of new task to the already learned tasks. ","rating":"7: Good paper, accept","review":"This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors. This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \"unused\" until the present task in hand.\n\nThe paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem. In Section 2 the authors review conceptors. This method is algebraic method closely related to spanning sub spaces and SVD. The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally. In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation. The authors provide a version with batch SGD as well.\n\nIn Section 4, the authors show their method on permuted MNIST. They compare the method to EWC with the same architecture. They show that their method more efficiently suffers on permuted MNIST from less degradation. Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance.\n\nIn general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life. Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\nI think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.  What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods. Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation","abstract":"Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","pdf":"/pdf/0e18b4a1505f05c171a73454246eafb613749a6b.pdf","TL;DR":"We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.","paperhash":"anonymous|overcoming_catastrophic_interference_using_conceptoraided_backpropagation","_bibtex":"@article{\n  anonymous2018overcoming,\n  title={Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1al7jg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper368/Authors"],"keywords":["Catastrophic Interference","Conceptor","Backpropagation","Continual Learning","Lifelong Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739340085,"tcdate":1509106421269,"number":368,"cdate":1509739337427,"id":"B1al7jg0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1al7jg0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation","abstract":"Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, \"Conceptor-Aided Backprop\" (CAB), in which gradients are shielded by conceptors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.","pdf":"/pdf/0e18b4a1505f05c171a73454246eafb613749a6b.pdf","TL;DR":"We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.","paperhash":"anonymous|overcoming_catastrophic_interference_using_conceptoraided_backpropagation","_bibtex":"@article{\n  anonymous2018overcoming,\n  title={Overcoming Catastrophic Interference using Conceptor-Aided Backpropagation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1al7jg0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper368/Authors"],"keywords":["Catastrophic Interference","Conceptor","Backpropagation","Continual Learning","Lifelong Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}