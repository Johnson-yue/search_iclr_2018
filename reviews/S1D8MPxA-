{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222614715,"tcdate":1511741761488,"number":3,"cdate":1511741761488,"id":"SktHYC_xM","invitation":"ICLR.cc/2018/Conference/-/Paper288/Official_Review","forum":"S1D8MPxA-","replyto":"S1D8MPxA-","signatures":["ICLR.cc/2018/Conference/Paper288/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The authors use Viterbi encoding to dramatically compress the sparse matrix index of a pruned network, reducing one of the main memory overheads of a pruned neural network and speeding up inference in the parallel setting.","rating":"7: Good paper, accept","review":"quality: this paper is of good quality\nclarity: this paper is very clear but contains a few minor typos/grammatical mistakes (missing -s for plurals, etc.)\noriginality: this paper is original\nsignificance: this paper is significant\n\nPROS\n- Using ECC theory for reducing the memory footprint of a neural network seems both intuitive and innovative, while being grounded in well-understood theory.\n- The authors address a consequence of current approaches to neural network pruning, i.e., the high cost of sparse matrix index storage.\n- The results are extensive and convincing.\n\nCONS\n- The authors mention in the introduction that this encoding can speed up inference by allowing efficient parallel sparse-to-dense matrix conversion, and hence batch inference, but do not provide any experimental confirmation.\n\nMain questions\n- It is not immediately clear to me why the objective function (2) correlates to a good accuracy of the pruned network. Did you try out other functions before settling on this one, or is there a larger reason for which (2) is a logical choice? \n- On a related note, I would find a plot of the final objective value assigned to a pruning scheme compared to the true network accuracy very helpful in understanding how these two correlate.\n- Could this approach be generalized to RNNs?\n- How long does the Viterbi pruning algorithm take, as it explores all 2^p possible prunings?\n- How difficult is it to tune the pruning algorithm hyper-parameters?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio","abstract":"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","pdf":"/pdf/7767c0e3687c37b11545ddc6f7abe5ec9645991f.pdf","TL;DR":"We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process.","paperhash":"anonymous|viterbibased_pruning_for_sparse_matrix_with_fixed_and_high_index_compression_ratio","_bibtex":"@article{\n  anonymous2018viterbi-based,\n  title={Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1D8MPxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper288/Authors"],"keywords":["pruning","sparse matrix","memory footprint","model size","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222614757,"tcdate":1511726312228,"number":2,"cdate":1511726312228,"id":"BJle65dxG","invitation":"ICLR.cc/2018/Conference/-/Paper288/Official_Review","forum":"S1D8MPxA-","replyto":"S1D8MPxA-","signatures":["ICLR.cc/2018/Conference/Paper288/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Viterbi-based pruning review","rating":"5: Marginally below acceptance threshold","review":"The paper proposes VCM, a novel way to store sparse matrices that is based on the Viterbi Decompressor. Only a subset of sparse matrices can be represented in the VCM format, however, unlike CSR format, it allows for faster parallel decoding and requires much less index space. The authors also propose a novel method of pruning of neural network that constructs an (sub)optimal (w.r.t. a weight magnitude based loss) Viterbi-compressed matrix given the weights of a pretrained DNN.\nVCM is an interesting analog to the conventional CSR format that may be more computationally efficient given particular software and/or hardware implementations of the Viterbi Decompressor. However, the empirical study of possible acceleration remains as an open question.\nHowever, I have a major concern regarding the efficiency of the pruning procedure. Authors report practically the same level of sparsity, as the pruning procedure from the Deep Compression paper. Both the proposed Viterbi-based pruning, and Deep Compression pruning belong to the previous era of pruning methods. They separate the pruning procedure and the training procedure, so that the model is not trained end-to-end. However, during the last two years a lot of new adaptive pruning methods have been developed, e.g. Dynamic Network Surgery, Soft Weight Sharing, and Sparse Variational DropOut. All of them in some sense incorporate the pruning procedure into the training procedure and achieve a much higher level of sparsity (e.g. DC achieves ~13x compression of LeNet5, and SVDO achieves ~280x compression of the same network). Therefore the reported 35-50% compression of the index storage is not very significant.\nIt is not clear whether it is possible to take a very sparse matrix and transform it into the VCM format without a high accuracy degradation. It is also not clear whether the VCM format would be efficient for storage of extremely sparse matrices, as they would likely be more sensitive to the mismatch of the original sparsity mask, and the best possible VCM sparsity mask. Therefore Iâ€™m concerned whether it would be possible to achieve a close-to-SotA level of compression using this method, and it is not yet clear whether this method can be used for practical acceleration or not.\nThe paper presents an interesting idea that potentially has useful applications, however the experiments are not convincing enough.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio","abstract":"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","pdf":"/pdf/7767c0e3687c37b11545ddc6f7abe5ec9645991f.pdf","TL;DR":"We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process.","paperhash":"anonymous|viterbibased_pruning_for_sparse_matrix_with_fixed_and_high_index_compression_ratio","_bibtex":"@article{\n  anonymous2018viterbi-based,\n  title={Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1D8MPxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper288/Authors"],"keywords":["pruning","sparse matrix","memory footprint","model size","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1512222614895,"tcdate":1511635059318,"number":1,"cdate":1511635059318,"id":"Hkiuu4PlM","invitation":"ICLR.cc/2018/Conference/-/Paper288/Official_Review","forum":"S1D8MPxA-","replyto":"S1D8MPxA-","signatures":["ICLR.cc/2018/Conference/Paper288/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Solid work but question its usefulness","rating":"6: Marginally above acceptance threshold","review":"It seems like the authors have carefully thought about this problem, and have come up with some elegant solutions, but I am not sold on whether it's an appropriate match for this conference, mainly because it's not clear how many machine learning people will be interested in this approach.\n\nThere was a time about 2 or 3 years ago when sparse-matrix approaches seemed to have a lot of promise, but I get the impression that a lot of people have moved on.  The issue is that it's hard to construct a scenario where it makes sense from a speed or memory standpoint to do this.  The authors seem to have found a way to substantially compress the indexes, but it's not clear to me that this really ends up solving any practical problem.  Towards the end of the paper I see mention of a 38.1% reduction in matrix size.  That is way too little to make sense in any practical application, especially when you consider the overhead of decompression.   It seems to me that you could easily get a factor of 4 to 8 of compression just by finding a suitable way to encode the floating-point numbers in many fewer bits (since the weight parameters are quite Gaussian-distributed and don't need to be that accurate).\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio","abstract":"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","pdf":"/pdf/7767c0e3687c37b11545ddc6f7abe5ec9645991f.pdf","TL;DR":"We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process.","paperhash":"anonymous|viterbibased_pruning_for_sparse_matrix_with_fixed_and_high_index_compression_ratio","_bibtex":"@article{\n  anonymous2018viterbi-based,\n  title={Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1D8MPxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper288/Authors"],"keywords":["pruning","sparse matrix","memory footprint","model size","model compression"]}},{"tddate":null,"ddate":null,"tmdate":1509739384021,"tcdate":1509089870668,"number":288,"cdate":1509739381364,"id":"S1D8MPxA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1D8MPxA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio","abstract":"Weight pruning has proven to be an effective method in reducing the model size and computation cost while not sacrificing the model accuracy. Conventional sparse matrix formats, however, involve irregular index structures with large storage requirement and sequential reconstruction process, resulting in inefficient use of highly parallel computing resources. Hence, pruning is usually restricted to inference with a batch size of one, for which an efficient parallel matrix-vector multiplication method exists. In this paper, a new class of sparse matrix representation utilizing Viterbi algorithm that has a high, and more importantly, fixed index compression ratio regardless of the pruning rate, is proposed. In this approach, numerous sparse matrix candidates are first generated by the Viterbi encoder, and then the one that aims to minimize the model accuracy degradation is selected by the Viterbi algorithm. The model pruning process based on the proposed Viterbi encoder and Viterbi algorithm is highly parallelizable, and can be implemented efficiently in hardware to achieve low-energy, high-performance index decoding process. Compared with the existing magnitude-based pruning methods, index data storage requirement can be further compressed by 85.2% in MNIST and 83.9% in AlexNet while achieving similar pruning rate. Even compared with the relative index compression technique, our method can still reduce the index storage requirement by 52.7% in MNIST and 35.5% in AlexNet.","pdf":"/pdf/7767c0e3687c37b11545ddc6f7abe5ec9645991f.pdf","TL;DR":"We present a new pruning method and sparse matrix format to enable high index compression ratio and parallel index decoding process.","paperhash":"anonymous|viterbibased_pruning_for_sparse_matrix_with_fixed_and_high_index_compression_ratio","_bibtex":"@article{\n  anonymous2018viterbi-based,\n  title={Viterbi-based Pruning for Sparse Matrix with Fixed and High Index Compression Ratio},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1D8MPxA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper288/Authors"],"keywords":["pruning","sparse matrix","memory footprint","model size","model compression"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}