{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222743720,"tcdate":1511936609027,"number":2,"cdate":1511936609027,"id":"BkYvzAixG","invitation":"ICLR.cc/2018/Conference/-/Paper752/Official_Review","forum":"r16Vyf-0-","replyto":"r16Vyf-0-","signatures":["ICLR.cc/2018/Conference/Paper752/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Requires improvement","rating":"3: Clear rejection","review":"This paper extends the PixelCNN/RNN based (conditional) image generation approaches with self-attention mechanism. \n\nPros:\n- qualitatively the proposed method has good results in several tasks\n\nCons:\n- writing needs to be improved\n- lack of motivation\n- not easy to follow technique details\n\n\nThe motivation part is missing. It seems to me that the paper simply try to combine the Transformer with PixelCNN/RNN based image generation without a clear explanation why this is needed. Why self-attention is so important for image generation? Why not just a deeper network with more parameters? Throughout the paper I cannot find a clear answer for this. Based on this I couldn't see a clear contribution. \n\nThe paper is not well-written. Firstly of all it is difficult to keep the track given the current flow. Paragraphs are spitted quite arbitrarily and some sentences are not rigorous. Note that this is an academic paper submission for double-blind review. Each subsection of section 3 starts with technique details without explaining why we do this. \n\nThe experiments lack comparisons except the human evaluation. I am wondering how the human evaluation is conducted. Does it compare all the competing algorithms against the same sub-samples of the GT data? How many pairs have been compared for each algorithm?  Apart from this metric, I would like to see qualitative comparison between competing algorithms in the paper as well. Also other approaches e.g. SRGAN could be compared. \n\nI am also interested about the author's claim that the implementation error that influences the log-likelihood. Has this been fixed after the deadline?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Image Transformer","abstract":"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model based on self-attention, the Transformer, to a sequence modeling formulation of image generation with  a  tractable likelihood. By applying a self-attention mechanism with a limited receptive field multiple times in parallel to different parts of the sequence, we significantly increase the length of sequences the model can process efficiently. We propose another simple extension of self-attention to allow it to take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art. Lastly, we provide examples of images generated or completed by our various models which, following previous work, we also believe to look pretty cool.","pdf":"/pdf/5544f5d9c99385ae716116b51ba7f57e2dc805fa.pdf","paperhash":"anonymous|image_transformer","_bibtex":"@article{\n  anonymous2018image,\n  title={Image Transformer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r16Vyf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper752/Authors"],"keywords":["image generation","super-resolution","self-attention","transformer"]}},{"tddate":null,"ddate":null,"tmdate":1512222743760,"tcdate":1511725141863,"number":1,"cdate":1511725141863,"id":"SyA8u9OlG","invitation":"ICLR.cc/2018/Conference/-/Paper752/Official_Review","forum":"r16Vyf-0-","replyto":"r16Vyf-0-","signatures":["ICLR.cc/2018/Conference/Paper752/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting idea, weak evaluation","rating":"5: Marginally below acceptance threshold","review":"Summary\n\nThis paper extends self-attention layers (Vaswani et al., 2017) from sequences to images and proposes to use the layers as part of PixelCNNs (van den Oord et al., 2016). The proposed model is evaluated in terms of visual appearance of samples and log-likelihoods. The authors find a small improvement in terms of log-likelihood over PixelCNNs and that super-resolved CelebA images are able to fool human observers significantly more often than PixelRNN based super-resolution (Dahl et al., 2017).\n\nReview\n\nAutoregressive models are of large interest to the ICLR community and exploring new architectures is a valuable contribution. Using self-attention in autoregressive models is an intriguing idea. It is a little bit disappointing that the added model complexity only yields a small improvement compared to the more straight-forward modifications of the PixelCNN++. I think the paper would benefit from a little bit more work, but I am open to adjusting my score based on feedback.\n\nI find it somewhat surprising that the proposed model is only slightly better in terms of log-likelihood than a PixelRNN, but much better in terms of human evaluation – given that both models were optimized for log-likelihood. Was the setup used with Mechanical Turk exactly the same as the one used by Dahl et al.? These types of human evaluations can be extremely sensitive to changes in the setup, even the phrasing of the task can influence results. E.g., presenting images scaled differently can mask certain artifacts. In addition, the variance between subjects can be very high. Ideally, each method included in the comparison would be re-evaluated using the same set of observers. Please include error bars.\n\nThe CelebA super-resolution task furthermore seems fairly limited. Given the extreme downsampling of the input, the task becomes similar to simply generating any realistic image. A useful baseline would be the following method: Store the entire training set. For a given query image, look for the nearest neighbor in the downsampled space, then return the corresponding high-resolution image. This trivial method might not only perform well, it also highlights a flaw in the evaluation: Any method which returns stored high-resolution images – even if they don’t match the input – would perform at 50%. To fix this, the human observers should also receive the low-resolution image and be asked to identify the correct corresponding high-resolution image.\n\nUsing multiplicative operations to model images seems important. How does the self-attention mechanism relate to “gated” convolutions used in PixelCNNs? Could gated convolutions not also be considered a form of self-attention?\n\nThe presentation/text could use some work. Much of the text assumes that the reader is familiar with Vaswani et al. (2017) but could easily be made more self-contained by directly including the definitions used. E.g., the encoding of positions using sines and cosines or the multi-head attention model. I also felt too much of the architecture is described in prose and could be more efficiently and precisely conveyed in equations.\n\nOn page 7 the authors write “we believe our cherry-picked images for various classes to be of higher perceptual quality”. This is a meaningless result, not only because the images were cherry-picked. Generating realistic images is trivial - you just need to store the training images. Analyzing samples generated by a generative model (outside the context of an application) should therefore only be used for diagnostic purposes or to build intuitions but not to judge the quality of a model.\n\nPlease consider rephrasing the last sentence of the abstract. Generating images which “look pretty cool” should not be the goal of a serious machine learning paper or a respected machine learning conference.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Image Transformer","abstract":"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model based on self-attention, the Transformer, to a sequence modeling formulation of image generation with  a  tractable likelihood. By applying a self-attention mechanism with a limited receptive field multiple times in parallel to different parts of the sequence, we significantly increase the length of sequences the model can process efficiently. We propose another simple extension of self-attention to allow it to take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art. Lastly, we provide examples of images generated or completed by our various models which, following previous work, we also believe to look pretty cool.","pdf":"/pdf/5544f5d9c99385ae716116b51ba7f57e2dc805fa.pdf","paperhash":"anonymous|image_transformer","_bibtex":"@article{\n  anonymous2018image,\n  title={Image Transformer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r16Vyf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper752/Authors"],"keywords":["image generation","super-resolution","self-attention","transformer"]}},{"tddate":null,"ddate":null,"tmdate":1509739123352,"tcdate":1509134133489,"number":752,"cdate":1509739120696,"id":"r16Vyf-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"r16Vyf-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Image Transformer","abstract":"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model based on self-attention, the Transformer, to a sequence modeling formulation of image generation with  a  tractable likelihood. By applying a self-attention mechanism with a limited receptive field multiple times in parallel to different parts of the sequence, we significantly increase the length of sequences the model can process efficiently. We propose another simple extension of self-attention to allow it to take advantage of the two-dimensional nature of images. While conceptually simple, our generative models trained on two image data sets are competitive with or outperform the current state of the art in autoregressive image generation on two different data sets, CIFAR-10 and ImageNet. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve significantly over previously published autoregressive super-resolution models. Images they generate fool human observers three times more often than the previous state of the art. Lastly, we provide examples of images generated or completed by our various models which, following previous work, we also believe to look pretty cool.","pdf":"/pdf/5544f5d9c99385ae716116b51ba7f57e2dc805fa.pdf","paperhash":"anonymous|image_transformer","_bibtex":"@article{\n  anonymous2018image,\n  title={Image Transformer},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=r16Vyf-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper752/Authors"],"keywords":["image generation","super-resolution","self-attention","transformer"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}