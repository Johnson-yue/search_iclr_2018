{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222812632,"tcdate":1511822968850,"number":2,"cdate":1511822968850,"id":"ByZKLz5gz","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Review","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper","rating":"7: Good paper, accept","review":"This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems. The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems. Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach. In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem. In all three cases, the proposed solution outperforms the baselines on larger problem instances. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on two combinatorial and geometric tasks: convex hull,\nclustering and euclidean TSP. Thanks to the dynamic programming nature of our\nmodel, we show significant improvements in terms of generalization error and\ncomputational complexity","pdf":"/pdf/69e2371a976edc0d109b604168709fdda5c6823b.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1512222812675,"tcdate":1511630590577,"number":1,"cdate":1511630590577,"id":"H1wZwQwef","invitation":"ICLR.cc/2018/Conference/-/Paper907/Official_Review","forum":"B1jscMbAW","replyto":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference/Paper907/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Neural networks enriched with divide and conquer strategy","rating":"5: Marginally below acceptance threshold","review":"This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics. Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients. The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent). Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.  However, while results on convex hull task are good, k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on. However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story.\n\nI will be happy to revisit the rating if the experimental section is enriched.\n\nPros:\n- very easy to follow idea and model\n- simple merge or RL and SL in an end-to-end trainable model\n- improvements over previous solutions\n\nCons:\n- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there. In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed). It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret. With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful. \n- TSP experiments show that \"in distribution\" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success. Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place). \n- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments\n\nSide notes:\n- DCN is already quite commonly used abbreviation for \"Deep Classifier Network\" as well as \"Dynamic Capacity Network\", thus might be a good idea to find different name.\n- please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017) \nshould be\nGraph Neural Network (Nowak et al. (2017))\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on two combinatorial and geometric tasks: convex hull,\nclustering and euclidean TSP. Thanks to the dynamic programming nature of our\nmodel, we show significant improvements in terms of generalization error and\ncomputational complexity","pdf":"/pdf/69e2371a976edc0d109b604168709fdda5c6823b.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]}},{"tddate":null,"ddate":null,"tmdate":1510092386003,"tcdate":1509137061432,"number":907,"cdate":1510092362555,"id":"B1jscMbAW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1jscMbAW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Divide and Conquer Networks","abstract":"We consider the learning of algorithmic tasks by mere observation of input-output\npairs. Rather than studying this as a black-box discrete regression problem with\nno assumption whatsoever on the input-output mapping, we concentrate on tasks\nthat are amenable to the principle of divide and conquer, and study what are its\nimplications in terms of learning.\nThis principle creates a powerful inductive bias that we leverage with neural\narchitectures that are defined recursively and dynamically, by learning two scale-\ninvariant atomic operations: how to split a given input into smaller sets, and how\nto merge two partially solved tasks into a larger partial solution. Our model can be\ntrained in weakly supervised environments, namely by just observing input-output\npairs, and in even weaker environments, using a non-differentiable reward signal.\nMoreover, thanks to the dynamic aspect of our architecture, we can incorporate\nthe computational complexity as a regularization term that can be optimized by\nbackpropagation. We demonstrate the flexibility and efficiency of the Divide-\nand-Conquer Network on two combinatorial and geometric tasks: convex hull,\nclustering and euclidean TSP. Thanks to the dynamic programming nature of our\nmodel, we show significant improvements in terms of generalization error and\ncomputational complexity","pdf":"/pdf/69e2371a976edc0d109b604168709fdda5c6823b.pdf","TL;DR":"Dynamic model that learns divide and conquer strategies by weak supervision.","paperhash":"anonymous|divide_and_conquer_networks","_bibtex":"@article{\n  anonymous2018divide,\n  title={Divide and Conquer Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1jscMbAW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper907/Authors"],"keywords":["Neural Networks","Combinatorial Optimization","Algorithms"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}