{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222697410,"tcdate":1511819281863,"number":3,"cdate":1511819281863,"id":"ByqfOWqlM","invitation":"ICLR.cc/2018/Conference/-/Paper597/Official_Review","forum":"rkEfPeZRb","replyto":"rkEfPeZRb","signatures":["ICLR.cc/2018/Conference/Paper597/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Simple yet efficient new algorithm for gradient compression with good performance.","rating":"7: Good paper, accept","review":"The authors propose a new gradient compression method for efficient distributed training of neural networks. The authors propose a novel way of measuring ambiguity based on the variance of the gradients. In the experiment, the proposed method shows no or slight degradation of accuracy with big savings in communication cost. The proposed method can easily be combined with other existing method, i.e., Storm (2015), based on the absolute value of the gradient and shows further efficiency. \n\nThe paper is well written: clear and easy to understand. The proposed method is simple yet powerful. Particularly, I found it interesting to re-evaluate the variance with (virtually) increasing larger batch size. The performance shown in the experiments is also impressive. \n\nI found it would have also been interesting and helpful to define and show a new metric that incorporates both accuracy and compression rate into a single metric, e.g., how much accuracy is lost (or gained) per compression rate relatively to the baseline of no compression. With this metric, the comparison would be easier and more intuitive. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance-based Gradient Compression for Efficient Distributed Deep Learning","abstract":"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","pdf":"/pdf/d06ff7651ff54a171b7a0fdb38699d6fa1a9d841.pdf","TL;DR":"A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing ‘unambiguous’ gradients.","paperhash":"anonymous|variancebased_gradient_compression_for_efficient_distributed_deep_learning","_bibtex":"@article{\n  anonymous2018variance-based,\n  title={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkEfPeZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper597/Authors"],"keywords":["distributed deep learning","gradient compression","collective communication","data parallel distributed sgd","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222697449,"tcdate":1511811688565,"number":2,"cdate":1511811688565,"id":"rkZd9y9xz","invitation":"ICLR.cc/2018/Conference/-/Paper597/Official_Review","forum":"rkEfPeZRb","replyto":"rkEfPeZRb","signatures":["ICLR.cc/2018/Conference/Paper597/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Ok but not good enough","rating":"4: Ok but not good enough - rejection","review":"The paper proposes a novel way of compressing gradient updates for distributed SGD, in order to speed up overall execution. While the technique is novel as far as I know (eq. (1) in particular), many details in the paper are poorly explained (I am unable to understand) and experimental results do not demonstrate that the problem targeted is actually alleviated.\n\nMore detailed remarks:\n1: Motivating with ImageNet taking over a week to train seems misplaced when we have papers claiming to train ImageNet in 1 hour, 24 mins, 15 mins...\n4.1: Lemma 4.1 seems like you want B > 1, or clarify definition of V_B.\n4.2: This section is not fully comprehensible to me.\n- It seems you are confusingly overloading the term gradient and words derived (also in other parts or the paper). What is \"maximum value of gradients in a matrix\"? Make sure to use something else, when talking about individual elements of a vector (which is constructed as an average of gradients), etc.\n- Rounding: do you use deterministic or random rounding? Do you then again store the inaccuracy?\n- I don't understand definition of d. It seems you subtract logarithm of a gradient from a scalar.\n- In total, I really don't know what is the object that actually gets communicated, and consequently when you remark that this can be combined with QSGD and the more below it, I don't understand it. This section has to be thoroughly explained, perhaps with some illustrative examples.\n4.3: allgatherv remark: does that mean that this approach would not scale well to higher number of workers?\n4.4: Remarks about quantization and mantissa manipulation are not clear to me again, or what is the point in doing so. Possible because the problems above.\n5: I think this section is not too useful unless you can accompany it with actual efficient implementation and contrast the practical performance. \n6: Given that I don't understand how you compress the information being communicated, it is hard to believe the utility of the method. The objective was to speed up training time because communication is bottleneck. If you provide 12,000x compression, is it any more practically useful than providing 120x compression? What would be the difference in runtime? Such questions are never discussed. Further, if in the implementation you discuss masking mantissa, I have serious concern about whether the compression protocol is feasible to implement efficiently, without writing some extremely low-level code. I think the soundness of work addressing this particular problem is damaged if not implemented properly (compared to other kinds of works in current ML related research). Therefore I highly recommend including proper time comparison with a baseline in the future.\nFurther, I don't understand 2 things about the Tables. a) how do you combine the proposed method with Momentum in SGD? This is not discussed as far as I can see. b) What is \"QSGD, 2bit\" If I remember QSGD protocol correctly, there's no natural mapping of 2bit to its parameters.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance-based Gradient Compression for Efficient Distributed Deep Learning","abstract":"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","pdf":"/pdf/d06ff7651ff54a171b7a0fdb38699d6fa1a9d841.pdf","TL;DR":"A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing ‘unambiguous’ gradients.","paperhash":"anonymous|variancebased_gradient_compression_for_efficient_distributed_deep_learning","_bibtex":"@article{\n  anonymous2018variance-based,\n  title={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkEfPeZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper597/Authors"],"keywords":["distributed deep learning","gradient compression","collective communication","data parallel distributed sgd","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1512222697487,"tcdate":1511799920209,"number":1,"cdate":1511799920209,"id":"B1O_32YeM","invitation":"ICLR.cc/2018/Conference/-/Paper597/Official_Review","forum":"rkEfPeZRb","replyto":"rkEfPeZRb","signatures":["ICLR.cc/2018/Conference/Paper597/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting. However, there also exist several major issues in the paper.","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a variance-based gradient compression method to reduce the communication overhead of distributed deep learning. Experiments on real datasets are used for evaluation. \n\nThe idea to adopt approximated variances of gradients to reduce communication cost seems to be interesting. However, there also exist several major issues in the paper.\n\nFirstly, the authors propose to combine two components to reduce communication cost, one being variance-based gradient compression and the other being quantization and parameter encoding. But the contributions of these two components are not separately analyzed or empirically verified. \n\nSecondly, the experimental results are unconvincing. The accuracy of Momentum SGD for ‘Strom, \\tau=0.01’ on CIFAR-10 is only 10.6%. Obviously, the learning procedure is not convergent. It is highly possible that the authors do not choose a good hyper-parameter. Furthermore, the proposed method (not the hybrid) is not necessarily better than Strom except for the case of Momentum SGD on CIFAR-10. Please note that the case of Momentum SGD on CIFAR-10 may have a problematic experimental setting for Strom. In addition, it is weird that the experiment on ImageNet does not adopt the same setting as that on CIFAR-10 to evaluate both Adam and Momentum SGD. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Variance-based Gradient Compression for Efficient Distributed Deep Learning","abstract":"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","pdf":"/pdf/d06ff7651ff54a171b7a0fdb38699d6fa1a9d841.pdf","TL;DR":"A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing ‘unambiguous’ gradients.","paperhash":"anonymous|variancebased_gradient_compression_for_efficient_distributed_deep_learning","_bibtex":"@article{\n  anonymous2018variance-based,\n  title={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkEfPeZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper597/Authors"],"keywords":["distributed deep learning","gradient compression","collective communication","data parallel distributed sgd","image classification"]}},{"tddate":null,"ddate":null,"tmdate":1509739210542,"tcdate":1509127948492,"number":597,"cdate":1509739207883,"id":"rkEfPeZRb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rkEfPeZRb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Variance-based Gradient Compression for Efficient Distributed Deep Learning","abstract":"Due to the substantial computational cost, training state-of-the-art deep neural networks for large-scale datasets often requires distributed training using multiple computation workers. However, by nature, workers need to frequently communicate gradients, causing severe bottlenecks, especially on lower bandwidth connections. A few methods have been proposed to compress gradient for efficient communication, but they either suffer a low compression ratio or significantly harm the resulting model accuracy, particularly when applied to convolutional neural networks. To address these issues, we propose a method to reduce the communication overhead of distributed deep learning. Our key observation is that gradient updates can be delayed until an unambiguous (high amplitude, low variance) gradient has been calculated. We also present an efficient algorithm to compute the variance and prove that it can be obtained with negligible additional cost. We experimentally show that our method can achieve very high compression ratio while maintaining the result model accuracy. We also analyze the efficiency using computation and communication cost models and provide the evidence that this method enables distributed deep learning for many scenarios with commodity environments.","pdf":"/pdf/d06ff7651ff54a171b7a0fdb38699d6fa1a9d841.pdf","TL;DR":"A new algorithm to reduce the communication overhead of distributed deep learning by distinguishing ‘unambiguous’ gradients.","paperhash":"anonymous|variancebased_gradient_compression_for_efficient_distributed_deep_learning","_bibtex":"@article{\n  anonymous2018variance-based,\n  title={Variance-based Gradient Compression for Efficient Distributed Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rkEfPeZRb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper597/Authors"],"keywords":["distributed deep learning","gradient compression","collective communication","data parallel distributed sgd","image classification"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}