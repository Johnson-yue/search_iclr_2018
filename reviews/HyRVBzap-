{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222717447,"tcdate":1511973455904,"number":3,"cdate":1511973455904,"id":"HkdIMPhez","invitation":"ICLR.cc/2018/Conference/-/Paper67/Official_Review","forum":"HyRVBzap-","replyto":"HyRVBzap-","signatures":["ICLR.cc/2018/Conference/Paper67/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"The paper presents a novel adversarial training setup, based on distance based loss of the feature embedding.\n\n+ novel loss\n+ good experimental evaluation\n+ better performance\n- way too long\n- structure could be improved\n- pivot loss seems hacky\n\nThe distance based loss is novel, and significantly different from prior work. It seems to perform well in practice as shown in the experimental section.\nThe experimental section is extensive, and offers new insights into both the presented algorithm and baselines. Judging the content of the paper alone, it should be accepted.\n\nHowever, the exposition needs significant improvements to warrant acceptance. First, the paper is way too long and unfocused. The recommended length is 8 pages + 1 page for citations. This paper is 12+1 pages long, plus a 5 page supplement. I'd highly recommend the authors to cut a third of their text, it would help focus the paper on the actual message: pushing their new algorithm. Try to remove any sentence or word that doesn't serve a purpose (help sell the algorithm).\nThe structure of the paper could also be improved. For example the cascade adversarial training is buried deep inside the experimental section. Considering that it is part of the title, I would have expected a proper exposition of the idea in the technical section (before any results are presented). While condensing the paper, consider presenting all technical material before evaluation.\nFinally, the pivot \"loss\" seems a bit hacky. First, the pivot objective and bidirectional loss are exactly the same thing. While the bidirectional loss is a proper loss and optimized as such (by optimizing both E^adv and E), the pivot objective is no loss function, as it does not correspond to any function any optimization algorithm could minimize. I'd recommend the just remove the pivot objective, or at least not call it a loss.\n\nIn summary, the results and presented method are good, and eventually deserve publication. However the exposition needs to significantly improve for the paper to be ready for ICLR.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cascade Adversarial Machine Learning Regularized with a Unified Embedding","abstract":"Deep neural network classifiers are vulnerable to small input perturbations carefully generated by the adversaries. Injecting adversarial inputs during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). This additional regularization encourages two similar images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels, enhancing classifier’s robustness against pixel level perturbation. Next, we show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we also propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","pdf":"/pdf/290ee2151cabb7545056e1094cd49bf397d7b0ae.pdf","TL;DR":"Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.","paperhash":"anonymous|cascade_adversarial_machine_learning_regularized_with_a_unified_embedding","_bibtex":"@article{\n  anonymous2018cascade,\n  title={Cascade Adversarial Machine Learning Regularized with a Unified Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRVBzap-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper67/Authors"],"keywords":["adversarial machine learning","embedding","regularization","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222717492,"tcdate":1511959567314,"number":2,"cdate":1511959567314,"id":"HyPfhQ2ez","invitation":"ICLR.cc/2018/Conference/-/Paper67/Official_Review","forum":"HyRVBzap-","replyto":"HyRVBzap-","signatures":["ICLR.cc/2018/Conference/Paper67/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space. This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense. Note that a similar strategy has been used in the recent past under the name of stability training. The proposed method works well on CIFAR and MNIST datasets. My main concerns are:\n\n\t- The adversarial objective and the stability objective are potentially conflicting. Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label). Have the authors considered this issue? Can they elaborate more on how they with this?\n\n\t- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data. Did the authors try such experiment? It would be interesting to see these results. \n\nLastly, The insights regarding label leaking are not compelling.  Label leaking is not a mysterious phenomenon. An adversarially trained model learns on two different distributions. Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model. Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models. I lean on accept side.  \n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cascade Adversarial Machine Learning Regularized with a Unified Embedding","abstract":"Deep neural network classifiers are vulnerable to small input perturbations carefully generated by the adversaries. Injecting adversarial inputs during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). This additional regularization encourages two similar images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels, enhancing classifier’s robustness against pixel level perturbation. Next, we show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we also propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","pdf":"/pdf/290ee2151cabb7545056e1094cd49bf397d7b0ae.pdf","TL;DR":"Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.","paperhash":"anonymous|cascade_adversarial_machine_learning_regularized_with_a_unified_embedding","_bibtex":"@article{\n  anonymous2018cascade,\n  title={Cascade Adversarial Machine Learning Regularized with a Unified Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRVBzap-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper67/Authors"],"keywords":["adversarial machine learning","embedding","regularization","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1512222717532,"tcdate":1511865099049,"number":1,"cdate":1511865099049,"id":"Hy7Gjh9eM","invitation":"ICLR.cc/2018/Conference/-/Paper67/Official_Review","forum":"HyRVBzap-","replyto":"HyRVBzap-","signatures":["ICLR.cc/2018/Conference/Paper67/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar. The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks. The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks. \n\nThe idea proposed is fairly straight-forward. Despite being a simple approach, the experimental results are quite promising.  The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.  \n\nAs pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings. Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?\n\nTable 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive. Do the authors have any insight on this? \n\nComments:\n1. The writing of the paper could be improved. For example, \"Transferability analysis\" in section 1 is barely understandable;\n2. Arrow in Figure 3 are not quite readable;\n3. The paper is over 11 pages. The authors might want to consider shrink it down the recommended length. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Cascade Adversarial Machine Learning Regularized with a Unified Embedding","abstract":"Deep neural network classifiers are vulnerable to small input perturbations carefully generated by the adversaries. Injecting adversarial inputs during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). This additional regularization encourages two similar images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels, enhancing classifier’s robustness against pixel level perturbation. Next, we show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we also propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","pdf":"/pdf/290ee2151cabb7545056e1094cd49bf397d7b0ae.pdf","TL;DR":"Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.","paperhash":"anonymous|cascade_adversarial_machine_learning_regularized_with_a_unified_embedding","_bibtex":"@article{\n  anonymous2018cascade,\n  title={Cascade Adversarial Machine Learning Regularized with a Unified Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRVBzap-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper67/Authors"],"keywords":["adversarial machine learning","embedding","regularization","adversarial attack"]}},{"tddate":null,"ddate":null,"tmdate":1509739504377,"tcdate":1508873526529,"number":67,"cdate":1509739501722,"id":"HyRVBzap-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyRVBzap-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Cascade Adversarial Machine Learning Regularized with a Unified Embedding","abstract":"Deep neural network classifiers are vulnerable to small input perturbations carefully generated by the adversaries. Injecting adversarial inputs during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). This additional regularization encourages two similar images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels, enhancing classifier’s robustness against pixel level perturbation. Next, we show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we also propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","pdf":"/pdf/290ee2151cabb7545056e1094cd49bf397d7b0ae.pdf","TL;DR":"Cascade adversarial training + low level similarity learning improve robustness against both white box and black box attacks.","paperhash":"anonymous|cascade_adversarial_machine_learning_regularized_with_a_unified_embedding","_bibtex":"@article{\n  anonymous2018cascade,\n  title={Cascade Adversarial Machine Learning Regularized with a Unified Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyRVBzap-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper67/Authors"],"keywords":["adversarial machine learning","embedding","regularization","adversarial attack"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}