{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222563362,"tcdate":1512057743097,"number":2,"cdate":1512057743097,"id":"SJvcooagM","invitation":"ICLR.cc/2018/Conference/-/Paper1167/Official_Review","forum":"HkGJUXb0-","replyto":"HkGJUXb0-","signatures":["ICLR.cc/2018/Conference/Paper1167/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"6: Marginally above acceptance threshold","review":"This paper presents a tensor decomposition method called tensor ring (TR) decomposition. The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors. This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1).\n\nTo learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm. To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable.\n\nThe proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results.\n\nThis is an interesting work. TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations. This seems to be a good direction. The experimental results are somewhat limited but the overall framework looks appealing.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Efficient Tensor Representations with Ring Structure Networks","abstract":"\\emph{Tensor train (TT) decomposition} is a powerful representation for high-order tensors,\nwhich has been successfully applied to various machine learning tasks in recent years. \nIn this paper, we propose a more generalized tensor decomposition with ring structure network  by\nemploying circular multilinear products over a sequence of lower-order core tensors, which is termed as TR representation. Several learning algorithms\nincluding blockwise ALS  with adaptive tensor ranks and  SGD  with high scalability are presented. Furthermore, the mathematical properties are investigated, which enables us to perform basic algebra operations in a computationally efficiently way by using TR representations.\nExperimental results on synthetic signals and real-world datasets demonstrate the effectiveness of TR model and the learning algorithms. In particular, we show that  the structure information and high-order correlations within a 2D image can be captured efficiently by employing tensorization and TR representation. \n","pdf":"/pdf/a2f569c8fabb4aa65611d077829bfff2946df00d.pdf","paperhash":"anonymous|learning_efficient_tensor_representations_with_ring_structure_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Efficient Tensor Representations with Ring Structure Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkGJUXb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1167/Authors"],"keywords":["Tensor Decomposition","Tensor Networks","Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1512222563403,"tcdate":1511923424342,"number":1,"cdate":1511923424342,"id":"ry_11ijeM","invitation":"ICLR.cc/2018/Conference/-/Paper1167/Official_Review","forum":"HkGJUXb0-","replyto":"HkGJUXb0-","signatures":["ICLR.cc/2018/Conference/Paper1167/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Preliminary examination. Lacks novelty.","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression. Most of the techniques used are well-known in the tensor community (outside of machine learning). The main contribution of the paper is the introduce such techniques to the ML community and presents experimental results for support.\n\nThe paper is rather preliminary in its examination. For example, it is claimed that the proposed decomposition provides \"enhanced representation ability\", but this is not justified rigorously either via more comprehensive experimentation or via a theoretical justification. Furthermore, the paper lacks in novelty aspect, as it is uses mostly well-known techniques. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Learning Efficient Tensor Representations with Ring Structure Networks","abstract":"\\emph{Tensor train (TT) decomposition} is a powerful representation for high-order tensors,\nwhich has been successfully applied to various machine learning tasks in recent years. \nIn this paper, we propose a more generalized tensor decomposition with ring structure network  by\nemploying circular multilinear products over a sequence of lower-order core tensors, which is termed as TR representation. Several learning algorithms\nincluding blockwise ALS  with adaptive tensor ranks and  SGD  with high scalability are presented. Furthermore, the mathematical properties are investigated, which enables us to perform basic algebra operations in a computationally efficiently way by using TR representations.\nExperimental results on synthetic signals and real-world datasets demonstrate the effectiveness of TR model and the learning algorithms. In particular, we show that  the structure information and high-order correlations within a 2D image can be captured efficiently by employing tensorization and TR representation. \n","pdf":"/pdf/a2f569c8fabb4aa65611d077829bfff2946df00d.pdf","paperhash":"anonymous|learning_efficient_tensor_representations_with_ring_structure_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Efficient Tensor Representations with Ring Structure Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkGJUXb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1167/Authors"],"keywords":["Tensor Decomposition","Tensor Networks","Stochastic Gradient Descent"]}},{"tddate":null,"ddate":null,"tmdate":1510092379117,"tcdate":1509139929561,"number":1167,"cdate":1510092359204,"id":"HkGJUXb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HkGJUXb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Learning Efficient Tensor Representations with Ring Structure Networks","abstract":"\\emph{Tensor train (TT) decomposition} is a powerful representation for high-order tensors,\nwhich has been successfully applied to various machine learning tasks in recent years. \nIn this paper, we propose a more generalized tensor decomposition with ring structure network  by\nemploying circular multilinear products over a sequence of lower-order core tensors, which is termed as TR representation. Several learning algorithms\nincluding blockwise ALS  with adaptive tensor ranks and  SGD  with high scalability are presented. Furthermore, the mathematical properties are investigated, which enables us to perform basic algebra operations in a computationally efficiently way by using TR representations.\nExperimental results on synthetic signals and real-world datasets demonstrate the effectiveness of TR model and the learning algorithms. In particular, we show that  the structure information and high-order correlations within a 2D image can be captured efficiently by employing tensorization and TR representation. \n","pdf":"/pdf/a2f569c8fabb4aa65611d077829bfff2946df00d.pdf","paperhash":"anonymous|learning_efficient_tensor_representations_with_ring_structure_networks","_bibtex":"@article{\n  anonymous2018learning,\n  title={Learning Efficient Tensor Representations with Ring Structure Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HkGJUXb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1167/Authors"],"keywords":["Tensor Decomposition","Tensor Networks","Stochastic Gradient Descent"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}