{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222802086,"tcdate":1511754746608,"number":2,"cdate":1511754746608,"id":"Sk7b2-tlz","invitation":"ICLR.cc/2018/Conference/-/Paper88/Official_Review","forum":"SkA-IE06W","replyto":"SkA-IE06W","signatures":["ICLR.cc/2018/Conference/Paper88/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Compared with relevant existing literature in this line of work, this paper extends the commonly-used Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions. This work is very solid in that the authors rigorously demonstrate that SGD can learn the convolutional filter in polynomial time with random initialization. ","rating":"9: Top 15% of accepted papers, strong accept","review":"(a) Significance\nThis is an interesting theoretical deep learning paper, where the authors try to provide the theoretical insights why SGD can learn the neural network well. The motivation is well-justified and clearly presented in the introduction and related work section. And the major contribution of this work is the generalization to the non-Gaussian case, which is more in line with the real world settings. Indeed, this is the first work analyzing the input distribution beyond Gaussian, which might be an important work towards understanding the empirical success of deep learning. \n\n(b) Originality\nThe division of the input space and the analytical formulation of the gradient are interesting, which are also essential for the convergence analysis. Also, the analysis framework relies on novel but reasonable distribution assumptions, and is different from the relevant literature, i.e., Li & Yuan 2017, Soltanolkotabi 2017, Zhong et al. 2017. I curious whether the angular smoothness assumptions can be applied to a more general network architecture, say two-layer neural network.\n\n(c) Clarity & Quality \nOverall, this is a well-written paper. The theoretical results are well-presented and followed by insightful explanations or remarks. And the experiments are demonstrated to justify the theoretical findings as well. The authors did a really good job in explaining the intuitions behind the imposed assumptions and justifying them based on the theoretical and experimental results. I think the quality of this work is above the acceptance bar of ICLR and it should be published in ICLR 2018.\n\nMinor comments: \n1. Figure 3 looks a little small. It is better to make them clearer.\n2. In the appendix, ZZ^{\\top} and the indicator function are missing in the first equation of page 13.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"When is a Convolutional Filter Easy to Learn?","abstract":"We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that justify our theoretical findings.","pdf":"/pdf/1abf810331c982c57060b9b853d25e3445bedf5c.pdf","TL;DR":"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time.","paperhash":"anonymous|when_is_a_convolutional_filter_easy_to_learn","_bibtex":"@article{\n  anonymous2018when,\n  title={When is a Convolutional Filter Easy to Learn?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkA-IE06W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper88/Authors"],"keywords":["deep learning","convolutional neural network","non-convex optimization","convergence analysis"]}},{"tddate":null,"ddate":null,"tmdate":1512222802127,"tcdate":1511710261918,"number":1,"cdate":1511710261918,"id":"SJA4C8_gG","invitation":"ICLR.cc/2018/Conference/-/Paper88/Official_Review","forum":"SkA-IE06W","replyto":"SkA-IE06W","signatures":["ICLR.cc/2018/Conference/Paper88/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper shows that under certain conditions, SGD learns a single convolutional filter. The conditions are a bit hard to understand and might be fairly strong.","rating":"5: Marginally below acceptance threshold","review":"This paper studies the problem of learning a single convolutional filter using SGD. The main result is: if the \"patches\" of the convolution are sufficiently aligned with each other, then SGD with a random initialization can recover the ground-truth parameter of a convolutional filter (single filter, ReLU, average pooling). The convergence rate, and how \"sufficiently aligned\" depend on some quantities related to the underlying data distribution. A major strength of the result is that it can work for general continuous distributions and does not really rely on the input distribution being Gaussian; the main weakness is that some of the distribution dependent quantities are not very intuitive, and the alignment requirement might be very high.\n\nDetailed comments:\n1. It would be good to clarify what the angle requirement means on page 2. It says the angle between Z_i, Z_j is at most \\rho, is this for any i,j? From the later part it seems that each Z_i should be \\rho close to the average, which would imply pairwise closeness (with some constant factor).\n2. The paper first proves result for a single neuron, which is a clean result. It would be interesting to see what are values of \\gamma(\\phi) and L(\\phi) for some distributions (e.g. Gaussian, uniform in hypercube, etc.) to give more intuitions. \n3. The convergence rate depends on \\gamma(\\phi_0), from the initialization, \\phi_0 is probably very close to \\pi/2 (the closeness depend on dimension), which is  also likely to make \\gamma(\\phi_0) depend on dimension (this is especially true of Gaussian). \n4. More precisely, \\gamma(\\phi_0) needs to be at least 6L_{cross} for the result to work, and L_{cross} seems to be a problem dependent constant that is not related to the dimension of the data. Also \\gamma(\\phi_0) depends on \\gamma_{avg}(\\phi_0) and \\rho, when \\rho is reasonable (say a constant), \\gamma(\\phi_0) really needs to be a constant that is independent of dimension. On the other hand, in Theorem 3.4 we can see that the upperbound on \\alpha (the quality of initialization) depends on the dimension. \n5. Even assuming \\rho is a constant strictly smaller than \\pi/2 seems a bit strong. It is certainly plausible that nearby patches are highly correlated, but what is required here is that all patches are close to the average. Given an image it is probably not too hard to find an almost all white patch and an almost all dark patch so that they cannot both be within a good angle to the average. \n\nOverall I feel the result is interesting but hard to interpret correctly. The details of the theorem do not really support the high level claims very strongly. The paper would be much better if it goes over several example distributions and show explicitly what are the guarantees. The reviewer tried to do that for Gaussian and as I mentioned above (esp. 4) the result does not seem very impressive, maybe there are other distributions where this result works better?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"When is a Convolutional Filter Easy to Learn?","abstract":"We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that justify our theoretical findings.","pdf":"/pdf/1abf810331c982c57060b9b853d25e3445bedf5c.pdf","TL;DR":"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time.","paperhash":"anonymous|when_is_a_convolutional_filter_easy_to_learn","_bibtex":"@article{\n  anonymous2018when,\n  title={When is a Convolutional Filter Easy to Learn?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkA-IE06W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper88/Authors"],"keywords":["deep learning","convolutional neural network","non-convex optimization","convergence analysis"]}},{"tddate":null,"ddate":null,"tmdate":1509739494016,"tcdate":1508947462401,"number":88,"cdate":1509739491361,"id":"SkA-IE06W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SkA-IE06W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"When is a Convolutional Filter Easy to Learn?","abstract":"We analyze the convergence of (stochastic) gradient descent algorithm for learning a convolutional filter with Rectified Linear Unit (ReLU) activation function. Our analysis does not rely on any specific form of the input distribution and our proofs only use the definition of ReLU, in contrast with previous works that are restricted to standard Gaussian input. We show that (stochastic) gradient descent with random initialization can learn the convolutional filter in polynomial time and the convergence rate depends on the smoothness of the input distribution and the closeness of patches. To the best of our knowledge, this is the first recovery guarantee of gradient-based algorithms for convolutional filter on non-Gaussian input distributions. Our theory also justifies the two-stage learning rate strategy in deep neural networks. While our focus is theoretical, we also present experiments that justify our theoretical findings.","pdf":"/pdf/1abf810331c982c57060b9b853d25e3445bedf5c.pdf","TL;DR":"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time.","paperhash":"anonymous|when_is_a_convolutional_filter_easy_to_learn","_bibtex":"@article{\n  anonymous2018when,\n  title={When is a Convolutional Filter Easy to Learn?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SkA-IE06W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper88/Authors"],"keywords":["deep learning","convolutional neural network","non-convex optimization","convergence analysis"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}