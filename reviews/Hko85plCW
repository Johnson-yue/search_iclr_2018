{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222651320,"tcdate":1511820166863,"number":3,"cdate":1511820166863,"id":"H1J9s-cef","invitation":"ICLR.cc/2018/Conference/-/Paper425/Official_Review","forum":"Hko85plCW","replyto":"Hko85plCW","signatures":["ICLR.cc/2018/Conference/Paper425/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"Monotonic Chunkwise Attention\"","rating":"7: Good paper, accept","review":"This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window).  \n\nPros.\n- the paper is clearly written.\n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments).\nCons.\n- in terms of the originality, this method is rather incremental from the prior study (Raffel et al)\n- in terms of considering a monotonic alignment, Hori et al, \"Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,\" in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods. The paper should also discuss this method in Section 4.\n\nComments:\n- Eq. (16): $j$ in the denominator should be $t_j$.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Monotonic Chunkwise Attention","abstract":"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of an offline soft attention model. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention model.","pdf":"/pdf/125a7eb4baddaf43019a2b14b03d156f2bb0ba98.pdf","TL;DR":"An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence.","paperhash":"anonymous|monotonic_chunkwise_attention","_bibtex":"@article{\n  anonymous2018monotonic,\n  title={Monotonic Chunkwise Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hko85plCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper425/Authors"],"keywords":["attention","sequence-to-sequence","speech recognition","document summarization"]}},{"tddate":null,"ddate":null,"tmdate":1512222651358,"tcdate":1511806637516,"number":2,"cdate":1511806637516,"id":"ryr2L0FeG","invitation":"ICLR.cc/2018/Conference/-/Paper425/Official_Review","forum":"Hko85plCW","replyto":"Hko85plCW","signatures":["ICLR.cc/2018/Conference/Paper425/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Limited extension to previous work","rating":"5: Marginally below acceptance threshold","review":"The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position. Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model. In terms of the model this is a relatively small extention of Raffel et al 2017.\n\nResults show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. Is the offline attention baseline unidirectional or bidirectional? In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.\n\nMy concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model. Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping. My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer. While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.\n\nFor document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this. If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper. \nSentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?\n\nI like the fact that the model is truely offline, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Monotonic Chunkwise Attention","abstract":"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of an offline soft attention model. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention model.","pdf":"/pdf/125a7eb4baddaf43019a2b14b03d156f2bb0ba98.pdf","TL;DR":"An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence.","paperhash":"anonymous|monotonic_chunkwise_attention","_bibtex":"@article{\n  anonymous2018monotonic,\n  title={Monotonic Chunkwise Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hko85plCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper425/Authors"],"keywords":["attention","sequence-to-sequence","speech recognition","document summarization"]}},{"tddate":null,"ddate":null,"tmdate":1512222651401,"tcdate":1511475020947,"number":1,"cdate":1511475020947,"id":"HJS8P6Vgf","invitation":"ICLR.cc/2018/Conference/-/Paper425/Official_Review","forum":"Hko85plCW","replyto":"Hko85plCW","signatures":["ICLR.cc/2018/Conference/Paper425/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Great writing but lacks analysis","rating":"6: Marginally above acceptance threshold","review":"This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention. The paper is very well written and easy to follow. The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger.\n\nThe first set of questions is about the monotonic attention. Training the monotonic attention with expected context vectors is intuitive, but can this be justified further? For example, how far does using the expected context vector deviate from marginalizing the monotonic attention? The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention. How does the greedy step affect training and decoding? It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding. These questions should really be answered in [1]. Since the authors are extending their work and since these issues might cause training difficulties, it might be useful to look into these design choices.\n\nThe second question is about the window size $w$. Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper, why not attend to the entire segment, i.e., from the current boundary to the previous boundary?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2. (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.) How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition. If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention? The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information. Would the special cases lead to worse performance and if so why is there a difference?\n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Monotonic Chunkwise Attention","abstract":"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of an offline soft attention model. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention model.","pdf":"/pdf/125a7eb4baddaf43019a2b14b03d156f2bb0ba98.pdf","TL;DR":"An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence.","paperhash":"anonymous|monotonic_chunkwise_attention","_bibtex":"@article{\n  anonymous2018monotonic,\n  title={Monotonic Chunkwise Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hko85plCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper425/Authors"],"keywords":["attention","sequence-to-sequence","speech recognition","document summarization"]}},{"tddate":null,"ddate":null,"tmdate":1509739310523,"tcdate":1509116498743,"number":425,"cdate":1509739307869,"id":"Hko85plCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hko85plCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Monotonic Chunkwise Attention","abstract":"Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of an offline soft attention model. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention model.","pdf":"/pdf/125a7eb4baddaf43019a2b14b03d156f2bb0ba98.pdf","TL;DR":"An online and linear-time attention mechanism that performs soft attention over adaptively-located chunks of the input sequence.","paperhash":"anonymous|monotonic_chunkwise_attention","_bibtex":"@article{\n  anonymous2018monotonic,\n  title={Monotonic Chunkwise Attention},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hko85plCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper425/Authors"],"keywords":["attention","sequence-to-sequence","speech recognition","document summarization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}