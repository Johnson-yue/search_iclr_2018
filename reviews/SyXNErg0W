{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222600362,"tcdate":1511856332034,"number":2,"cdate":1511856332034,"id":"S1EAO5qxM","invitation":"ICLR.cc/2018/Conference/-/Paper253/Official_Review","forum":"SyXNErg0W","replyto":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference/Paper253/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper presented a modification to the Center loss softmax for feature learning","rating":"4: Ok but not good enough - rejection","review":"In the centre loss, the centre is learned. Now it's calculated as the average of the last layer's features\nTo enable training with SGD, the authors calculate the centre within a mini batch","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222600407,"tcdate":1511680399560,"number":1,"cdate":1511680399560,"id":"Hyd9YyOlf","invitation":"ICLR.cc/2018/Conference/-/Paper253/Official_Review","forum":"SyXNErg0W","replyto":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference/Paper253/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A solid empirical study on an isotropic  softmax loss function","rating":"6: Marginally above acceptance threshold","review":"The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space. The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class. The proposed loss has been compared extensively against a number of closely related approaches in methodology. Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means. \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated\n\n- The paper is mostly clearly organized and presented.\n\n- Numerical study shows some promise of the proposed method.\n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss.\n\n- The training procedure of the proposed method remains unclear in this paper. \n\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739402549,"tcdate":1509082154917,"number":253,"cdate":1509739399887,"id":"SyXNErg0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SyXNErg0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Softmax Supervision with Isotropic Normalization","abstract":"The softmax function is widely used to train deep neural networks for multi-class classification. Despite its outstanding performance in classification tasks, the features derived from the supervision of softmax are usually sub-optimal in some scenarios where Euclidean distances apply in feature spaces. To address this issue, we propose a new loss, dubbed the isotropic loss, in the sense that the overall distribution of data points is regularized to approach the isotropic normal one. Combined with the vanilla softmax, we formalize a novel criterion called the isotropic softmax, or isomax for short, for supervised learning of deep neural networks. By virtue of the isomax, the intra-class features are penalized by the isotropic loss while inter-class distances are well kept by the original softmax loss. Moreover, the isomax loss does not require any additional modifications to the network, mini-batches or the training process. Extensive experiments on classification and clustering are performed to demonstrate the superiority and robustness of the isomax loss.","pdf":"/pdf/f62d85da2a0b4616799e1322b99541f7a0504c33.pdf","TL;DR":"The discriminative capability of softmax for learning feature vectors of objects is effectively enhanced by virture of isotropic normalization on global distribution of data points.","paperhash":"anonymous|softmax_supervision_with_isotropic_normalization","_bibtex":"@article{\n  anonymous2018softmax,\n  title={Softmax Supervision with Isotropic Normalization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SyXNErg0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper253/Authors"],"keywords":["softmax","center loss","triplet loss","convolution neural network","supervised learning"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}