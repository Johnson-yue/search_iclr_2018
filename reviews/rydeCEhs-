{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222535614,"tcdate":1512090386109,"number":3,"cdate":1512090386109,"id":"SycMimAgG","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper tackles an important problem on learning neural net architectures that outperforms comparable methods and is reasonably faster","rating":"6: Marginally above acceptance threshold","review":"This paper tackles the problem of finding an optimal architecture for deep neural nets . They propose to solve it by training an auxiliary HyperNet to generate the main model. The authors propose the so called \"SMASH\" algorithm that ranks the neural net architectures based on their validation error. The authors adopt a memory-bank view of the network configurations for exploring a varied collection of network configurations. It is not clear whether this is a new contribution of this paper or whether the authors merely adopt this idea.  A clearer note on this would be welcome. My key concern is with the results as described in 4.1.; the correlation structure breaks down completely for \"low-budget\" SMASH in Figure 5(a) as compared Figure (4). Doesn't this then entail an investigation of what is the optimal size of the hyper network? Also I couldn't quite follow the importance of figure 5(b) - is it referenced in the text? The authors also note that SMASH is saves a lot of computation time; some time-comparison numbers would probably be more helpful to drive home the point especially when other methods out-perform SMASH. \nOne final point, for the uninitiated reader- sections 3.1 and 3.2 could probably be written somewhat more lucidly for better access.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/cfe71e109b9065f89dc608da8ac14fd9875cc212.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1512222535720,"tcdate":1511821012458,"number":2,"cdate":1511821012458,"id":"rJ200-5ez","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An experimental framework for designing neural architectures","rating":"7: Good paper, accept","review":"This paper is about a new experimental technique for exploring different neural architectures. It is well-written in general, numerical experiments demonstrate the framework and its capabilities as well as its limitations. \n\nA disadvantage of the approach may be that the search for architectures is random. It would be interesting to develop a framework where the search for the architecture is done with a framework where the updates to the architecture is done using a data-driven approach. Nevertheless, there are so many different neural architectures in the literature and this paper is a step towards comparing various architectures efficiently. \n\nMinor comments:\n\n1) Page 7,  \".. moreso than domain specificity.\" \n\nIt may be better to spell the word \"moreso\" as \"more so\", please see: https://en.wiktionary.org/wiki/moreso","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/cfe71e109b9065f89dc608da8ac14fd9875cc212.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1512222535771,"tcdate":1511662859420,"number":1,"cdate":1511662859420,"id":"SkmGrjvlz","invitation":"ICLR.cc/2018/Conference/-/Paper1/Official_Review","forum":"rydeCEhs-","replyto":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference/Paper1/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well written paper that introduces and applies SMASH framework with some experimental success","rating":"5: Marginally below acceptance threshold","review":"Summary of paper - This paper presents SMASH (or the one-Shot Model Architecture Search through Hypernetworks) which has two training phases (one to quickly train a random sample of network architectures and one to train the best architecture from the first stage). The paper presents a number of interesting experiments and discussions about those experiments, but offers more exciting ideas about training neural nets than experimental successes. \n\nReview - The paper is very well written with clear examples and an excellent contextualization of the work among current work in the field. The introduction and related work are excellently written providing both context for the paper and a preview of the rest of the paper. The clear writing make the paper easy to read, which also makes clear the various weaknesses and pitfalls of SMASH. \n\nThe SMASH framework appears to provide more interesting contributions to the theory of training Neural Nets than the application of said training. While in some experiments SMASH offers excellent results, in others the results are lackluster (which the authors admit, offering possible explanations). \n\nIt is a shame that the authors chose to push their section on future work to the appendices. The glimmers of future research directions (such as the end of the last paragraph in section 4.2) were some of the most intellectually exciting parts of the paper. This choice may be a reflection of preferring to highlight the experimental results over possible contributions to theory of neural nets.  \n\n\nPros - \n* Strong related work section that contextualizes this paper among current work\n* Very interesting idea to more efficiently find and train best architectures \n* Excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e. last paragraph of section 4.1, and last paragraph of section 4.2)\n* Publicly available code \n\nCons - \n* Some very strong experimental results contrasted with some mediocre results\n* The balance of the paper seems off, using more text on experiments than the contributions to theory. \n* (Minor) - The citation style is inconsistent in places. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/cfe71e109b9065f89dc608da8ac14fd9875cc212.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]}},{"tddate":null,"ddate":null,"tmdate":1509739537185,"tcdate":1506721263693,"number":1,"cdate":1509739534531,"id":"rydeCEhs-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rydeCEhs-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"SMASH: One-Shot Model Architecture Search through HyperNetworks","abstract":"Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.","pdf":"/pdf/cfe71e109b9065f89dc608da8ac14fd9875cc212.pdf","TL;DR":"A technique for accelerating neural architecture selection by approximating the weights of each candidate architecture instead of training them individually.","paperhash":"anonymous|smash_oneshot_model_architecture_search_through_hypernetworks","_bibtex":"@article{\n  anonymous2018smash:,\n  title={SMASH: One-Shot Model Architecture Search through HyperNetworks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rydeCEhs-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1/Authors"],"keywords":["meta-learning","architecture search","deep learning","computer vision"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}