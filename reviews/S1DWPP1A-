{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222566261,"tcdate":1511989262647,"number":3,"cdate":1511989262647,"id":"ByvGgjhez","invitation":"ICLR.cc/2018/Conference/-/Paper132/Official_Review","forum":"S1DWPP1A-","replyto":"S1DWPP1A-","signatures":["ICLR.cc/2018/Conference/Paper132/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Some interesting ideas, yet no clear message","rating":"3: Clear rejection","review":"Summary:\n\nThe authors aim to overcome one of the central limitations of intrinsically motivated goal exploration algorithms by learning a representation without relying on a \"designer\" to manually specify the space of possible goals. This work is significant as it would allow one to learn a policy in complex environments even in the absence of a such a designer or even a clear notion of what would constitute a \"good\" distribution of goal states.\n\nHowever, even after multiple reads, much of the remainder of the paper remains unclear. Many important details, including the metrics by which the authors evaluate performance of their work, can only be found in the appendix; this makes the paper very difficult to follow.\n\nThere are too many metrics and too few conclusions for this paper. The authors introduce a handful of metrics for evaluating the performance of their approach; I am unfamiliar with a couple of these metrics and there is not much exposition justifying their significance and inclusion in the paper. Furthermore, there are myriad plots showing the performance of the different algorithms, but very little explanation of the importance of the results. For instance, in the middle of page 9, it is noted that some of the techniques \"yield almost as low performance as\" the randomized baseline, yet no attempt is made to explain why this might be the case or what implications it has for the authors' approach. This problem pervades the paper: many metrics are introduced for how we might want to evaluate these techniques, yet there is no provided reason to prefer one over another (or even why we might want to prefer them over the classical techniques).\n\nOther comments:\n- There remain open questions about the quality of the MSE numbers; there are a number of instances in which the authors cite that the \"Meta-Policy MSE is not a simple to interpret\" (The remainder of this sentence is incomplete in the paper), yet little is done to further justify why it was used here, or why many of the deep representation techniques do not perform very well.\n- The authors do not list how many observations they are given before the deep representations are learned. Why is this? Additionally, is it possible that not enough data was provided?\n- The authors assert that 10 dimensions was chosen arbitrarily for the size of the latent space, but this seems like a hugely important choice of parameter. What would happen if a dimension of 2 were chosen? Would the performance of the deep representation models improve? Would their performance rival that of RGE-FI?\n- The authors should motivate the algorithm on page 6 in words before simply inserting it into the body of the text. It would improve the clarity of the paper.\n- The authors need to be clearer about their notation in a number of places. For instance, they use \\gamma to represent the distribution of goals, yet it does not appear on page 7, in the experimental setup.\n- It is never explicitly mentioned exactly how the deep representation learning methods will be used. It is pretty clear to those who are familiar with the techniques that the latent space is what will be used, but a few equations would be instructive (and would make the paper more self-contained).\n\nIn short, the paper has some interesting ideas, yet lacks a clear takeaway message. Instead, it contains a large number of metrics and computes them for a host of different possible variations of the proposed techniques, and does not include significant explanation for the results. Even given my lack of expertise in this subject, the paper has some clear flaws that need addressing.\n\nPros:\n- A clear, well-written abstract and introduction\n- While I am not experienced enough in the field to really comment on the originality, it does seem that the approach the authors have taken is original, and applies deep learning techniques to avoid having to custom-design a \"feature space\" for their particular family of problems.\n\nCons:\n- The figure captions are all very \"matter-of-fact\" and, while they explain what each figure shows, provide no explanation of the results. The figure captions should be as self-contained as possible (I should be able to understand the figures and the implications of the results from the captions alone).\n- There is not much significance in the current form of the paper, owing to the lack of clear message. While the overarching problem is potentially interesting, the authors seem to make very little effort to draw conclusions from their results. I.e. it is difficult for me to easily visualize all of the \"moving parts\" of this work: a figure showing the relationship bet\n- Too many individual ideas are presented in the paper, hurting clarity. As a result, the paper feels scattered. The authors do not have a clear message that neatly ties the results together.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration","abstract":"Intrinsically  motivated  goal  exploration  algorithms  enable  machines  to  explore and discover a diversity of policies in large and complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous action and state spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered space. In this work, we propose to use deep representation learning algorithms to learn a goal space, leveraging observations of world changes produced by another agent. We present experiments with a simulated robot arm interacting with an object, and we study how the performances of exploration algorithms on such learned representations relate with their performances on engineered representations. We also uncover a link between the exploration performances and the quality of the learned representation regarding the underlying state space.","pdf":"/pdf/66b69241401f88d670d888ea2c4a6ac1d1d706e7.pdf","TL;DR":"We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of a space where goals can be sampled, and compare systematically various representation learning algorithms in this context. ","paperhash":"anonymous|unsupervised_learning_of_goal_spaces_for_intrinsically_motivated_exploration","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1DWPP1A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper132/Authors"],"keywords":["Intrinsic Motivation","Exploration","Multi-Goal Learning","Autonomous Learning","Developmental Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222566300,"tcdate":1511814817555,"number":2,"cdate":1511814817555,"id":"Bk9oIe5gG","invitation":"ICLR.cc/2018/Conference/-/Paper132/Official_Review","forum":"S1DWPP1A-","replyto":"S1DWPP1A-","signatures":["ICLR.cc/2018/Conference/Paper132/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting, but not substantial enough","rating":"5: Marginally below acceptance threshold","review":"The paper investigates different representation learning methods to create a latent space for intrinsic goal generation in guided exploration algorithms.  The research is in principle very important and interesting.\n\nThe introduction discusses a great deal about intrinsic motivations and about goal generating algorithms. This is really great, just that the paper only focuses on a very small aspect of learning a state representation in an agent that has no intrinsic motivation other than trying to achieve random goals.\nI think the paper (not only the Intro) could be a bit condensed to more concentrate on the actual contribution. \n\nThe contribution is that the quality of the representation and the sampling of goals is important for the exploration performance and that classical methods like ISOMap are better than Autoencoder-type methods. \n\nAlso, it is written in the Conclusions (and in other places): \"[..] we propose a new intrinsically Motivated goal exploration strategy....\". This is not really true.  There is nothing new with the intrinsically motivated selection of goals here, just that they are in another space. Also, there is no intrinsic motivation. I also think the title is misleading.\n\nThe paper is in principle interesting. However, I doubt that the experimental evaluations are substantial enough for profound conclusion. \n\nSeveral points of critic: \n- the input space was very simple in all experiments, not suitable for distinguishing between the algorithms, for instance, ISOMap typically suffers from noise and higher dimensional manifolds, etc.\n- only the ball/arrow was in the input image, not the robotic arm. I understand this because in phase 1 the robot would not move, but this connects to the next point:\n- The representation learning is only a preprocessing step requiring a magic first phase.\n    -> Representation is not updated during exploration\n- The performance of any algorithm (except FI) in the Arm-Arrow task is really bad but without comment. \n- I am skeptical about the VAE  and RFVAE results. The difference between Gaussian sampling and the KDE is a bit alarming, as the KL in the VAE training is supposed to match the p(z) with N(0,1). Given the power of the encoder/decoder it should be possible to properly represent the simple embedded 2D/3D manifold and not just a very small part of it as suggested by Fig 10. \nI have a hard time believing these results. I urge you to check for any potential errors made. If there are not mistakes then this is indeed alarming.\n\nQuestions:\n- Is it true that the robot always starts from same initial condition?! Context=Emptyset. \n- For ISOMap etc, you also used a 10dim embedding?\n\nSuggestion:\n- The main problem seems to be that some algorithms are not representing the whole input space.\n- an additional measure that quantifies the difference between true input distribution and reproduced input distribution could tier the algorithms apart and would measure more what seems to be relevant here.  One could for instance measure the KL-divergence between the true input and the sampled (reconstructed) input (using samples and KDE or the like). \n- This could be evaluated on many different inputs (also those with a bit more complicated structure) without actually performing the goal finding.\n- BTW: I think Fig 10 is rather illustrative and should be somehow in the main part of the paper\n \nOn the positive side, the paper provides lots of details in the Appendix.\nAlso, it uses many different Representation Learning algorithms and uses measures from manifold learning to access their quality.\n\nIn the related literature, in particular concerning the intrinsic motivation, I think the following papers are relevant:\nJ. Schmidhuber, PowerPlay: training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Front. Psychol., 2013.\n\nand\n\nG. Martius, R. Der, and N. Ay. Information driven self-organization of complex robotic behaviors. PLoS ONE, 8(5):e63400, 2013.\n\n\nTypos and small details:\np3 par2: for PCA you cited Bishop. Not critical, but either cite one the original papers or maybe remove the cite altogether\np4 par-2: has multiple interests...: interests -> purposes?\np4 par-1: Outcome Space to the agent is is ...\nSec 2.2 par1: are rapidly mentioned... -> briefly\nSec 2.3 ...Outcome Space O, we can rewrite the architecture as:\n  and then comes the algorithm. This is a bit weird\nSec 3: par1: experimental campaign -> experiments?\np7: Context Space: the object was reset to a random position or always to the same position?\nFootnote 14: superior to -> larger than\np8 par2: Exploration Ratio Ratio_expl... probably also want to add (ER) as it is later used\nSec 4: slightly underneath -> slightly below\np9 par1: unfinished sentence: It is worth noting that the....\none sentence later: RP architecture? RPE?\nFig 3: the error of the methods (except FI) are really bad. An MSE of 1 means hardly any performance!\np11 par2: for e.g. with the SAGG..... grammar?\n\nPlots in general: use bigger font sizes.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration","abstract":"Intrinsically  motivated  goal  exploration  algorithms  enable  machines  to  explore and discover a diversity of policies in large and complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous action and state spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered space. In this work, we propose to use deep representation learning algorithms to learn a goal space, leveraging observations of world changes produced by another agent. We present experiments with a simulated robot arm interacting with an object, and we study how the performances of exploration algorithms on such learned representations relate with their performances on engineered representations. We also uncover a link between the exploration performances and the quality of the learned representation regarding the underlying state space.","pdf":"/pdf/66b69241401f88d670d888ea2c4a6ac1d1d706e7.pdf","TL;DR":"We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of a space where goals can be sampled, and compare systematically various representation learning algorithms in this context. ","paperhash":"anonymous|unsupervised_learning_of_goal_spaces_for_intrinsically_motivated_exploration","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1DWPP1A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper132/Authors"],"keywords":["Intrinsic Motivation","Exploration","Multi-Goal Learning","Autonomous Learning","Developmental Learning"]}},{"tddate":null,"ddate":null,"tmdate":1512222566340,"tcdate":1511474977824,"number":1,"cdate":1511474977824,"id":"HJcQvaVef","invitation":"ICLR.cc/2018/Conference/-/Paper132/Official_Review","forum":"S1DWPP1A-","replyto":"S1DWPP1A-","signatures":["ICLR.cc/2018/Conference/Paper132/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Review of Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration","rating":"6: Marginally above acceptance threshold","review":"This paper introduces a representation learning step in the Intrinsically Motivated Exploration Process (IMGEP)  framework.\n\nThough this work is far from my expertise fields I find it quite easy to read and a good introduction to IMGEP.\nNevertheless I have some major concerns that prevent me from giving an acceptance decision.\n\n1) The method uses mechanisms than can project back and forth a signal  to the \"outcome\" space. Nevertheless only the encoder/projection part seems to be used in the algorithm presented p6. For example the encoder part of an AE/VAE is used as a preprocesing stage of the phenomenon dynamic D. It should be obviously noticed that the decoder part could also be used for helping the inverse model I but apparently that is not the case in the proposed method.\n\n2) The representation stage R seems to be learned at the beginning of the algorithm and then fixed. When using DNN as R (when using AE/VAE) why don't you propagate a gradient through R when optimizing D and I ? In this way, learning R at the beginning is only an old good pre-training of DNN with AE.\n\n3) Eventually, Why not directly considering R as lower layers of D and using up to date techniques to train it ? (drop-out, weight clipping, batch normalization ...).\nWhy not using architecture adapted to images such as CNN ?\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration","abstract":"Intrinsically  motivated  goal  exploration  algorithms  enable  machines  to  explore and discover a diversity of policies in large and complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous action and state spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered space. In this work, we propose to use deep representation learning algorithms to learn a goal space, leveraging observations of world changes produced by another agent. We present experiments with a simulated robot arm interacting with an object, and we study how the performances of exploration algorithms on such learned representations relate with their performances on engineered representations. We also uncover a link between the exploration performances and the quality of the learned representation regarding the underlying state space.","pdf":"/pdf/66b69241401f88d670d888ea2c4a6ac1d1d706e7.pdf","TL;DR":"We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of a space where goals can be sampled, and compare systematically various representation learning algorithms in this context. ","paperhash":"anonymous|unsupervised_learning_of_goal_spaces_for_intrinsically_motivated_exploration","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1DWPP1A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper132/Authors"],"keywords":["Intrinsic Motivation","Exploration","Multi-Goal Learning","Autonomous Learning","Developmental Learning"]}},{"tddate":null,"ddate":null,"tmdate":1509739467943,"tcdate":1509025535097,"number":132,"cdate":1509739465289,"id":"S1DWPP1A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1DWPP1A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration","abstract":"Intrinsically  motivated  goal  exploration  algorithms  enable  machines  to  explore and discover a diversity of policies in large and complex environments. These exploration algorithms have been shown to allow real world robots to acquire skills such as tool use in high-dimensional continuous action and state spaces. However, they have so far assumed that self-generated goals are sampled in a specifically engineered space. In this work, we propose to use deep representation learning algorithms to learn a goal space, leveraging observations of world changes produced by another agent. We present experiments with a simulated robot arm interacting with an object, and we study how the performances of exploration algorithms on such learned representations relate with their performances on engineered representations. We also uncover a link between the exploration performances and the quality of the learned representation regarding the underlying state space.","pdf":"/pdf/66b69241401f88d670d888ea2c4a6ac1d1d706e7.pdf","TL;DR":"We propose a novel Intrinsically Motivated Goal Exploration architecture with unsupervised learning of a space where goals can be sampled, and compare systematically various representation learning algorithms in this context. ","paperhash":"anonymous|unsupervised_learning_of_goal_spaces_for_intrinsically_motivated_exploration","_bibtex":"@article{\n  anonymous2018unsupervised,\n  title={Unsupervised Learning of Goal Spaces for Intrinsically Motivated Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1DWPP1A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper132/Authors"],"keywords":["Intrinsic Motivation","Exploration","Multi-Goal Learning","Autonomous Learning","Developmental Learning"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}