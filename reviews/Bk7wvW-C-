{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222719914,"tcdate":1511982702288,"number":3,"cdate":1511982702288,"id":"SyU_UK2lf","invitation":"ICLR.cc/2018/Conference/-/Paper686/Official_Review","forum":"Bk7wvW-C-","replyto":"Bk7wvW-C-","signatures":["ICLR.cc/2018/Conference/Paper686/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Compelling results with a (relatively) efficient encoder-decoder architecture for sentence embedding learning","rating":"6: Marginally above acceptance threshold","review":"This paper is about modifications to the skip-thought framework for learning sentence embeddings. The results show performance comparable to or better than skip-thought while decreasing training time. \n\nI think the overall approach makes sense: use an RNN encoder because we know it works well, but improve training efficiency by changing the decoder to a combination of feed-forward and convolutional layers. \n\nI think it may be the case that this works well because the decoder is not auto-regressive but merely predicts each word independently. This is possible because the decoder will not be used after training. So all the words can be predicted all at once with a fixed maximum sentence length. In typical encoder-decoder applications, the decoder is used at test time to get predictions, so it is natural to make it auto-regressive. But in this case, the decoder is thrown away after training, so it makes more sense to make the decoder non-auto-regressive.  I think this point should be made in the paper. \n\nAlso, I think it's worth noting that an RNN decoder could be used in a non-auto-regressive architecture as well. That is, the sentence encoding could be mapped to a sequence of length 30 as is done with the CNN decoder currently; then a (multi-layer) BiLSTM could be run over that sequence, and then a softmax classifier can be attached to each hidden vector to predict the word at that position. It would be interesting to compare that BiLSTM decoder with the proposed CNN decoder and also to compare it to a skip-thought-style auto-regressive RNN decoder. This would let us understand whether the benefit is coming more from the non-auto-regressive nature of the decoder or from the CNN vs RNN differences. \n\nThat is, it would make sense to factor the decision of decoder design along multiple axes. One axis could be auto-regressive vs predict-all-words. Another axis could be using a CNN over the sequence of word positions or an RNN over the sequence of word positions.  For auto-regressive models, another axis could be train using previous ground-truth word vs train using previous predicted word.  Skip-thought corresponds to an auto-regressive RNN (using the previous ground-truth word IIRC).  The proposed decoder is a predict-all-words CNN.  It would be natural to also experiment with an auto-regressive CNN and a predict-all-words RNN (like what I described in the paragraph above). The paper is choosing a single point in the space and referring to it as a \"CNN decoder\" whereas there are many possible architectures that can be described this way and I think it would strengthen the paper to increase the precision in discussing the architecture and possible alternatives. \n\nOverall, I think the architectural choices and results are strong enough to merit publication. Adding any of the above empirical comparisons would further strengthen the paper. \n\nHowever, I did have quibbles with some of the exposition and some of the claims made throughout the paper. They are detailed below:\n\nSec. 2:\n\nIn the \"Decoder\" paragraph: please add more details about how the words are predicted. Are there final softmax layers that provide distributions over output words? I couldn't find this detail in the paper. What loss is minimized during training? Is it the sum of log losses over all words being predicted?\n\nSec. 3:\n\nSection 3 does not add much to the paper. The motivations there are mostly suggestive rather than evidence-based. Section 3 could be condensed by about 80% or so without losing much information. Overall, the paper has more than 10 pages of content, and the use of 2 extra pages beyond the desired submission length of 8 should be better justified. I would recommend adding a few more details to Section 2 and removing most of Section 3. I'll mention below some problematic passages in Section 3 that should be removed.\n\nSec. 3.2:\n\"...this same constraint (if using RNN as the decoder) could be an inappropriate constraint in the decoding process.\"  What is the justification or evidence for this claim?  I think the claim should be supported by an argument or some evidence or else should be removed. If the authors intend the subsequent paragraphs to justify the claim, then see my next comments. \n\nSec. 3.2:\n\"The existence of the ground-truth current word embedding potentially decreases the tendency for the decoder to exploit other information from the sentence representation.\"\nBut this is not necessarily an inherent limitation of RNN decoders since it could be addressed by using the embedding of the previously-predicted word rather than the ground-truth word. This is a standard technique in sequence-to-sequence learning; cf. scheduled sampling (Bengio et al., 2015). \n\nSec. 3.2: \n\"Although the word order information is implicitly encoded in the CNN decoder, it is not emphasized as it is explicitly in the RNN decoder. The CNN decoder cares about the quality of generated sequences globally instead of the quality of the next generated word. Relaxing the emphasis on the next word, may help the CNN decoder model to explore the contribution of context in a larger space.\"\nAgain, I don't see any evidence or justification for these arguments. Also see my discussion above about decoder variations; these are not properties of RNNs vs CNNs but rather properties of auto-regressive vs predict-all-words decoders. \n\nSec. 5.2-5.3:\nThere are a few high-level decisions being tuned on the test sets for some of the tasks, e.g., the length of target sequences in Section 5.2 and the number of layers and channel size in Section 5.3. \n\nSec. 5.4:\nWhen trying to explain why an RNN encoder works better than a CNN encoder, the paper includes the following: \"We stated above that, in our belief, explicit usage of the word order information will augment the transferability of the encoder, and constrain the search space of the parameters in the encoder. The results match our belief.\"\nI don't think these beliefs are concrete enough to be upheld or contradicted. Both encoders explicitly use word order information. Can you provide some formal or theoretical statement about how the two encoders treat word order differently? I fear that it's only impressions and suppositions that lead to this difference, rather than necessarily something formal. \n\nSec. 5.4:\nIn Table 1, it is unclear why the \"future predictor\" model is the one selected to be reported from Gan et al (2017). Gan et al has many settings and the \"future predictor\" setting is the worst. An explanation is needed for this choice. \n\nSec. 6.1: \n\nIn the \"BYTE m-LSTM\" paragraph:\n\n\"Our large RNN-CNN model trained on Amazon Book Review (the largest subset of Amazon Review) performs on par with BYTE m-LSTM model, and ours works better than theirs on semantic relatedness and entailment tasks.\"  I'm not sure this \"on par\" assessment is warranted by the results in Table 2.  BYTE m-LSTM is better on MR by 1.6 points and better on CR by 4.7 points. The authors' method is better on SUBJ by 0.7 and better on MPQA by 0.5.  So on sentiment tasks, BYTE m-LSTM is clearly better, and on the other tasks the RNN-CNN is typically better, especially on SICK-r.  \n\n\nMore minor things are below:\n\nSec. 1:\nThe paper contains this: \"The idea of learning from the context information was first successfully applied to vector representation learning for words in Mikolov et al. (2013b)\"\n\nI don't think this is accurate. When restricting attention to neural network methods, it would be more correct to give credit to Collobert et al. (2011). But moving beyond neural methods, there were decades of previous work in using context information (counts of context words) to produce vector representations of words. \n\ntypo: \"which d reduces\" --> \"which reduces\"\n\nSec. 2:\nThe notation in the text doesn't match that in Figure 1: w_i^1 vs. w_1 and h_i^1 vs h_1. \n\nInstead of writing \"non-parametric composition function\", describe it as \"parameter-free\". \"Non-parametric\" means that the number of parameters grows with the data, not that there are no parameters. \n\nIn the \"Representation\" paragraph: how do you compute a max over vectors? Is it a separate max for each dimension? This is not clear from the notation used.\n\nSec. 3.1:\ninappropriate word choice: the use of \"great\" in \"a great and efficient encoding model\"\n\nSec. 3.2:\ninappropriate word choice: the use of \"unveiled\" in \"is still to be unveiled\"\n\nSec. 3.4:\nTying input and output embeddings can be justified with a single sentence and the relevant citations (which are present here). There is no need for speculation about what may be going on, e.g.: \"the model learns to explore the non-linear compositionality of the input words and the uncertain contribution of the target words in the same space\".\n\nSec. 4:\nI think STS14 should be defined and cited where the other tasks are described. \n\nSec. 5.3:\ntypo in Figure 2 caption: \"and and\"\n\n\nSec. 6.1: \n\nIn the \"Skip-thought\" paragraph:\n\ninappropriate word choice: \"kindly\"\n\nThe description that says \"we cut off a branch for decoding\" is not clear to me. What is a \"branch for decoding\" in this context? Please modify it to make it more clear. \n\n\nReferences:\n\nBengio S, Vinyals O, Jaitly N, Shazeer N. Scheduled sampling for sequence prediction with recurrent neural networks. NIPS 2015.\n\nCollobert R, Weston J, Bottou L, Karlen M, Kavukcuoglu K, Kuksa P. Natural language processing (almost) from scratch. Journal of Machine Learning Research 2011.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1512222719952,"tcdate":1511428748461,"number":2,"cdate":1511428748461,"id":"Hy4cMGVlf","invitation":"ICLR.cc/2018/Conference/-/Paper686/Official_Review","forum":"Bk7wvW-C-","replyto":"Bk7wvW-C-","signatures":["ICLR.cc/2018/Conference/Paper686/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The contribution of the paper is to extend Skip-thought by 1) decoding only one target sentence; 2) using a CNN decoder.","rating":"3: Clear rejection","review":"The authors build on the work of Tang et al. (2017), who made a minor change to the skip-thought model by decoding only the next sentence, rather than the previous one also. The additional minor change in this paper is to use a CNN, rather than RNN, decoder.\n\nI am sympathetic to the goals of the work, and believe this sort of work should be carried out, but I see the contribution as too minor to constitute a paper at the conference track of a leading international conference such as ICLR. Given the incremental nature of the work, I think this would be a good fit for something like a short paper at *ACL.\n\nI found the more theoretical motivation of the CNN decoder not terribly convincing, and somewhat post-hoc. I feel as though analogous arguments could just as easily be made for an RNN decoder. Ultimately I see these questions - such as CNN vs. RNN for the decoder - as empirical ones.\n\nFinally, the authors have admirably attempted a thorough comparison with existing work, in the related work section, but this section takes up a large chunk of the paper at the end, and again I would have preferred this section to be much shorter and more concise.\n\nSummary: worthwhile empirical goal, but the paper could have been easily written using half as much space.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1512222719989,"tcdate":1511127565475,"number":1,"cdate":1511127565475,"id":"Skrfq_Jlz","invitation":"ICLR.cc/2018/Conference/-/Paper686/Official_Review","forum":"Bk7wvW-C-","replyto":"Bk7wvW-C-","signatures":["ICLR.cc/2018/Conference/Paper686/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good results, but tons of minor issues","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new RNN encoder–CNN decoder hybrid design for use in pretraining reusable sentence encoders on Kiros's SkipThought objective. The task is interesting and important, and the results are generally good: The new model outperforms SkipThought, and all other prior models for training sentence encoders on unlabeled data. However, some of the design choices seem a bit odd, and I have a large number of minor concerns about the paper. I'd like to see the authors' replies and the other reviews before I can confidently endorse this paper as correct.\n\n\nNon-autoregressive decoding with a CNN strikes me as a somewhat ill-posed problem, even for in this case where you don't actually use the decoder in the final application of your model. At each position, you're training your model to predict a distribution over all words that could appear at the beginning/tenth position/twentieth position in sentences on some topic. I'd appreciate some more discussion of why this should or shouldn't hurt performance. I'd be less concerned about this if the results supporting the use of the CNN decoder were a bit more conclusive: while they are better on average across your smaller experiments, your largest experiment (2400D) shows them roughly tied.\n\nYour paper opens with the line \"Context information plays an important role in human language understanding.\" This sounds like it's making an empirical claim that your paper doesn't support, but it's so vague that it's hard to tell exactly what that claim is. Please clarify this or remove it.\n\nThis sentence is quite inaccurate: \"The idea of learning from the context information was first successfully applied to vector representation learning for words in Mikolov et al. (2013b) and learning from the occurrence of words also succeeded in Pennington et al. (2014).\" Turney and Pantel 2010 ( https://www.jair.org/media/2934/live-2934-4846-jair.pdf ) offer a survey of the substantial prior work that existed at that time.\n\nThe \"Neighborhood Hypothesis\" is given quite a lot of importance, given that it's a fairly small empirical effect without any corresponding theory. The fact that it's emphasized so heavily makes me suspect that I can guess the author of the paper. I'd tone down that part of your framing.\n\nI would appreciate some more analysis of which of the non-central tricks that you describe in section 3 help. For example, max pooling seems reasonable, but you report yourself that mean pooling generally works much better in prior work. Without an explicit experiment, it's not clear why you'd add a mean pooling component.\n\nIt seems misleading to claim that your CNN is modeled after AdaSent, as that model uses a number of layers that varies with the length of the sentence (and differs from yours in a few other less-important ways). Please correct or clarify.\n\nThe use of “†” in table to denote models that predict the next sentence in a sequence doesn't make sense. It should apply to all of your models if I understand correctly. Please clarify.\n\nYou could do a better job at table placement and formatting. Table 3 is in the wrong section, for example.\n\nYou write that: \"Our proposed RNN-CNN model gets similar result on SNLI as Skip-thought, but with much less training time.\" This seems to be based on a comparison between your model run on your hardware and their model run on their (presumably older) hardware, and possibly also with their older version of CuDNN. If that's right, you should tone down this claim or offer some more evidence.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1510092430531,"tcdate":1509686045502,"number":2,"cdate":1509686045502,"id":"rJBQsOt0b","invitation":"ICLR.cc/2018/Conference/-/Paper686/Official_Comment","forum":"Bk7wvW-C-","replyto":"rknG_y_Cb","signatures":["ICLR.cc/2018/Conference/Paper686/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper686/Authors"],"content":{"title":"Results","comment":"We trained our small RNN-CNN model with 4 different amounts of unlabeled data from BookCorpus, which are 20%, 10%, 5%, and 2% of total data, respectively. All the models were evaluated on the SICK-r, and SICK-Entailment (supervised) and STS14 (unsupervised). The results are presented in the table below:\n \nPercentage      sick-r   sick-E(%)    sts14(Pearson/Spearman)\n2%                    0.8347    81.4               0.59/0.57\n5%                    0.8367    81.1               0.60/0.58\n10%                  0.8415    81.7               0.60/0.58\n20%                  0.8528    82.1               0.59/0.56\n100%                0.8530    82.6              0.58/0.56\n \nWe are not able to copy the performance curve of each model during training to this discussion forum, but we will update this curve into our paper in the future revised version. Here, we report some interesting observations.\n\n1/ Longer training time helps all the models learn better representations for sentences, but the performance of each model converges after a certain number of iterations, which matches the Figure 2 in our paper. Therefore, there is no need to train the model for an unlimited time.\n\nUnexpectedly, the models trained with only 2% and 5% both have a slight performance drop on supervised evaluation tasks after training for a long time. However, all 4 models keep improving on unsupervised STS14 evaluation task during training until they converge.\n\n2/ Larger size of training data requires a longer time to converge, and it generally performs better than those with a smaller size of data.\n \nIt suggests that we need to train our model on Amazon Book Review for more iterations, and our model potentially is able to get even better results. As stated in Radford et al. (2017), their BYTE m-LSTM model was trained on Amazon Review for a month, while we only trained our model for around 33 hours, and we still got comparable results on classification tasks and better results on relatedness and entailment tasks.\n\n3/ When it comes to a large model (large dimension of the representation), more data and longer training time will result in better sentence representations."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1509582867898,"tcdate":1509582867898,"number":2,"cdate":1509582867898,"id":"rknG_y_Cb","invitation":"ICLR.cc/2018/Conference/-/Paper686/Public_Comment","forum":"Bk7wvW-C-","replyto":"B1hPJJd0-","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"...","comment":"Thanks! It's certainly reasonable to just report one number (no other similar paper reports learning curves that I know of), but a learning curve would help to at least suggest an answer to an interesting question: Could you get even better results with another order of magnitude more data/training time?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1510092430575,"tcdate":1509580644033,"number":1,"cdate":1509580644033,"id":"B1hPJJd0-","invitation":"ICLR.cc/2018/Conference/-/Paper686/Official_Comment","forum":"Bk7wvW-C-","replyto":"rJtgBTDCZ","signatures":["ICLR.cc/2018/Conference/Paper686/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper686/Authors"],"content":{"title":"Thanks and we are working it.","comment":"Thank you for your reply. It is a great question. Currently, we don’t have the results for your question, but it’ll be good to see the effect of the quantity of unlabeled training data.\n \nFor clarification, we trained models on BookCorpus (74 million) and Amazon Book Review (142 million), respectively, and they were both trained with the same number of iterations and batch size. The results indicate that the model trained on Amazon Book Review outperforms that on BookCorpus. We think that the performance boost was mostly brought by the domain matching between the evaluation tasks and the training data, not by the amount of training data in different corpora.\n \nWe’ll start our experiments on the effect of the quantity of unlabeled training data and get back with results very soon."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1509573872963,"tcdate":1509573872963,"number":1,"cdate":1509573872963,"id":"rJtgBTDCZ","invitation":"ICLR.cc/2018/Conference/-/Paper686/Public_Comment","forum":"Bk7wvW-C-","replyto":"Bk7wvW-C-","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Data volume question","comment":"Just out of curiosity, do you have any results on how the quantity of unlabeled training data you use impacts model performance?"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]}},{"tddate":null,"ddate":null,"tmdate":1509739159927,"tcdate":1509132123401,"number":686,"cdate":1509739157268,"id":"Bk7wvW-C-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bk7wvW-C-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning","abstract":"Context information plays an important role in human language understanding, and it is also useful for machines to learn vector representations of language. In this paper, we explore an asymmetric encoder-decoder structure for unsupervised context-based sentence representation learning. As a result, we build an encoder-decoder architecture with an RNN encoder and a CNN decoder.  We further combine a suite of effective designs to significantly improve model efficiency while also achieving better performance. Our model is trained on two different large unlabeled corpora, and in both cases, transferability is evaluated on a set of downstream language understanding tasks. We empirically show that our model is simple and fast while producing rich sentence representations that excel in downstream tasks.","pdf":"/pdf/3f94a2cb67539fbc0e4a1e67dea081c8c47aeb3a.pdf","TL;DR":"We proposed an RNN-CNN encoder-decoder model for fast unsupervised sentence representation learning.","paperhash":"anonymous|exploring_asymmetric_encoderdecoder_structure_for_contextbased_sentence_representation_learning","_bibtex":"@article{\n  anonymous2018exploring,\n  title={Exploring Asymmetric Encoder-Decoder Structure for Context-based Sentence Representation Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bk7wvW-C-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper686/Authors"],"keywords":["asymmetric structure","RNN-CNN","fast","unsupervised","representation","sentence"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}