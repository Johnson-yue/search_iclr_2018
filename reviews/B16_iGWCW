{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222823898,"tcdate":1511857031669,"number":2,"cdate":1511857031669,"id":"HJecicqxG","invitation":"ICLR.cc/2018/Conference/-/Paper941/Official_Review","forum":"B16_iGWCW","replyto":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference/Paper941/AnonReviewer2"],"readers":["everyone"],"content":{"title":"This paper proposed a boosting method for learning an ensemble of neural networks","rating":"6: Marginally above acceptance threshold","review":"In conventional boosting methods, one puts a weight on each sample. The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.  Thus the learned weak learner at this round will make different mistakes.\nThis idea however is difficult to be applied to deep learning with a large amount of data. This paper instead designed a new boosting method which puts large weights on the category with large error in this round.  In other words samples in the same category will have the same weight \n\nError bound is derived.  Experiments show its usefulness though experiments are limited\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1512222823938,"tcdate":1511445974007,"number":1,"cdate":1511445974007,"id":"ry00rINxM","invitation":"ICLR.cc/2018/Conference/-/Paper941/Official_Review","forum":"B16_iGWCW","replyto":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference/Paper941/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A boosting with CNNs approach with fixed class weights between iterations - Many fundamental questions and experiments haven't been addressed","rating":"2: Strong rejection","review":"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks.\n\nWhile the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:\n\n- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners. It's not clear what kind of loss function is really being optimised here. It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end.\n- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them. And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup.\n- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners.\n- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method) 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST) 3) comparison to simply ensembling with random initialisations.\n\nOther comments:\n- Paper would benefit from writing improvements to make it read better.\n- \"simply use the weighted error function\": I don't think this is correct, AdaBoost loss function is an exponential loss. When you train the base learners, their loss functions will become weighted.\n-  \"to replace the softmax error function (used in deep learning)\": I don't think we have softmax error function","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]}},{"tddate":null,"ddate":null,"tmdate":1510092384661,"tcdate":1509137272664,"number":941,"cdate":1510092362148,"id":"B16_iGWCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B16_iGWCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep Boosting of Diverse Experts","abstract":"In this paper, a deep boosting algorithm is developed to\nlearn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts)\nwith diverse capabilities, e.g., these base deep CNNs are\nsequentially trained to recognize a set of \nobject classes in an easy-to-hard way according to their\nlearning complexities. Our experimental results have demonstrated\nthat our deep boosting algorithm can significantly improve the\naccuracy rates on large-scale visual recognition.","pdf":"/pdf/1bf8847d8795430bde1519605578e2fad6d35ee7.pdf","TL;DR":" A deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs.","paperhash":"anonymous|deep_boosting_of_diverse_experts","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep Boosting of Diverse Experts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B16_iGWCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper941/Authors"],"keywords":["boosting learning","deep learning","neural network"]},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}